{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c711483c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "from multiprocessing import Process,Manager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import models\n",
    "import data\n",
    "import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2499a047-f928-4276-b810-ec3664fbdbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'metric' from '/home/jupyter/vae-cf-pytorch/metric.py'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fffb4bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3cf2289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6867759eb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Set the random seed manually for reproductibility.\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b95f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20254a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items:20101\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "loader = data.DataLoader('ml-20m')\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "print(\"# of items:{}\".format(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a233eb47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:MultiVAE(\n",
      "  (q_layers): ModuleList(\n",
      "    (0): Linear(in_features=20101, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=400, bias=True)\n",
      "  )\n",
      "  (p_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=20101, bias=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "p_dims = [200, 600, n_items]\n",
    "model = models.MultiVAE(p_dims).to(device)\n",
    "\n",
    "print(f\"Model Structure:{model}\\n\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.00)\n",
    "criterion = models.loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47fcf582-afc3-45aa-bb09-44d7a250f03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorboardX Writer\n",
    "writer= SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5190bee1-0d8a-4738-a998-bbf66ae68880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cffc33d-a12b-4704-aa18-9956c9dd6de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "TOTAL_ANNEAL_STEPS = 200000\n",
    "ANNEAL_CAP = 0.2\n",
    "LOG_INTERVAL = 100\n",
    "# EPOCHS = 100\n",
    "EPOCHS = 200\n",
    "SAVE_PATH = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e9a5898-5c51-4c4b-b361-41185fd6294d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33bfa8a2-9c1b-475c-a9dc-6527f09aee18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718bbfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate(range(0, N, BATCH_SIZE)):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "        if TOTAL_ANNEAL_STEPS > 0:\n",
    "            anneal = min(ANNEAL_CAP, \n",
    "                            1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "        else:\n",
    "            anneal = ANNEAL_CAP\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % LOG_INTERVAL == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, BATCH_SIZE)),\n",
    "                        elapsed * 1000 / LOG_INTERVAL,\n",
    "                        train_loss / LOG_INTERVAL))\n",
    "            \n",
    "            # Log loss to tensorboard\n",
    "            n_iter = (epoch - 1) * len(range(0, N, BATCH_SIZE)) + batch_idx\n",
    "            writer.add_scalars('data/loss', {'train': train_loss / LOG_INTERVAL}, n_iter)\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0058f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_tr, data_te):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "    \n",
    "            # cno : avoid users who have no clicks in heldout_data\n",
    "            u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "            data = data[u_idxlist_wo_any_iteracts]\n",
    "            heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "            \n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "            if TOTAL_ANNEAL_STEPS > 0:\n",
    "                anneal = min(ANNEAL_CAP, \n",
    "                               1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "            else:\n",
    "                anneal = ANNEAL_CAP\n",
    "\n",
    "            recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "            loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n1 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1)\n",
    "            n100 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            r20 = metric.Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
    "            r50 = metric.Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
    "\n",
    "            n1_list.append(n1)\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r50_list.append(r50)\n",
    " \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331a9527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  100/ 233 batches | ms/batch 134.92 | loss 572.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "best_n100 = -np.inf\n",
    "update_count = 0\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss, n100, r20, r50 = evaluate(vad_data_tr, vad_data_te)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "                'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                    epoch, time.time() - epoch_start_time, val_loss,\n",
    "                    n100, r20, r50))\n",
    "        print('-' * 89)\n",
    "\n",
    "        n_iter = epoch * len(range(0, N, BATCH_SIZE))\n",
    "        writer.add_scalars('data/loss', {'valid': val_loss}, n_iter)\n",
    "        writer.add_scalar('data/n100', n100, n_iter)\n",
    "        writer.add_scalar('data/r20', r20, n_iter)\n",
    "        writer.add_scalar('data/r50', r50, n_iter)\n",
    "\n",
    "        # Save the model if the n100 is the best we've seen so far.\n",
    "        if n100 > best_n100:\n",
    "            with open(SAVE_PATH, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_n100 = n100\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "print(update_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f3a071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "MODEL_PATH = SAVE_PATH\n",
    "with open(SAVE_PATH, 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19c6cc47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.369 | n100 0.428 | r20 0.400 | r50 0.537\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "# update_count = 0\n",
    "test_loss, n1, n100, r20, r50 = evaluate(test_data_tr, test_data_te)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f} | n100 {:4.3f} | r20 {:4.3f} | '\n",
    "        'r50 {:4.3f}'.format(test_loss, np.mean(n1), np.mean(n100), np.mean(r20), np.mean(r50)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679326ab-6f6d-494c-87bb-add56d986b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259db998-751a-4f30-8d5d-5b93d7ffe1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e99b2-f0c6-4e2e-8cdf-1ad2fd087867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index items by using weights in the encoding of VAE-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29b62da-9d3f-415c-a70f-a6769d2a2483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['q_layers.0.weight', 'q_layers.0.bias', 'q_layers.1.weight', 'q_layers.1.bias', 'p_layers.0.weight', 'p_layers.0.bias', 'p_layers.1.weight', 'p_layers.1.bias'])\n"
     ]
    }
   ],
   "source": [
    "stdict = model.state_dict()\n",
    "print(stdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe985269-d0cf-4a57-b4bb-12ca7d9992a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B(Az+b)+b' = BAz + Bb + b' = np.column_stack((BA,Bb+b'))*np.append(z,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63b3168b-1c7d-4bac-8823-a09662151aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 200])\n",
      "torch.Size([600])\n",
      "torch.Size([20101, 600])\n",
      "torch.Size([20101])\n"
     ]
    }
   ],
   "source": [
    "P0 = stdict['p_layers.0.weight']\n",
    "p0_bias = stdict['p_layers.0.bias']\n",
    "P1 = stdict['p_layers.1.weight']\n",
    "p1_bias = stdict['p_layers.1.bias']\n",
    "print(P0.shape)\n",
    "print(p0_bias.shape)\n",
    "print(P1.shape)\n",
    "print(p1_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5d443b3-a041-4b45-89d9-5062a4399492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.155851   16.968382   19.889275   ...  4.789069    1.4559346\n",
      "  0.31526902]\n",
      "[66.67609949 78.86078206 91.26196631 ... 15.33567651  1.39615124\n",
      " -4.64658844]\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# z = np.random.randn(200)\n",
    "# z_gpu = torch.FloatTensor(z).to(device)\n",
    "\n",
    "# a = model.decode(z_gpu).cpu().detach().numpy()\n",
    "\n",
    "# b = np.matmul(P,np.append(z,1))\n",
    "\n",
    "# print(a)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f1abaf8-1dec-4544-9995-7a0a3681acd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.15585063, 16.96838256, 19.88927587, ...,  4.78906943,\n",
       "        1.45593452,  0.31526898])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.matmul(P1,np.tanh((np.matmul(P0,z)+P0_bias)))+P1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afd3e483-4a68-456a-b100-e7abae307cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20101, 601])\n"
     ]
    }
   ],
   "source": [
    "# B(tanh(Az+b))+b' = Bz'+b' = ([B,b'](z',1))\n",
    "P1_dash = torch.column_stack((P1,p1_bias))\n",
    "print(P1_dash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c6117b0c-d8cf-4c64-b651-7c0c9d966a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 200])\n",
      "torch.Size([600])\n",
      "tensor(1.3187, device='cuda:0')\n",
      "torch.Size([600])\n",
      "tensor(-2.3254, device='cuda:0')\n",
      "tensor(-1.0067, device='cuda:0')\n",
      "tensor(-1.0067, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(P0.shape)\n",
    "# print(P0_bias.shape)\n",
    "# print(torch.matmul(z_gpu,P0.T).to(device)[5])\n",
    "# print(torch.matmul(z_gpu,P0.T).to(device).shape)\n",
    "# print(P0_bias.to(device)[5])\n",
    "# print((torch.matmul(P0,z_gpu)+P0_bias).to(device)[5])\n",
    "# print(torch.add(torch.matmul(P0,z_gpu),P0_bias).to(device)[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d29a33c9-2075-4ca3-ae97-1ed504783928",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17.1559, 16.9684, 19.8893,  ...,  4.7891,  1.4559,  0.3153],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "z_dash=torch.tanh(torch.add(torch.matmul(z_gpu,P0.T),P0_bias)).to(device)\n",
    "print(torch.matmul(P1_dash,torch.cat((z_dash,torch.tensor([1]).to(device)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "998ce726-22bb-4e21-aa99-19b32bdeef5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600])\n"
     ]
    }
   ],
   "source": [
    "print(z_dash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8351423d-a7cc-4037-8689-f143a6abbe2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20101\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "# use a single GPU\n",
    "res = faiss.StandardGpuResources()\n",
    "# # use a multi GPUs\n",
    "# ngpus = faiss.get_num_gpus()\n",
    "\n",
    "# build a flat (CPU) index\n",
    "d = P1_dash.shape[1]\n",
    "index_flat = faiss.IndexFlatIP(d)\n",
    "\n",
    "# make it into a gpu index\n",
    "gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "# # make it into a gpu index(multi GPUs)\n",
    "# gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n",
    "#     cpu_index\n",
    "# )\n",
    "\n",
    "# indexing\n",
    "gpu_index_flat.add(P1_dash.cpu())  \n",
    "print(gpu_index_flat.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90ed9490-93c7-449a-bef9-55e51239ac2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# P0 = torch.FloatTensor(P0).to(device)\n",
    "# P0_bias = torch.FloatTensor(P0_bias).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7ae6291-1e93-46ed-b561-ea00f73629a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_count = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42967c4-bf2f-46d7-aa9d-6932a5b001c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# tauをアイテム数で決める(gumbel-sharpをアイテム数で決めてみる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c91a190-0519-46a8-be0a-93f88b120df8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43603469522020893\n",
      "2.293395481969557\n"
     ]
    }
   ],
   "source": [
    "# beta = np.log(1/n_items)\n",
    "beta = 1\n",
    "# tau = 0.1\n",
    "tau = 1/np.log(np.log(n_items))\n",
    "print(tau)\n",
    "print(1/tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c19f807-4f42-4920-8977-4aaf48b7ea71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampling_ranking(scores, heldout_data, seed, n1_list_per_sampling, n100_list_per_sampling, r20_list_per_sampling, r50_list_per_sampling):\n",
    "    # Add Gumbel samples\n",
    "    np.random.seed(seed=seed)\n",
    "    gumbel_sampled_scores = scores + np.vectorize(gumbel_inverse)(np.random.uniform(size=scores.shape))\n",
    "    # Exclude examples from training set\n",
    "    # gumbel_sampled_scores[data.nonzero()] = -np.inf\n",
    "\n",
    "    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(gumbel_sampled_scores, heldout_data, 1))\n",
    "    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(gumbel_sampled_scores, heldout_data, 100))\n",
    "    r20_list_per_sampling.append(metric.Recall_at_k_batch(gumbel_sampled_scores, heldout_data, 20))\n",
    "    r50_list_per_sampling.append(metric.Recall_at_k_batch(gumbel_sampled_scores, heldout_data, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc803cf5-1add-4814-ab04-415eed2c2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_expectation2(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    manager = Manager()\n",
    "    dummy = manager.dict()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                # print(torch.mean(recon_batch,1))\n",
    "                # print(torch.transpose(recon_batch,0,1).size())\n",
    "                # print(torch.mean(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),1))\n",
    "                # print(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau).size())\n",
    "                # print(torch.mean(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau),1))\n",
    "\n",
    "                recon_batch = F.log_softmax(torch.div(recon_batch,tau), 1)\n",
    "                # recon_batch = F.log_softmax(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau), 1)\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "                \n",
    "                with Manager() as manager:\n",
    "                    # d = manager.dict()\n",
    "                    # l = manager.list()\n",
    "                    n1_list_per_sampling = manager.list()\n",
    "                    n100_list_per_sampling = manager.list()\n",
    "                    r20_list_per_sampling = manager.list()\n",
    "                    r50_list_per_sampling = manager.list()\n",
    "                    p_list = []\n",
    "                    for l in range(n_sampling):\n",
    "                        p = Process(target=sampling_ranking, args=(recon_batch,heldout_data,l,n1_list_per_sampling,n100_list_per_sampling,r20_list_per_sampling,r50_list_per_sampling))\n",
    "                        p.start()\n",
    "                        p_list.append(p)\n",
    "                        if len(p_list) % 4 == 0:\n",
    "                            for p in p_list:\n",
    "                                p.join()\n",
    "                            p_list = []\n",
    "                    \n",
    "                    for p in p_list:\n",
    "                        p.join()\n",
    "                    \n",
    "                    # print(n1_list_per_sampling)\n",
    "                    # print(n100_list_per_sampling)\n",
    "\n",
    "                    n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                    n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                    r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                    r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16bc6408-e2f6-4708-92b5-5d2a08a61493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [2:08:43<00:00, 386.16s/it]t]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.068(0.001) | n100 0.247(0.000) | r20 0.201(0.001) | r50 0.386(0.001)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss2, n1_list2, n100_list2, r20_list2, r50_list2 = evaluate_expectation2(test_data_tr, test_data_te, n_sampling=20)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss2, np.mean(n1_list2), np.std(n1_list2)/np.sqrt(len(n1_list2)), np.mean(n100_list2), np.std(n100_list2)/np.sqrt(len(n100_list2)), np.mean(r20_list2), np.std(r20_list2)/np.sqrt(len(r20_list2)), np.mean(r50_list2), np.std(r50_list2)/np.sqrt(len(r50_list2))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9cede-ab56-4cee-a7fb-078777a384a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b52f22da-cea6-4a38-a562-39e6198e480e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# stochastic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6896e891-13a5-46c6-8017-6e68b3b5d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochasticVAE(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                # recon_batch, mu, logvar = model(data_tensor)\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    z = reparameterize(mu, logvar)\n",
    "                    # print(z)\n",
    "                    recon_batch = model.decode(z)\n",
    "                    recon_batch = recon_batch.cpu().numpy()\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb7676a0-ab7d-4b82-bfbf-794bba403ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [02:07<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 0.00 | n1 0.059(0.000) | n100 0.254(0.000) | r20 0.217(0.000) | r50 0.395(0.000)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_stochasticVAE(test_data_tr, test_data_te, n_sampling=20)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86e996-0062-4257-8e54-5fa7012d1220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a959eb-8f48-4e62-982b-1e4b44a0b77f",
   "metadata": {},
   "source": [
    "# evaluate multi-VAE, Gumbel-VAE, Stochastic-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e3fa5c05-5122-4d56-8fed-1205f1750616",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.2\n",
    "# beta = 1\n",
    "# beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@20\":[[] for _ in range(n_sampling)],\n",
    "               \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@20\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@20\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@20\" : [[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic-Faiss\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                # if TOTAL_ANNEAL_STEPS > 0:\n",
    "                #     anneal = min(ANNEAL_CAP, \n",
    "                #                    1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                # else:\n",
    "                #     anneal = ANNEAL_CAP\n",
    "\n",
    "                # encoding\n",
    "                start = time.perf_counter()\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "                t_encode = time.perf_counter() - start\n",
    "                \n",
    "                # decoding\n",
    "                start = time.perf_counter()\n",
    "                recon_batch = model.decode(mu)  \n",
    "                t_decode = time.perf_counter() - start\n",
    "                \n",
    "                start = time.perf_counter()\n",
    "                recon_batch_cpu = recon_batch.cpu()\n",
    "                t_to_cpu = time.perf_counter() - start\n",
    "                recon_batch_cpu = recon_batch_cpu.numpy()\n",
    "                recon_batch_cpu[data.nonzero()] = -np.inf\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                # \n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    torch.manual_seed(l)\n",
    "                    \n",
    "                    # bluring z\n",
    "                    start = time.perf_counter()\n",
    "                    z_blurred = reparameterize(mu, logvar)\n",
    "                    t_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    # Stochastic multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch_blurred = model.decode(z_blurred)\n",
    "                    recon_batch_blurred = recon_batch_blurred.cpu()\n",
    "                    t_decode_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "                    recon_batch_blurred[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append(t_encode + t_blurred + t_decode_blurred)\n",
    "                    \n",
    "                    # Stochastic multi-VAE with Faiss NNS\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "                    z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(z_dash_blurred.shape[0], device=device)))\n",
    "                    topk_scores, topk_indexes = gpu_index_flat.search(z_dash_blurred_wi_constant.cpu(), 200)\n",
    "                    t_nns_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((z_blurred.shape[0],n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"prediction_time\"][l].append(t_encode + t_blurred + t_nns_topk)\n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling\n",
    "                    start = time.perf_counter() \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch - beta * (-torch.rand(recon_batch.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "                    t_gumbel_sampling = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append(t_encode + t_decode + t_gumbel_sampling)\n",
    "                    \n",
    "                    # multi-VAE\n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_cpu, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_cpu, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_cpu, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_cpu, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append(t_encode + t_decode + t_to_cpu)\n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "06e63bb1-8290-42e7-b289-2f0ae16c1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [50:35<00:00, 151.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "total_loss, metrics_dic = evaluate_stochastic(test_data_tr, test_data_te, n_sampling=50)\n",
    "# print('=' * 89)\n",
    "# print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "#         'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "# print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a062481a-3d3c-4165-aa5f-e9c09a07a595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>precision@50</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>hit_rate@100</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.339002</td>\n",
       "      <td>0.427922</td>\n",
       "      <td>0.365644</td>\n",
       "      <td>0.530972</td>\n",
       "      <td>0.175879</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>0.877735</td>\n",
       "      <td>0.959722</td>\n",
       "      <td>0.025123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.307795</td>\n",
       "      <td>0.403535</td>\n",
       "      <td>0.350634</td>\n",
       "      <td>0.521254</td>\n",
       "      <td>0.163382</td>\n",
       "      <td>0.109058</td>\n",
       "      <td>0.872374</td>\n",
       "      <td>0.958691</td>\n",
       "      <td>0.020013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.294610</td>\n",
       "      <td>0.386627</td>\n",
       "      <td>0.326748</td>\n",
       "      <td>0.493264</td>\n",
       "      <td>0.159074</td>\n",
       "      <td>0.105908</td>\n",
       "      <td>0.853723</td>\n",
       "      <td>0.949405</td>\n",
       "      <td>0.022555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.294611</td>\n",
       "      <td>0.384927</td>\n",
       "      <td>0.326748</td>\n",
       "      <td>0.493262</td>\n",
       "      <td>0.159075</td>\n",
       "      <td>0.105902</td>\n",
       "      <td>0.853723</td>\n",
       "      <td>0.949405</td>\n",
       "      <td>0.008355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ndcg@20  ndcg@100  recall@20  recall@50  \\\n",
       "multi-VAE                   0.339002  0.427922   0.365644   0.530972   \n",
       "multi-VAE-Gumbel            0.307795  0.403535   0.350634   0.521254   \n",
       "multi-VAE-Stochastic        0.294610  0.386627   0.326748   0.493264   \n",
       "multi-VAE-Stochastic-Faiss  0.294611  0.384927   0.326748   0.493262   \n",
       "\n",
       "                            precision@20  precision@50  hit_rate@20  \\\n",
       "multi-VAE                       0.175879      0.113153     0.877735   \n",
       "multi-VAE-Gumbel                0.163382      0.109058     0.872374   \n",
       "multi-VAE-Stochastic            0.159074      0.105908     0.853723   \n",
       "multi-VAE-Stochastic-Faiss      0.159075      0.105902     0.853723   \n",
       "\n",
       "                            hit_rate@100  prediction_time  \n",
       "multi-VAE                       0.959722         0.025123  \n",
       "multi-VAE-Gumbel                0.958691         0.020013  \n",
       "multi-VAE-Stochastic            0.949405         0.022555  \n",
       "multi-VAE-Stochastic-Faiss      0.949405         0.008355  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6e99047-04f5-41ad-a8c6-98cf26aa21ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>precision@50</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>hit_rate@100</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.339002</td>\n",
       "      <td>0.427922</td>\n",
       "      <td>0.365644</td>\n",
       "      <td>0.530972</td>\n",
       "      <td>0.175879</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>0.877735</td>\n",
       "      <td>0.959722</td>\n",
       "      <td>0.025123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.364756</td>\n",
       "      <td>0.450299</td>\n",
       "      <td>0.391848</td>\n",
       "      <td>0.554228</td>\n",
       "      <td>0.184884</td>\n",
       "      <td>0.117140</td>\n",
       "      <td>0.899652</td>\n",
       "      <td>0.965283</td>\n",
       "      <td>0.020288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.363809</td>\n",
       "      <td>0.445374</td>\n",
       "      <td>0.394537</td>\n",
       "      <td>0.558132</td>\n",
       "      <td>0.186135</td>\n",
       "      <td>0.117301</td>\n",
       "      <td>0.909998</td>\n",
       "      <td>0.970722</td>\n",
       "      <td>0.022841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.363809</td>\n",
       "      <td>0.443706</td>\n",
       "      <td>0.394537</td>\n",
       "      <td>0.558129</td>\n",
       "      <td>0.186135</td>\n",
       "      <td>0.117293</td>\n",
       "      <td>0.909998</td>\n",
       "      <td>0.970722</td>\n",
       "      <td>0.008388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ndcg@20  ndcg@100  recall@20  recall@50  \\\n",
       "multi-VAE                   0.339002  0.427922   0.365644   0.530972   \n",
       "multi-VAE-Gumbel            0.364756  0.450299   0.391848   0.554228   \n",
       "multi-VAE-Stochastic        0.363809  0.445374   0.394537   0.558132   \n",
       "multi-VAE-Stochastic-Faiss  0.363809  0.443706   0.394537   0.558129   \n",
       "\n",
       "                            precision@20  precision@50  hit_rate@20  \\\n",
       "multi-VAE                       0.175879      0.113153     0.877735   \n",
       "multi-VAE-Gumbel                0.184884      0.117140     0.899652   \n",
       "multi-VAE-Stochastic            0.186135      0.117301     0.909998   \n",
       "multi-VAE-Stochastic-Faiss      0.186135      0.117293     0.909998   \n",
       "\n",
       "                            hit_rate@100  prediction_time  \n",
       "multi-VAE                       0.959722         0.025123  \n",
       "multi-VAE-Gumbel                0.965283         0.020288  \n",
       "multi-VAE-Stochastic            0.970722         0.022841  \n",
       "multi-VAE-Stochastic-Faiss      0.970722         0.008388  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method_names = list(metrics_dic.keys())\n",
    "# metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_top20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        results.append(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.8, axis=0)))\n",
    "    results_top20per.append(results)\n",
    "results_top20per = pd.DataFrame(results_top20per, columns=metric_names, index=method_names)\n",
    "print(\"Top 20%\")\n",
    "results_top20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5285addb-1baf-4241-8379-46c4ae9bfbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>precision@50</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>hit_rate@100</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.339002</td>\n",
       "      <td>0.427922</td>\n",
       "      <td>0.365644</td>\n",
       "      <td>0.530972</td>\n",
       "      <td>0.175879</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>0.877735</td>\n",
       "      <td>0.959722</td>\n",
       "      <td>0.025123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.248089</td>\n",
       "      <td>0.353923</td>\n",
       "      <td>0.307521</td>\n",
       "      <td>0.486771</td>\n",
       "      <td>0.141408</td>\n",
       "      <td>0.100782</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.952075</td>\n",
       "      <td>0.019732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.222258</td>\n",
       "      <td>0.324836</td>\n",
       "      <td>0.257024</td>\n",
       "      <td>0.428374</td>\n",
       "      <td>0.131647</td>\n",
       "      <td>0.094487</td>\n",
       "      <td>0.803844</td>\n",
       "      <td>0.929258</td>\n",
       "      <td>0.022267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.222255</td>\n",
       "      <td>0.323111</td>\n",
       "      <td>0.257024</td>\n",
       "      <td>0.428371</td>\n",
       "      <td>0.131647</td>\n",
       "      <td>0.094480</td>\n",
       "      <td>0.803844</td>\n",
       "      <td>0.929258</td>\n",
       "      <td>0.008314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ndcg@20  ndcg@100  recall@20  recall@50  \\\n",
       "multi-VAE                   0.339002  0.427922   0.365644   0.530972   \n",
       "multi-VAE-Gumbel            0.248089  0.353923   0.307521   0.486771   \n",
       "multi-VAE-Stochastic        0.222258  0.324836   0.257024   0.428374   \n",
       "multi-VAE-Stochastic-Faiss  0.222255  0.323111   0.257024   0.428371   \n",
       "\n",
       "                            precision@20  precision@50  hit_rate@20  \\\n",
       "multi-VAE                       0.175879      0.113153     0.877735   \n",
       "multi-VAE-Gumbel                0.141408      0.100782     0.846473   \n",
       "multi-VAE-Stochastic            0.131647      0.094487     0.803844   \n",
       "multi-VAE-Stochastic-Faiss      0.131647      0.094480     0.803844   \n",
       "\n",
       "                            hit_rate@100  prediction_time  \n",
       "multi-VAE                       0.959722         0.025123  \n",
       "multi-VAE-Gumbel                0.952075         0.019732  \n",
       "multi-VAE-Stochastic            0.929258         0.022267  \n",
       "multi-VAE-Stochastic-Faiss      0.929258         0.008314  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bottom20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        results.append(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.2, axis=0)))\n",
    "    results_bottom20per.append(results)\n",
    "results_bottom20per = pd.DataFrame(results_bottom20per, columns=metric_names, index=method_names)\n",
    "print(\"Bottom 20%\")\n",
    "results_bottom20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a1a6e-4481-44c0-8a71-8d45ff37856c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f59ae7-a146-451e-b98e-531ef67bfbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "354c947b-9f1f-4522-af57-89cdd460997e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-VAE\tndcg@20:0.3390019526235259\n",
      "multi-VAE\tndcg@100:0.427921597610972\n",
      "multi-VAE\trecall@20:0.36564431951977444\n",
      "multi-VAE\trecall@50:0.5309715538503059\n",
      "multi-VAE\tprecision@20:0.1758791678022434\n",
      "multi-VAE\tprecision@50:0.11315272909866547\n",
      "multi-VAE\thit_rate@20:0.8777346145982416\n",
      "multi-VAE\thit_rate@100:0.9597219382539358\n",
      "multi-VAE\tprediction_time:0.025096308455737015\n",
      "multi-VAE-Gumbel\tndcg@20:0.24272735428304837\n",
      "multi-VAE-Gumbel\tndcg@100:0.3353621752809509\n",
      "multi-VAE-Gumbel\trecall@20:0.3126150629968296\n",
      "multi-VAE-Gumbel\trecall@50:0.482291047211655\n",
      "multi-VAE-Gumbel\tprecision@20:0.11779002481942416\n",
      "multi-VAE-Gumbel\tprecision@50:0.08251114230331066\n",
      "multi-VAE-Gumbel\thit_rate@20:0.9085217746882028\n",
      "multi-VAE-Gumbel\thit_rate@100:0.9757963606624412\n",
      "multi-VAE-Gumbel\tprediction_time:0.02695225974983259\n",
      "multi-VAE-Stochastic\tndcg@20:0.39673035017837077\n",
      "multi-VAE-Stochastic\tndcg@100:0.4738134190134512\n",
      "multi-VAE-Stochastic\trecall@20:0.42287913535252136\n",
      "multi-VAE-Stochastic\trecall@50:0.5839383764597308\n",
      "multi-VAE-Stochastic\tprecision@20:0.19731323060719935\n",
      "multi-VAE-Stochastic\tprecision@50:0.12172169247350062\n",
      "multi-VAE-Stochastic\thit_rate@20:0.9205929257820487\n",
      "multi-VAE-Stochastic\thit_rate@100:0.9745982416683705\n",
      "multi-VAE-Stochastic\tprediction_time:0.02917656848584011\n"
     ]
    }
   ],
   "source": [
    "for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            # if metric_name in (\"prediction_time\"):\n",
    "                # print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.9, axis=1))))\n",
    "            # if metric_name in (\"hit_rate@20\",\"hit_rate@50\"):\n",
    "                # print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.5, axis=1))))\n",
    "            print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.99, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4583a77-938f-430a-8f37-bf7ec11962e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "prob-vae-pytorch",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "prob-vae-pytorch",
   "language": "python",
   "name": "prob-vae-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
