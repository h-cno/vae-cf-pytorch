{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c711483c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "from multiprocessing import Process,Manager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import models\n",
    "import data\n",
    "import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2499a047-f928-4276-b810-ec3664fbdbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'metric' from '/home/jupyter/vae-cf-pytorch/metric.py'>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fffb4bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cf2289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbe68043270>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Set the random seed manually for reproductibility.\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b95f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20254a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items:20101\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "loader = data.DataLoader('ml-20m')\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "print(\"# of items:{}\".format(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a233eb47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:MultiVAE(\n",
      "  (q_layers): ModuleList(\n",
      "    (0): Linear(in_features=20101, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=400, bias=True)\n",
      "  )\n",
      "  (p_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=20101, bias=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "p_dims = [200, 600, n_items]\n",
    "model = models.MultiVAE(p_dims).to(device)\n",
    "\n",
    "print(f\"Model Structure:{model}\\n\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.00)\n",
    "criterion = models.loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47fcf582-afc3-45aa-bb09-44d7a250f03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorboardX Writer\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5190bee1-0d8a-4738-a998-bbf66ae68880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cffc33d-a12b-4704-aa18-9956c9dd6de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "TOTAL_ANNEAL_STEPS = 200000\n",
    "ANNEAL_CAP = 0.2\n",
    "LOG_INTERVAL = 100\n",
    "# EPOCHS = 100\n",
    "EPOCHS = 200\n",
    "SAVE_PATH = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e9a5898-5c51-4c4b-b361-41185fd6294d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33bfa8a2-9c1b-475c-a9dc-6527f09aee18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "718bbfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate(range(0, N, BATCH_SIZE)):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "        if TOTAL_ANNEAL_STEPS > 0:\n",
    "            anneal = min(ANNEAL_CAP, \n",
    "                            1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "        else:\n",
    "            anneal = ANNEAL_CAP\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % LOG_INTERVAL == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, BATCH_SIZE)),\n",
    "                        elapsed * 1000 / LOG_INTERVAL,\n",
    "                        train_loss / LOG_INTERVAL))\n",
    "            \n",
    "            # Log loss to tensorboard\n",
    "            n_iter = (epoch - 1) * len(range(0, N, BATCH_SIZE)) + batch_idx\n",
    "            writer.add_scalars('data/loss', {'train': train_loss / LOG_INTERVAL}, n_iter)\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0058f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_tr, data_te):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "    \n",
    "            # cno : avoid users who have no clicks in heldout_data\n",
    "            u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "            data = data[u_idxlist_wo_any_iteracts]\n",
    "            heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "            \n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "            if TOTAL_ANNEAL_STEPS > 0:\n",
    "                anneal = min(ANNEAL_CAP, \n",
    "                               1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "            else:\n",
    "                anneal = ANNEAL_CAP\n",
    "\n",
    "            recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "            loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n1 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1)\n",
    "            n100 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            r20 = metric.Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
    "            r50 = metric.Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
    "\n",
    "            n1_list.append(n1)\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r50_list.append(r50)\n",
    " \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331a9527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  100/ 233 batches | ms/batch 134.92 | loss 572.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "best_n100 = -np.inf\n",
    "update_count = 0\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss, n100, r20, r50 = evaluate(vad_data_tr, vad_data_te)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "                'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                    epoch, time.time() - epoch_start_time, val_loss,\n",
    "                    n100, r20, r50))\n",
    "        print('-' * 89)\n",
    "\n",
    "        n_iter = epoch * len(range(0, N, BATCH_SIZE))\n",
    "        writer.add_scalars('data/loss', {'valid': val_loss}, n_iter)\n",
    "        writer.add_scalar('data/n100', n100, n_iter)\n",
    "        writer.add_scalar('data/r20', r20, n_iter)\n",
    "        writer.add_scalar('data/r50', r50, n_iter)\n",
    "\n",
    "        # Save the model if the n100 is the best we've seen so far.\n",
    "        if n100 > best_n100:\n",
    "            with open(SAVE_PATH, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_n100 = n100\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "print(update_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3f3a071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "MODEL_PATH = SAVE_PATH\n",
    "with open(SAVE_PATH, 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19c6cc47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.369 | n100 0.428 | r20 0.400 | r50 0.537\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "# update_count = 0\n",
    "test_loss, n1, n100, r20, r50 = evaluate(test_data_tr, test_data_te)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f} | n100 {:4.3f} | r20 {:4.3f} | '\n",
    "        'r50 {:4.3f}'.format(test_loss, np.mean(n1), np.mean(n100), np.mean(r20), np.mean(r50)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a29b62da-9d3f-415c-a70f-a6769d2a2483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['q_layers.0.weight', 'q_layers.0.bias', 'q_layers.1.weight', 'q_layers.1.bias', 'p_layers.0.weight', 'p_layers.0.bias', 'p_layers.1.weight', 'p_layers.1.bias'])\n"
     ]
    }
   ],
   "source": [
    "stdict = model.state_dict()\n",
    "print(stdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe985269-d0cf-4a57-b4bb-12ca7d9992a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B(Az+b)+b' = BAz + Bb + b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63b3168b-1c7d-4bac-8823-a09662151aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 200)\n"
     ]
    }
   ],
   "source": [
    "P0 = stdict['p_layers.0.weight'].clone().cpu().numpy()\n",
    "# print(P0)\n",
    "print(P0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98549246-913f-4d56-b767-aae03e8f733d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "P0_bias = stdict['p_layers.0.bias'].clone().cpu().numpy()\n",
    "# print(P0_bias)\n",
    "print(P0_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11428ee2-edd2-406f-b32c-c92a3b23615b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20101, 600)\n"
     ]
    }
   ],
   "source": [
    "P1 = stdict['p_layers.1.weight'].clone().cpu().numpy()\n",
    "# print(P0)\n",
    "print(P1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f45b86ec-951d-4f62-a1af-0f89b99a0f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20101,)\n"
     ]
    }
   ],
   "source": [
    "P1_bias = stdict['p_layers.1.bias'].clone().cpu().numpy()\n",
    "# print(P0_bias)\n",
    "print(P1_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e04b1ef5-8566-4e90-a51e-99203b9e7093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20101,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_bias = np.matmul(P1,P0_bias) + P1_bias\n",
    "P_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d834dbb-ff2a-4cc0-9ba3-b15b6ae0cbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20101, 200)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.matmul(P1,P0)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbe9b60d-5353-4c95-8649-87f1e7226bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m P \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(P,P_bias, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m P\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m/opt/conda/envs/prob-vae-pytorch/lib/python3.11/site-packages/numpy/lib/function_base.py:5617\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5615\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[1;32m   5616\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "P = np.append(P,P_bias, axis=1)\n",
    "P.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf595b-0d1f-40f9-a727-c492f2970e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c062926-232a-413c-82d9-7ba275cd669b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae6291-1e93-46ed-b561-ea00f73629a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_count = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4820f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate expectational metrics via Gumbel sampling\n",
    "#　tau=0.01(gumbel-sharp=0,01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef8cf1c-fea8-4a18-8b62-62096ac66f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# beta = np.log(1/n_items)\n",
    "beta = 1\n",
    "# tau = 1/np.log(np.log(n_items))\n",
    "# print(tau)\n",
    "tau = 0.01\n",
    "# print(1/tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "126ad097-15ac-47df-a45e-4557e76d71e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def evaluate_expectation(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                # recon_batch = F.log_softmax(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau), 1)\n",
    "                # print(recon_batch.size())\n",
    "                recon_batch = F.log_softmax(torch.div(recon_batch,tau), 1)\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    # Add Gumbel samples\n",
    "                    np.random.seed(seed=l)\n",
    "                    recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # Exclude examples from training set\n",
    "                    # recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee9a7831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "MODEL_PATH = SAVE_PATH\n",
    "with open(SAVE_PATH, 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00d678fd-d2e7-47c2-ab85-ee0bd8030da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575323"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66516dd0-1d22-4224-973c-4199c686bb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138922"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_te.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5b73ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [17:53<00:00, 53.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.366(0.001) | n100 0.427(0.000) | r20 0.400(0.000) | r50 0.537(0.001)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_expectation(test_data_tr, test_data_te, n_sampling=3)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9fedc41-f066-4a75-8bbd-ada60d3bc06b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# beta = np.log(1/n_items)\n",
    "beta = 0.01\n",
    "# tau = 1/np.log(np.log(n_items))\n",
    "# print(tau)\n",
    "tau = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8a45243-7733-47af-8823-2995d5c067a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def evaluate_expectation(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                # print(recon_batch.size())\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    # Add Gumbel samples\n",
    "                    np.random.seed(seed=l)\n",
    "                    recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # Exclude examples from training set\n",
    "                    # recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62e1eed1-438f-4d96-b736-5aaa0fc12d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [17:54<00:00, 53.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.366(0.001) | n100 0.427(0.000) | r20 0.400(0.000) | r50 0.537(0.001)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_expectation(test_data_tr, test_data_te, n_sampling=3)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd907f-cbc3-496e-932b-ef4c964532c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979a5d3-e80a-4100-ba33-ea6542a8e236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d281429-80c9-4f4e-80df-981df31a0721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# beta = np.log(1/n_items)\n",
    "beta = 1\n",
    "# tau = 1/np.log(np.log(n_items))\n",
    "# print(tau)\n",
    "tau = 1\n",
    "# print(1/tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2dffb12-8588-4e1c-9f47-a036c7975e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def evaluate_expectation(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "                    \n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                # recon_batch = F.log_softmax(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau), 1)\n",
    "                # print(recon_batch.size())\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    # Add Gumbel samples\n",
    "                    np.random.seed(seed=l)\n",
    "                    recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # Exclude examples from training set\n",
    "                    # recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3bfd782-77bf-4c66-9354-326c0212e5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [18:10<00:00, 54.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 582.48 | n1 0.001(0.000) | n100 0.002(0.000) | r20 0.001(0.000) | r50 0.002(0.000)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_expectation(test_data_tr, test_data_te, n_sampling=3)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0061e522-c04f-45fa-bca2-a6f80d9e6692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_expectation(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                recon_batch = F.log_softmax(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau), 1)\n",
    "                # print(recon_batch.size())\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    # Add Gumbel samples\n",
    "                    np.random.seed(seed=l)\n",
    "                    recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # Exclude examples from training set\n",
    "                    # recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3907a642-5282-4fb4-91c5-9f974bc9795b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [18:04<00:00, 54.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 582.48 | n1 0.001(0.000) | n100 0.002(0.000) | r20 0.001(0.000) | r50 0.002(0.000)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_expectation(test_data_tr, test_data_te, n_sampling=3)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251bf48-3521-482c-866e-9a4431bc3911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a42967c4-bf2f-46d7-aa9d-6932a5b001c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# tauをアイテム数で決める(gumbel-sharpをアイテム数で決めてみる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c91a190-0519-46a8-be0a-93f88b120df8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43603469522020893\n",
      "2.293395481969557\n"
     ]
    }
   ],
   "source": [
    "# beta = np.log(1/n_items)\n",
    "beta = 1\n",
    "# tau = 0.1\n",
    "tau = 1/np.log(np.log(n_items))\n",
    "print(tau)\n",
    "print(1/tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c19f807-4f42-4920-8977-4aaf48b7ea71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampling_ranking(scores, heldout_data, seed, n1_list_per_sampling, n100_list_per_sampling, r20_list_per_sampling, r50_list_per_sampling):\n",
    "    # Add Gumbel samples\n",
    "    np.random.seed(seed=seed)\n",
    "    gumbel_sampled_scores = scores + np.vectorize(gumbel_inverse)(np.random.uniform(size=scores.shape))\n",
    "    # Exclude examples from training set\n",
    "    # gumbel_sampled_scores[data.nonzero()] = -np.inf\n",
    "\n",
    "    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(gumbel_sampled_scores, heldout_data, 1))\n",
    "    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(gumbel_sampled_scores, heldout_data, 100))\n",
    "    r20_list_per_sampling.append(metric.Recall_at_k_batch(gumbel_sampled_scores, heldout_data, 20))\n",
    "    r50_list_per_sampling.append(metric.Recall_at_k_batch(gumbel_sampled_scores, heldout_data, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc803cf5-1add-4814-ab04-415eed2c2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_expectation2(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    manager = Manager()\n",
    "    dummy = manager.dict()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                # print(torch.mean(recon_batch,1))\n",
    "                # print(torch.transpose(recon_batch,0,1).size())\n",
    "                # print(torch.mean(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),1))\n",
    "                # print(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau).size())\n",
    "                # print(torch.mean(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau),1))\n",
    "\n",
    "                recon_batch = F.log_softmax(torch.div(recon_batch,tau), 1)\n",
    "                # recon_batch = F.log_softmax(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau), 1)\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "                \n",
    "                with Manager() as manager:\n",
    "                    # d = manager.dict()\n",
    "                    # l = manager.list()\n",
    "                    n1_list_per_sampling = manager.list()\n",
    "                    n100_list_per_sampling = manager.list()\n",
    "                    r20_list_per_sampling = manager.list()\n",
    "                    r50_list_per_sampling = manager.list()\n",
    "                    p_list = []\n",
    "                    for l in range(n_sampling):\n",
    "                        p = Process(target=sampling_ranking, args=(recon_batch,heldout_data,l,n1_list_per_sampling,n100_list_per_sampling,r20_list_per_sampling,r50_list_per_sampling))\n",
    "                        p.start()\n",
    "                        p_list.append(p)\n",
    "                        if len(p_list) % 4 == 0:\n",
    "                            for p in p_list:\n",
    "                                p.join()\n",
    "                            p_list = []\n",
    "                    \n",
    "                    for p in p_list:\n",
    "                        p.join()\n",
    "                    \n",
    "                    # print(n1_list_per_sampling)\n",
    "                    # print(n100_list_per_sampling)\n",
    "\n",
    "                    n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                    n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                    r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                    r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16bc6408-e2f6-4708-92b5-5d2a08a61493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [2:08:43<00:00, 386.16s/it]t]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.068(0.001) | n100 0.247(0.000) | r20 0.201(0.001) | r50 0.386(0.001)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss2, n1_list2, n100_list2, r20_list2, r50_list2 = evaluate_expectation2(test_data_tr, test_data_te, n_sampling=20)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss2, np.mean(n1_list2), np.std(n1_list2)/np.sqrt(len(n1_list2)), np.mean(n100_list2), np.std(n100_list2)/np.sqrt(len(n100_list2)), np.mean(r20_list2), np.std(r20_list2)/np.sqrt(len(r20_list2)), np.mean(r50_list2), np.std(r50_list2)/np.sqrt(len(r50_list2))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9cede-ab56-4cee-a7fb-078777a384a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b52f22da-cea6-4a38-a562-39e6198e480e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# stochastic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6896e891-13a5-46c6-8017-6e68b3b5d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochasticVAE(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                # recon_batch, mu, logvar = model(data_tensor)\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    z = reparameterize(mu, logvar)\n",
    "                    # print(z)\n",
    "                    recon_batch = model.decode(z)\n",
    "                    recon_batch = recon_batch.cpu().numpy()\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb7676a0-ab7d-4b82-bfbf-794bba403ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [02:07<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 0.00 | n1 0.059(0.000) | n100 0.254(0.000) | r20 0.217(0.000) | r50 0.395(0.000)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_stochasticVAE(test_data_tr, test_data_te, n_sampling=20)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86e996-0062-4257-8e54-5fa7012d1220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a959eb-8f48-4e62-982b-1e4b44a0b77f",
   "metadata": {},
   "source": [
    "# evaluate multi-VAE, Gumbel-VAE, Stochastic-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e3fa5c05-5122-4d56-8fed-1205f1750616",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1\n",
    "beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@20\":[[] for _ in range(n_sampling)],\n",
    "               \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@20\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@20\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@20\" : [[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                # if TOTAL_ANNEAL_STEPS > 0:\n",
    "                #     anneal = min(ANNEAL_CAP, \n",
    "                #                    1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                # else:\n",
    "                #     anneal = ANNEAL_CAP\n",
    "\n",
    "                start = time.perf_counter()\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "                t_encode = time.perf_counter() - start\n",
    "                # start = time.perf_counter()\n",
    "                # recon_batch = model.decode(mu)   \n",
    "                # t_decode = time.perf_counter() - start\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                # \n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    torch.manual_seed(l)\n",
    "                    \n",
    "                    # Stochastic multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    z_blurred = reparameterize(mu, logvar)\n",
    "                    recon_batch_blurred = model.decode(z_blurred)\n",
    "                    recon_batch_blurred = recon_batch_blurred.cpu().numpy()\n",
    "                    recon_batch_blurred[data.nonzero()] = -np.inf\n",
    "                    t_decode_blurred = time.perf_counter() - start\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append(t_encode + t_decode_blurred)\n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch = model.decode(mu)  \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch - beta * (-torch.rand(recon_batch.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu().numpy()\n",
    "                    recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "                    t_decode_gumbel_sampling = time.perf_counter() - start\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append(t_encode + t_decode_gumbel_sampling)\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch = model.decode(mu)  \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch - beta_dash * (-torch.rand(recon_batch.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu().numpy()\n",
    "                    recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "                    t_decode_gumbel_sampling = time.perf_counter() - start\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-low-beta\"][\"prediction_time\"][l].append(t_encode + t_decode_gumbel_sampling)\n",
    "                    \n",
    "                    # multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch = model.decode(mu)  \n",
    "                    recon_batch = recon_batch.cpu().numpy()\n",
    "                    recon_batch[data.nonzero()] = -np.inf\n",
    "                    t_decode = time.perf_counter() - start\n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append(t_encode + t_decode)\n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "06e63bb1-8290-42e7-b289-2f0ae16c1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [48:47<00:00, 146.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "total_loss, metrics_dic = evaluate_stochastic(test_data_tr, test_data_te, n_sampling=50)\n",
    "# print('=' * 89)\n",
    "# print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "#         'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "# print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a062481a-3d3c-4165-aa5f-e9c09a07a595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>precision@50</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>hit_rate@100</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.339002</td>\n",
       "      <td>0.427922</td>\n",
       "      <td>0.365644</td>\n",
       "      <td>0.530972</td>\n",
       "      <td>0.175879</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>0.877735</td>\n",
       "      <td>0.959722</td>\n",
       "      <td>0.024456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.121517</td>\n",
       "      <td>0.232267</td>\n",
       "      <td>0.174082</td>\n",
       "      <td>0.344445</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.058949</td>\n",
       "      <td>0.681996</td>\n",
       "      <td>0.928178</td>\n",
       "      <td>0.026484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel-low-beta</th>\n",
       "      <td>0.307864</td>\n",
       "      <td>0.403521</td>\n",
       "      <td>0.350676</td>\n",
       "      <td>0.521277</td>\n",
       "      <td>0.163469</td>\n",
       "      <td>0.109056</td>\n",
       "      <td>0.872562</td>\n",
       "      <td>0.958749</td>\n",
       "      <td>0.026508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.294610</td>\n",
       "      <td>0.386627</td>\n",
       "      <td>0.326748</td>\n",
       "      <td>0.493264</td>\n",
       "      <td>0.159074</td>\n",
       "      <td>0.105908</td>\n",
       "      <td>0.853723</td>\n",
       "      <td>0.949405</td>\n",
       "      <td>0.024718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ndcg@20  ndcg@100  recall@20  recall@50  \\\n",
       "multi-VAE                  0.339002  0.427922   0.365644   0.530972   \n",
       "multi-VAE-Gumbel           0.121517  0.232267   0.174082   0.344445   \n",
       "multi-VAE-Gumbel-low-beta  0.307864  0.403521   0.350676   0.521277   \n",
       "multi-VAE-Stochastic       0.294610  0.386627   0.326748   0.493264   \n",
       "\n",
       "                           precision@20  precision@50  hit_rate@20  \\\n",
       "multi-VAE                      0.175879      0.113153     0.877735   \n",
       "multi-VAE-Gumbel               0.067726      0.058949     0.681996   \n",
       "multi-VAE-Gumbel-low-beta      0.163469      0.109056     0.872562   \n",
       "multi-VAE-Stochastic           0.159074      0.105908     0.853723   \n",
       "\n",
       "                           hit_rate@100  prediction_time  \n",
       "multi-VAE                      0.959722         0.024456  \n",
       "multi-VAE-Gumbel               0.928178         0.026484  \n",
       "multi-VAE-Gumbel-low-beta      0.958749         0.026508  \n",
       "multi-VAE-Stochastic           0.949405         0.024718  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f6e99047-04f5-41ad-a8c6-98cf26aa21ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>precision@50</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>hit_rate@100</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.339002</td>\n",
       "      <td>0.427922</td>\n",
       "      <td>0.365644</td>\n",
       "      <td>0.530972</td>\n",
       "      <td>0.175879</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>0.877735</td>\n",
       "      <td>0.959722</td>\n",
       "      <td>0.024724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.187514</td>\n",
       "      <td>0.286781</td>\n",
       "      <td>0.269904</td>\n",
       "      <td>0.445506</td>\n",
       "      <td>0.101607</td>\n",
       "      <td>0.075795</td>\n",
       "      <td>0.896402</td>\n",
       "      <td>0.970763</td>\n",
       "      <td>0.026694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel-low-beta</th>\n",
       "      <td>0.364962</td>\n",
       "      <td>0.450424</td>\n",
       "      <td>0.391937</td>\n",
       "      <td>0.554359</td>\n",
       "      <td>0.185014</td>\n",
       "      <td>0.117063</td>\n",
       "      <td>0.899857</td>\n",
       "      <td>0.964752</td>\n",
       "      <td>0.026714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.363809</td>\n",
       "      <td>0.445374</td>\n",
       "      <td>0.394537</td>\n",
       "      <td>0.558132</td>\n",
       "      <td>0.186135</td>\n",
       "      <td>0.117301</td>\n",
       "      <td>0.909998</td>\n",
       "      <td>0.970722</td>\n",
       "      <td>0.024836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ndcg@20  ndcg@100  recall@20  recall@50  \\\n",
       "multi-VAE                  0.339002  0.427922   0.365644   0.530972   \n",
       "multi-VAE-Gumbel           0.187514  0.286781   0.269904   0.445506   \n",
       "multi-VAE-Gumbel-low-beta  0.364962  0.450424   0.391937   0.554359   \n",
       "multi-VAE-Stochastic       0.363809  0.445374   0.394537   0.558132   \n",
       "\n",
       "                           precision@20  precision@50  hit_rate@20  \\\n",
       "multi-VAE                      0.175879      0.113153     0.877735   \n",
       "multi-VAE-Gumbel               0.101607      0.075795     0.896402   \n",
       "multi-VAE-Gumbel-low-beta      0.185014      0.117063     0.899857   \n",
       "multi-VAE-Stochastic           0.186135      0.117301     0.909998   \n",
       "\n",
       "                           hit_rate@100  prediction_time  \n",
       "multi-VAE                      0.959722         0.024724  \n",
       "multi-VAE-Gumbel               0.970763         0.026694  \n",
       "multi-VAE-Gumbel-low-beta      0.964752         0.026714  \n",
       "multi-VAE-Stochastic           0.970722         0.024836  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method_names = list(metrics_dic.keys())\n",
    "# metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_top20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        results.append(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.8, axis=0)))\n",
    "    results_top20per.append(results)\n",
    "results_top20per = pd.DataFrame(results_top20per, columns=metric_names, index=method_names)\n",
    "print(\"Top 20%\")\n",
    "results_top20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5285addb-1baf-4241-8379-46c4ae9bfbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>precision@50</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>hit_rate@100</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.339002</td>\n",
       "      <td>0.427922</td>\n",
       "      <td>0.365644</td>\n",
       "      <td>0.530972</td>\n",
       "      <td>0.175879</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>0.877735</td>\n",
       "      <td>0.959722</td>\n",
       "      <td>0.024144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.041696</td>\n",
       "      <td>0.167172</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.241581</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>0.041592</td>\n",
       "      <td>0.480495</td>\n",
       "      <td>0.891229</td>\n",
       "      <td>0.026226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel-low-beta</th>\n",
       "      <td>0.247762</td>\n",
       "      <td>0.353688</td>\n",
       "      <td>0.307003</td>\n",
       "      <td>0.487309</td>\n",
       "      <td>0.141159</td>\n",
       "      <td>0.100911</td>\n",
       "      <td>0.846964</td>\n",
       "      <td>0.952198</td>\n",
       "      <td>0.026246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.222258</td>\n",
       "      <td>0.324836</td>\n",
       "      <td>0.257024</td>\n",
       "      <td>0.428374</td>\n",
       "      <td>0.131647</td>\n",
       "      <td>0.094487</td>\n",
       "      <td>0.803844</td>\n",
       "      <td>0.929258</td>\n",
       "      <td>0.024371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ndcg@20  ndcg@100  recall@20  recall@50  \\\n",
       "multi-VAE                  0.339002  0.427922   0.365644   0.530972   \n",
       "multi-VAE-Gumbel           0.041696  0.167172   0.070291   0.241581   \n",
       "multi-VAE-Gumbel-low-beta  0.247762  0.353688   0.307003   0.487309   \n",
       "multi-VAE-Stochastic       0.222258  0.324836   0.257024   0.428374   \n",
       "\n",
       "                           precision@20  precision@50  hit_rate@20  \\\n",
       "multi-VAE                      0.175879      0.113153     0.877735   \n",
       "multi-VAE-Gumbel               0.030915      0.041592     0.480495   \n",
       "multi-VAE-Gumbel-low-beta      0.141159      0.100911     0.846964   \n",
       "multi-VAE-Stochastic           0.131647      0.094487     0.803844   \n",
       "\n",
       "                           hit_rate@100  prediction_time  \n",
       "multi-VAE                      0.959722         0.024144  \n",
       "multi-VAE-Gumbel               0.891229         0.026226  \n",
       "multi-VAE-Gumbel-low-beta      0.952198         0.026246  \n",
       "multi-VAE-Stochastic           0.929258         0.024371  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bottom20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        results.append(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.2, axis=0)))\n",
    "    results_bottom20per.append(results)\n",
    "results_bottom20per = pd.DataFrame(results_bottom20per, columns=metric_names, index=method_names)\n",
    "print(\"Bottom 20%\")\n",
    "results_bottom20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a1a6e-4481-44c0-8a71-8d45ff37856c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f59ae7-a146-451e-b98e-531ef67bfbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "354c947b-9f1f-4522-af57-89cdd460997e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-VAE\tndcg@20:0.3390019526235259\n",
      "multi-VAE\tndcg@100:0.427921597610972\n",
      "multi-VAE\trecall@20:0.36564431951977444\n",
      "multi-VAE\trecall@50:0.5309715538503059\n",
      "multi-VAE\tprecision@20:0.1758791678022434\n",
      "multi-VAE\tprecision@50:0.11315272909866547\n",
      "multi-VAE\thit_rate@20:0.8777346145982416\n",
      "multi-VAE\thit_rate@100:0.9597219382539358\n",
      "multi-VAE\tprediction_time:0.025096308455737015\n",
      "multi-VAE-Gumbel\tndcg@20:0.24272735428304837\n",
      "multi-VAE-Gumbel\tndcg@100:0.3353621752809509\n",
      "multi-VAE-Gumbel\trecall@20:0.3126150629968296\n",
      "multi-VAE-Gumbel\trecall@50:0.482291047211655\n",
      "multi-VAE-Gumbel\tprecision@20:0.11779002481942416\n",
      "multi-VAE-Gumbel\tprecision@50:0.08251114230331066\n",
      "multi-VAE-Gumbel\thit_rate@20:0.9085217746882028\n",
      "multi-VAE-Gumbel\thit_rate@100:0.9757963606624412\n",
      "multi-VAE-Gumbel\tprediction_time:0.02695225974983259\n",
      "multi-VAE-Stochastic\tndcg@20:0.39673035017837077\n",
      "multi-VAE-Stochastic\tndcg@100:0.4738134190134512\n",
      "multi-VAE-Stochastic\trecall@20:0.42287913535252136\n",
      "multi-VAE-Stochastic\trecall@50:0.5839383764597308\n",
      "multi-VAE-Stochastic\tprecision@20:0.19731323060719935\n",
      "multi-VAE-Stochastic\tprecision@50:0.12172169247350062\n",
      "multi-VAE-Stochastic\thit_rate@20:0.9205929257820487\n",
      "multi-VAE-Stochastic\thit_rate@100:0.9745982416683705\n",
      "multi-VAE-Stochastic\tprediction_time:0.02917656848584011\n"
     ]
    }
   ],
   "source": [
    "for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            # if metric_name in (\"prediction_time\"):\n",
    "                # print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.9, axis=1))))\n",
    "            # if metric_name in (\"hit_rate@20\",\"hit_rate@50\"):\n",
    "                # print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.5, axis=1))))\n",
    "            print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.99, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4583a77-938f-430a-8f37-bf7ec11962e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "prob-vae-pytorch",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "prob-vae-pytorch",
   "language": "python",
   "name": "prob-vae-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
