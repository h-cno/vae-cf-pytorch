{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c711483c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import OrderedDict\n",
    "from multiprocessing import Process,Manager\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import models\n",
    "import data\n",
    "import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499a047-f928-4276-b810-ec3664fbdbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fffb4bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3cf2289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc55b0f34f0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Set the random seed manually for reproductibility.\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97b95f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20254a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items:20101\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "loader = data.DataLoader('ml-20m')\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "print(\"# of items:{}\".format(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a233eb47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:MultiVAE(\n",
      "  (q_layers): ModuleList(\n",
      "    (0): Linear(in_features=20101, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=400, bias=True)\n",
      "  )\n",
      "  (p_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=20101, bias=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "p_dims = [200, 600, n_items]\n",
    "model = models.MultiVAE(p_dims).to(device)\n",
    "\n",
    "print(f\"Model Structure:{model}\\n\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.00)\n",
    "criterion = models.loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "47fcf582-afc3-45aa-bb09-44d7a250f03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorboardX Writer\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5190bee1-0d8a-4738-a998-bbf66ae68880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cffc33d-a12b-4704-aa18-9956c9dd6de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "TOTAL_ANNEAL_STEPS = 200000\n",
    "ANNEAL_CAP = 0.2\n",
    "LOG_INTERVAL = 100\n",
    "# EPOCHS = 100\n",
    "EPOCHS = 200\n",
    "SAVE_PATH = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e9a5898-5c51-4c4b-b361-41185fd6294d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "33bfa8a2-9c1b-475c-a9dc-6527f09aee18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "718bbfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate(range(0, N, BATCH_SIZE)):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "        if TOTAL_ANNEAL_STEPS > 0:\n",
    "            anneal = min(ANNEAL_CAP, \n",
    "                            1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "        else:\n",
    "            anneal = ANNEAL_CAP\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % LOG_INTERVAL == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, BATCH_SIZE)),\n",
    "                        elapsed * 1000 / LOG_INTERVAL,\n",
    "                        train_loss / LOG_INTERVAL))\n",
    "            \n",
    "            # Log loss to tensorboard\n",
    "            n_iter = (epoch - 1) * len(range(0, N, BATCH_SIZE)) + batch_idx\n",
    "            writer.add_scalars('data/loss', {'train': train_loss / LOG_INTERVAL}, n_iter)\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c0058f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_tr, data_te):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "    \n",
    "            # cno : avoid users who have no clicks in heldout_data\n",
    "            u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "            data = data[u_idxlist_wo_any_iteracts]\n",
    "            heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "            \n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "            if TOTAL_ANNEAL_STEPS > 0:\n",
    "                anneal = min(ANNEAL_CAP, \n",
    "                               1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "            else:\n",
    "                anneal = ANNEAL_CAP\n",
    "\n",
    "            recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "            loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n1 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1)\n",
    "            n100 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            r20 = metric.Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
    "            r50 = metric.Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
    "\n",
    "            n1_list.append(n1)\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r50_list.append(r50)\n",
    " \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, np.mean(n1_list), np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "331a9527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  100/ 233 batches | ms/batch 48.00 | loss 569.86\n",
      "| epoch   1 |  200/ 233 batches | ms/batch 45.99 | loss 543.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 17.03s | valid loss 430.40 | n100 0.269 | r20 0.242 | r50 0.357\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  100/ 233 batches | ms/batch 47.74 | loss 516.19\n",
      "| epoch   2 |  200/ 233 batches | ms/batch 45.93 | loss 505.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 16.92s | valid loss 410.80 | n100 0.319 | r20 0.289 | r50 0.414\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |  100/ 233 batches | ms/batch 59.74 | loss 505.24\n",
      "| epoch   3 |  200/ 233 batches | ms/batch 57.63 | loss 487.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 19.56s | valid loss 399.14 | n100 0.360 | r20 0.329 | r50 0.460\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |  100/ 233 batches | ms/batch 61.90 | loss 490.82\n",
      "| epoch   4 |  200/ 233 batches | ms/batch 57.84 | loss 483.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 20.02s | valid loss 394.20 | n100 0.373 | r20 0.345 | r50 0.478\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |  100/ 233 batches | ms/batch 61.45 | loss 483.48\n",
      "| epoch   5 |  200/ 233 batches | ms/batch 58.55 | loss 478.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 19.89s | valid loss 390.57 | n100 0.384 | r20 0.357 | r50 0.488\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |  100/ 233 batches | ms/batch 59.58 | loss 485.18\n",
      "| epoch   6 |  200/ 233 batches | ms/batch 57.51 | loss 472.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 19.90s | valid loss 387.84 | n100 0.391 | r20 0.365 | r50 0.500\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |  100/ 233 batches | ms/batch 60.30 | loss 478.20\n",
      "| epoch   7 |  200/ 233 batches | ms/batch 58.33 | loss 473.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 20.00s | valid loss 385.50 | n100 0.396 | r20 0.371 | r50 0.505\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |  100/ 233 batches | ms/batch 61.03 | loss 477.48\n",
      "| epoch   8 |  200/ 233 batches | ms/batch 58.68 | loss 468.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 20.10s | valid loss 383.68 | n100 0.400 | r20 0.373 | r50 0.510\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |  100/ 233 batches | ms/batch 59.44 | loss 470.87\n",
      "| epoch   9 |  200/ 233 batches | ms/batch 58.58 | loss 472.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 20.10s | valid loss 382.05 | n100 0.401 | r20 0.375 | r50 0.513\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |  100/ 233 batches | ms/batch 59.89 | loss 476.40\n",
      "| epoch  10 |  200/ 233 batches | ms/batch 57.42 | loss 461.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 19.97s | valid loss 380.43 | n100 0.404 | r20 0.378 | r50 0.517\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |  100/ 233 batches | ms/batch 59.32 | loss 469.90\n",
      "| epoch  11 |  200/ 233 batches | ms/batch 58.05 | loss 462.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 19.90s | valid loss 379.27 | n100 0.406 | r20 0.380 | r50 0.519\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |  100/ 233 batches | ms/batch 59.75 | loss 473.35\n",
      "| epoch  12 |  200/ 233 batches | ms/batch 57.48 | loss 457.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 19.98s | valid loss 378.09 | n100 0.408 | r20 0.383 | r50 0.520\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |  100/ 233 batches | ms/batch 59.84 | loss 464.67\n",
      "| epoch  13 |  200/ 233 batches | ms/batch 57.84 | loss 467.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 20.03s | valid loss 376.97 | n100 0.409 | r20 0.383 | r50 0.521\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |  100/ 233 batches | ms/batch 59.97 | loss 467.64\n",
      "| epoch  14 |  200/ 233 batches | ms/batch 58.02 | loss 459.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 19.91s | valid loss 375.90 | n100 0.411 | r20 0.385 | r50 0.521\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |  100/ 233 batches | ms/batch 59.25 | loss 461.00\n",
      "| epoch  15 |  200/ 233 batches | ms/batch 57.97 | loss 464.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 20.12s | valid loss 375.17 | n100 0.412 | r20 0.385 | r50 0.524\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |  100/ 233 batches | ms/batch 60.08 | loss 463.00\n",
      "| epoch  16 |  200/ 233 batches | ms/batch 58.41 | loss 461.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 20.12s | valid loss 374.34 | n100 0.412 | r20 0.385 | r50 0.524\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |  100/ 233 batches | ms/batch 59.39 | loss 458.05\n",
      "| epoch  17 |  200/ 233 batches | ms/batch 57.95 | loss 464.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 19.75s | valid loss 373.81 | n100 0.413 | r20 0.387 | r50 0.525\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |  100/ 233 batches | ms/batch 60.66 | loss 464.66\n",
      "| epoch  18 |  200/ 233 batches | ms/batch 59.70 | loss 454.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 20.30s | valid loss 373.12 | n100 0.413 | r20 0.387 | r50 0.524\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |  100/ 233 batches | ms/batch 59.13 | loss 464.40\n",
      "| epoch  19 |  200/ 233 batches | ms/batch 57.70 | loss 457.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 20.00s | valid loss 372.71 | n100 0.413 | r20 0.386 | r50 0.525\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |  100/ 233 batches | ms/batch 60.27 | loss 458.87\n",
      "| epoch  20 |  200/ 233 batches | ms/batch 59.62 | loss 457.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 20.35s | valid loss 372.30 | n100 0.412 | r20 0.384 | r50 0.526\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |  100/ 233 batches | ms/batch 60.21 | loss 460.44\n",
      "| epoch  21 |  200/ 233 batches | ms/batch 60.42 | loss 456.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 20.33s | valid loss 371.74 | n100 0.414 | r20 0.385 | r50 0.525\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |  100/ 233 batches | ms/batch 59.85 | loss 462.13\n",
      "| epoch  22 |  200/ 233 batches | ms/batch 57.74 | loss 452.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 20.26s | valid loss 371.41 | n100 0.415 | r20 0.386 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |  100/ 233 batches | ms/batch 60.80 | loss 463.40\n",
      "| epoch  23 |  200/ 233 batches | ms/batch 58.01 | loss 453.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 20.03s | valid loss 370.99 | n100 0.415 | r20 0.387 | r50 0.526\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |  100/ 233 batches | ms/batch 60.11 | loss 457.59\n",
      "| epoch  24 |  200/ 233 batches | ms/batch 61.35 | loss 457.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 20.39s | valid loss 370.72 | n100 0.415 | r20 0.386 | r50 0.526\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |  100/ 233 batches | ms/batch 59.87 | loss 459.27\n",
      "| epoch  25 |  200/ 233 batches | ms/batch 58.12 | loss 454.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 20.05s | valid loss 370.41 | n100 0.415 | r20 0.387 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |  100/ 233 batches | ms/batch 59.94 | loss 458.34\n",
      "| epoch  26 |  200/ 233 batches | ms/batch 58.08 | loss 456.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 19.91s | valid loss 370.22 | n100 0.414 | r20 0.386 | r50 0.525\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |  100/ 233 batches | ms/batch 60.42 | loss 457.32\n",
      "| epoch  27 |  200/ 233 batches | ms/batch 60.73 | loss 455.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 20.31s | valid loss 369.84 | n100 0.415 | r20 0.386 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |  100/ 233 batches | ms/batch 59.50 | loss 458.77\n",
      "| epoch  28 |  200/ 233 batches | ms/batch 58.13 | loss 453.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 20.00s | valid loss 369.77 | n100 0.416 | r20 0.385 | r50 0.526\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |  100/ 233 batches | ms/batch 60.78 | loss 462.70\n",
      "| epoch  29 |  200/ 233 batches | ms/batch 58.47 | loss 450.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 20.09s | valid loss 369.67 | n100 0.415 | r20 0.386 | r50 0.526\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |  100/ 233 batches | ms/batch 60.57 | loss 459.67\n",
      "| epoch  30 |  200/ 233 batches | ms/batch 59.29 | loss 453.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 20.17s | valid loss 369.32 | n100 0.413 | r20 0.386 | r50 0.525\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |  100/ 233 batches | ms/batch 59.69 | loss 460.58\n",
      "| epoch  31 |  200/ 233 batches | ms/batch 58.58 | loss 451.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 20.22s | valid loss 369.33 | n100 0.416 | r20 0.386 | r50 0.526\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |  100/ 233 batches | ms/batch 59.83 | loss 451.41\n",
      "| epoch  32 |  200/ 233 batches | ms/batch 57.62 | loss 457.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 19.84s | valid loss 369.40 | n100 0.415 | r20 0.386 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |  100/ 233 batches | ms/batch 61.14 | loss 456.28\n",
      "| epoch  33 |  200/ 233 batches | ms/batch 58.81 | loss 454.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 20.21s | valid loss 369.14 | n100 0.417 | r20 0.388 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |  100/ 233 batches | ms/batch 60.37 | loss 461.97\n",
      "| epoch  34 |  200/ 233 batches | ms/batch 57.95 | loss 447.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 20.08s | valid loss 369.12 | n100 0.417 | r20 0.388 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |  100/ 233 batches | ms/batch 59.61 | loss 457.25\n",
      "| epoch  35 |  200/ 233 batches | ms/batch 57.51 | loss 452.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 19.83s | valid loss 369.17 | n100 0.418 | r20 0.387 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |  100/ 233 batches | ms/batch 63.82 | loss 454.52\n",
      "| epoch  36 |  200/ 233 batches | ms/batch 59.07 | loss 455.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 20.43s | valid loss 369.00 | n100 0.418 | r20 0.388 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |  100/ 233 batches | ms/batch 61.41 | loss 451.71\n",
      "| epoch  37 |  200/ 233 batches | ms/batch 60.42 | loss 456.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 20.58s | valid loss 369.03 | n100 0.416 | r20 0.387 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |  100/ 233 batches | ms/batch 62.21 | loss 452.19\n",
      "| epoch  38 |  200/ 233 batches | ms/batch 60.31 | loss 454.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 20.51s | valid loss 369.16 | n100 0.417 | r20 0.387 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |  100/ 233 batches | ms/batch 63.04 | loss 452.15\n",
      "| epoch  39 |  200/ 233 batches | ms/batch 60.15 | loss 455.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 20.60s | valid loss 369.06 | n100 0.417 | r20 0.388 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |  100/ 233 batches | ms/batch 61.67 | loss 459.99\n",
      "| epoch  40 |  200/ 233 batches | ms/batch 61.25 | loss 446.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 20.65s | valid loss 369.31 | n100 0.416 | r20 0.387 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |  100/ 233 batches | ms/batch 61.18 | loss 452.19\n",
      "| epoch  41 |  200/ 233 batches | ms/batch 59.54 | loss 455.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 20.37s | valid loss 369.16 | n100 0.419 | r20 0.389 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |  100/ 233 batches | ms/batch 62.03 | loss 458.22\n",
      "| epoch  42 |  200/ 233 batches | ms/batch 60.48 | loss 452.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 20.47s | valid loss 369.22 | n100 0.417 | r20 0.387 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |  100/ 233 batches | ms/batch 59.39 | loss 457.56\n",
      "| epoch  43 |  200/ 233 batches | ms/batch 59.22 | loss 453.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 19.95s | valid loss 369.19 | n100 0.418 | r20 0.388 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |  100/ 233 batches | ms/batch 59.51 | loss 458.87\n",
      "| epoch  44 |  200/ 233 batches | ms/batch 57.74 | loss 450.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 20.01s | valid loss 369.31 | n100 0.417 | r20 0.388 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |  100/ 233 batches | ms/batch 59.92 | loss 458.52\n",
      "| epoch  45 |  200/ 233 batches | ms/batch 58.86 | loss 452.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 20.05s | valid loss 369.29 | n100 0.417 | r20 0.388 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |  100/ 233 batches | ms/batch 59.87 | loss 456.67\n",
      "| epoch  46 |  200/ 233 batches | ms/batch 59.02 | loss 450.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 20.77s | valid loss 369.36 | n100 0.420 | r20 0.389 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |  100/ 233 batches | ms/batch 60.37 | loss 460.18\n",
      "| epoch  47 |  200/ 233 batches | ms/batch 59.39 | loss 451.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 20.51s | valid loss 369.44 | n100 0.419 | r20 0.388 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |  100/ 233 batches | ms/batch 60.48 | loss 455.02\n",
      "| epoch  48 |  200/ 233 batches | ms/batch 59.40 | loss 453.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 20.19s | valid loss 369.44 | n100 0.419 | r20 0.389 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |  100/ 233 batches | ms/batch 60.05 | loss 460.64\n",
      "| epoch  49 |  200/ 233 batches | ms/batch 59.67 | loss 450.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 20.18s | valid loss 369.47 | n100 0.419 | r20 0.389 | r50 0.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |  100/ 233 batches | ms/batch 59.72 | loss 449.90\n",
      "| epoch  50 |  200/ 233 batches | ms/batch 59.72 | loss 459.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 21.02s | valid loss 369.64 | n100 0.418 | r20 0.390 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |  100/ 233 batches | ms/batch 65.50 | loss 459.56\n",
      "| epoch  51 |  200/ 233 batches | ms/batch 62.81 | loss 450.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 21.60s | valid loss 369.56 | n100 0.421 | r20 0.389 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |  100/ 233 batches | ms/batch 65.86 | loss 457.05\n",
      "| epoch  52 |  200/ 233 batches | ms/batch 65.10 | loss 452.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 21.87s | valid loss 369.58 | n100 0.419 | r20 0.389 | r50 0.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |  100/ 233 batches | ms/batch 64.50 | loss 453.20\n",
      "| epoch  53 |  200/ 233 batches | ms/batch 64.23 | loss 456.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 21.32s | valid loss 369.69 | n100 0.418 | r20 0.389 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |  100/ 233 batches | ms/batch 60.28 | loss 458.18\n",
      "| epoch  54 |  200/ 233 batches | ms/batch 57.75 | loss 451.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 19.89s | valid loss 369.84 | n100 0.419 | r20 0.388 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |  100/ 233 batches | ms/batch 60.77 | loss 459.57\n",
      "| epoch  55 |  200/ 233 batches | ms/batch 57.77 | loss 453.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 19.92s | valid loss 369.85 | n100 0.419 | r20 0.389 | r50 0.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |  100/ 233 batches | ms/batch 59.13 | loss 463.27\n",
      "| epoch  56 |  200/ 233 batches | ms/batch 61.99 | loss 449.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 20.28s | valid loss 370.11 | n100 0.418 | r20 0.390 | r50 0.528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |  100/ 233 batches | ms/batch 60.26 | loss 454.97\n",
      "| epoch  57 |  200/ 233 batches | ms/batch 57.43 | loss 456.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 19.81s | valid loss 370.09 | n100 0.419 | r20 0.390 | r50 0.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |  100/ 233 batches | ms/batch 59.76 | loss 459.54\n",
      "| epoch  58 |  200/ 233 batches | ms/batch 58.15 | loss 450.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 19.99s | valid loss 370.31 | n100 0.418 | r20 0.389 | r50 0.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |  100/ 233 batches | ms/batch 59.77 | loss 459.17\n",
      "| epoch  59 |  200/ 233 batches | ms/batch 61.25 | loss 452.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 20.25s | valid loss 370.24 | n100 0.419 | r20 0.390 | r50 0.530\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |  100/ 233 batches | ms/batch 59.66 | loss 458.89\n",
      "| epoch  60 |  200/ 233 batches | ms/batch 57.40 | loss 453.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 19.77s | valid loss 370.33 | n100 0.419 | r20 0.390 | r50 0.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |  100/ 233 batches | ms/batch 60.40 | loss 459.05\n",
      "| epoch  61 |  200/ 233 batches | ms/batch 59.29 | loss 454.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 20.11s | valid loss 370.42 | n100 0.419 | r20 0.391 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |  100/ 233 batches | ms/batch 59.22 | loss 452.68\n",
      "| epoch  62 |  200/ 233 batches | ms/batch 61.85 | loss 460.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 20.20s | valid loss 370.32 | n100 0.421 | r20 0.391 | r50 0.530\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |  100/ 233 batches | ms/batch 60.41 | loss 455.34\n",
      "| epoch  63 |  200/ 233 batches | ms/batch 58.18 | loss 453.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 20.01s | valid loss 370.48 | n100 0.420 | r20 0.389 | r50 0.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |  100/ 233 batches | ms/batch 59.76 | loss 460.86\n",
      "| epoch  64 |  200/ 233 batches | ms/batch 59.28 | loss 448.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 20.08s | valid loss 370.67 | n100 0.420 | r20 0.390 | r50 0.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |  100/ 233 batches | ms/batch 59.51 | loss 460.17\n",
      "| epoch  65 |  200/ 233 batches | ms/batch 61.55 | loss 449.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 20.39s | valid loss 370.62 | n100 0.421 | r20 0.390 | r50 0.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |  100/ 233 batches | ms/batch 59.84 | loss 454.89\n",
      "| epoch  66 |  200/ 233 batches | ms/batch 57.61 | loss 456.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 19.82s | valid loss 370.80 | n100 0.421 | r20 0.390 | r50 0.530\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |  100/ 233 batches | ms/batch 59.09 | loss 462.32\n",
      "| epoch  67 |  200/ 233 batches | ms/batch 59.23 | loss 452.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 20.06s | valid loss 370.93 | n100 0.421 | r20 0.391 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |  100/ 233 batches | ms/batch 59.68 | loss 460.06\n",
      "| epoch  68 |  200/ 233 batches | ms/batch 60.42 | loss 452.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 20.20s | valid loss 370.88 | n100 0.420 | r20 0.391 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |  100/ 233 batches | ms/batch 59.31 | loss 455.18\n",
      "| epoch  69 |  200/ 233 batches | ms/batch 57.75 | loss 455.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 19.87s | valid loss 370.97 | n100 0.420 | r20 0.392 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |  100/ 233 batches | ms/batch 59.46 | loss 456.75\n",
      "| epoch  70 |  200/ 233 batches | ms/batch 59.60 | loss 452.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 20.07s | valid loss 371.13 | n100 0.420 | r20 0.390 | r50 0.530\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |  100/ 233 batches | ms/batch 59.23 | loss 456.91\n",
      "| epoch  71 |  200/ 233 batches | ms/batch 61.28 | loss 457.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 20.22s | valid loss 371.04 | n100 0.421 | r20 0.392 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |  100/ 233 batches | ms/batch 59.76 | loss 459.55\n",
      "| epoch  72 |  200/ 233 batches | ms/batch 58.35 | loss 452.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 20.03s | valid loss 371.11 | n100 0.422 | r20 0.391 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 |  100/ 233 batches | ms/batch 59.48 | loss 458.60\n",
      "| epoch  73 |  200/ 233 batches | ms/batch 59.11 | loss 453.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 20.03s | valid loss 371.25 | n100 0.422 | r20 0.392 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |  100/ 233 batches | ms/batch 59.06 | loss 458.46\n",
      "| epoch  74 |  200/ 233 batches | ms/batch 60.76 | loss 456.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 20.19s | valid loss 371.21 | n100 0.422 | r20 0.392 | r50 0.530\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |  100/ 233 batches | ms/batch 59.98 | loss 460.78\n",
      "| epoch  75 |  200/ 233 batches | ms/batch 57.69 | loss 450.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 19.86s | valid loss 371.32 | n100 0.423 | r20 0.391 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |  100/ 233 batches | ms/batch 59.95 | loss 457.61\n",
      "| epoch  76 |  200/ 233 batches | ms/batch 59.21 | loss 452.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 20.12s | valid loss 371.69 | n100 0.420 | r20 0.392 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |  100/ 233 batches | ms/batch 61.30 | loss 452.89\n",
      "| epoch  77 |  200/ 233 batches | ms/batch 58.58 | loss 458.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 20.14s | valid loss 371.43 | n100 0.422 | r20 0.392 | r50 0.530\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |  100/ 233 batches | ms/batch 59.24 | loss 456.44\n",
      "| epoch  78 |  200/ 233 batches | ms/batch 58.24 | loss 453.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 19.86s | valid loss 371.50 | n100 0.422 | r20 0.395 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |  100/ 233 batches | ms/batch 59.80 | loss 458.97\n",
      "| epoch  79 |  200/ 233 batches | ms/batch 59.88 | loss 454.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 20.11s | valid loss 371.83 | n100 0.421 | r20 0.391 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |  100/ 233 batches | ms/batch 60.90 | loss 460.73\n",
      "| epoch  80 |  200/ 233 batches | ms/batch 58.83 | loss 454.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 20.12s | valid loss 371.92 | n100 0.420 | r20 0.391 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |  100/ 233 batches | ms/batch 59.82 | loss 459.47\n",
      "| epoch  81 |  200/ 233 batches | ms/batch 58.14 | loss 451.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 19.96s | valid loss 371.93 | n100 0.422 | r20 0.393 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |  100/ 233 batches | ms/batch 59.08 | loss 458.26\n",
      "| epoch  82 |  200/ 233 batches | ms/batch 58.91 | loss 453.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 19.90s | valid loss 372.11 | n100 0.423 | r20 0.394 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |  100/ 233 batches | ms/batch 60.56 | loss 454.97\n",
      "| epoch  83 |  200/ 233 batches | ms/batch 59.78 | loss 455.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 20.25s | valid loss 372.02 | n100 0.421 | r20 0.393 | r50 0.530\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |  100/ 233 batches | ms/batch 60.11 | loss 457.82\n",
      "| epoch  84 |  200/ 233 batches | ms/batch 58.03 | loss 454.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 19.92s | valid loss 372.16 | n100 0.422 | r20 0.392 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |  100/ 233 batches | ms/batch 59.20 | loss 457.80\n",
      "| epoch  85 |  200/ 233 batches | ms/batch 59.05 | loss 455.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 20.01s | valid loss 372.17 | n100 0.421 | r20 0.393 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |  100/ 233 batches | ms/batch 61.98 | loss 459.47\n",
      "| epoch  86 |  200/ 233 batches | ms/batch 59.01 | loss 456.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 20.32s | valid loss 372.23 | n100 0.422 | r20 0.392 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |  100/ 233 batches | ms/batch 59.00 | loss 456.14\n",
      "| epoch  87 |  200/ 233 batches | ms/batch 57.71 | loss 456.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 19.79s | valid loss 372.51 | n100 0.422 | r20 0.393 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |  100/ 233 batches | ms/batch 59.91 | loss 456.85\n",
      "| epoch  88 |  200/ 233 batches | ms/batch 60.09 | loss 458.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 20.14s | valid loss 372.44 | n100 0.422 | r20 0.393 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |  100/ 233 batches | ms/batch 60.61 | loss 462.52\n",
      "| epoch  89 |  200/ 233 batches | ms/batch 59.69 | loss 450.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 20.18s | valid loss 372.53 | n100 0.422 | r20 0.393 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |  100/ 233 batches | ms/batch 59.88 | loss 457.45\n",
      "| epoch  90 |  200/ 233 batches | ms/batch 58.48 | loss 458.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 20.01s | valid loss 372.62 | n100 0.423 | r20 0.393 | r50 0.531\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |  100/ 233 batches | ms/batch 59.30 | loss 457.93\n",
      "| epoch  91 |  200/ 233 batches | ms/batch 59.51 | loss 455.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 20.06s | valid loss 372.74 | n100 0.423 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |  100/ 233 batches | ms/batch 61.88 | loss 460.92\n",
      "| epoch  92 |  200/ 233 batches | ms/batch 58.42 | loss 452.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 20.25s | valid loss 372.65 | n100 0.424 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |  100/ 233 batches | ms/batch 59.58 | loss 459.29\n",
      "| epoch  93 |  200/ 233 batches | ms/batch 58.01 | loss 455.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 19.88s | valid loss 373.03 | n100 0.421 | r20 0.393 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |  100/ 233 batches | ms/batch 59.54 | loss 460.41\n",
      "| epoch  94 |  200/ 233 batches | ms/batch 59.48 | loss 455.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 20.16s | valid loss 372.90 | n100 0.423 | r20 0.396 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |  100/ 233 batches | ms/batch 62.55 | loss 459.13\n",
      "| epoch  95 |  200/ 233 batches | ms/batch 57.67 | loss 454.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 20.22s | valid loss 372.98 | n100 0.423 | r20 0.393 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |  100/ 233 batches | ms/batch 59.52 | loss 461.64\n",
      "| epoch  96 |  200/ 233 batches | ms/batch 58.18 | loss 455.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 19.96s | valid loss 373.13 | n100 0.422 | r20 0.391 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 |  100/ 233 batches | ms/batch 59.86 | loss 458.18\n",
      "| epoch  97 |  200/ 233 batches | ms/batch 60.07 | loss 458.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 20.03s | valid loss 372.99 | n100 0.425 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |  100/ 233 batches | ms/batch 63.15 | loss 463.94\n",
      "| epoch  98 |  200/ 233 batches | ms/batch 57.64 | loss 453.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 20.21s | valid loss 373.32 | n100 0.423 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |  100/ 233 batches | ms/batch 60.34 | loss 461.49\n",
      "| epoch  99 |  200/ 233 batches | ms/batch 58.54 | loss 453.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 20.05s | valid loss 373.13 | n100 0.424 | r20 0.393 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |  100/ 233 batches | ms/batch 59.23 | loss 462.84\n",
      "| epoch 100 |  200/ 233 batches | ms/batch 59.68 | loss 451.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 20.06s | valid loss 373.42 | n100 0.424 | r20 0.392 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 101 |  100/ 233 batches | ms/batch 61.53 | loss 459.90\n",
      "| epoch 101 |  200/ 233 batches | ms/batch 58.48 | loss 455.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 101 | time: 20.24s | valid loss 373.22 | n100 0.424 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 102 |  100/ 233 batches | ms/batch 59.02 | loss 461.93\n",
      "| epoch 102 |  200/ 233 batches | ms/batch 57.83 | loss 455.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 102 | time: 19.85s | valid loss 373.42 | n100 0.423 | r20 0.393 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 103 |  100/ 233 batches | ms/batch 59.38 | loss 461.41\n",
      "| epoch 103 |  200/ 233 batches | ms/batch 59.99 | loss 452.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 103 | time: 20.19s | valid loss 373.65 | n100 0.424 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 104 |  100/ 233 batches | ms/batch 63.46 | loss 461.66\n",
      "| epoch 104 |  200/ 233 batches | ms/batch 57.94 | loss 454.72\n",
      "| epoch 112 |  200/ 233 batches | ms/batch 58.24 | loss 452.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 112 | time: 20.17s | valid loss 374.23 | n100 0.424 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 113 |  100/ 233 batches | ms/batch 62.96 | loss 460.86\n",
      "| epoch 113 |  200/ 233 batches | ms/batch 57.65 | loss 457.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 113 | time: 20.17s | valid loss 374.23 | n100 0.424 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 114 |  100/ 233 batches | ms/batch 59.38 | loss 456.26\n",
      "| epoch 114 |  200/ 233 batches | ms/batch 57.75 | loss 461.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 114 | time: 19.88s | valid loss 374.41 | n100 0.424 | r20 0.393 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 115 |  100/ 233 batches | ms/batch 60.02 | loss 459.50\n",
      "| epoch 115 |  200/ 233 batches | ms/batch 58.49 | loss 459.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 115 | time: 20.10s | valid loss 374.61 | n100 0.424 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 116 |  100/ 233 batches | ms/batch 63.39 | loss 465.58\n",
      "| epoch 116 |  200/ 233 batches | ms/batch 57.96 | loss 453.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 116 | time: 20.25s | valid loss 374.70 | n100 0.424 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 117 |  100/ 233 batches | ms/batch 60.34 | loss 459.61\n",
      "| epoch 117 |  200/ 233 batches | ms/batch 58.63 | loss 458.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 117 | time: 20.12s | valid loss 374.59 | n100 0.424 | r20 0.393 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 118 |  100/ 233 batches | ms/batch 59.42 | loss 464.72\n",
      "| epoch 118 |  200/ 233 batches | ms/batch 57.61 | loss 452.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 118 | time: 20.01s | valid loss 374.71 | n100 0.423 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 119 |  100/ 233 batches | ms/batch 62.27 | loss 465.75\n",
      "| epoch 119 |  200/ 233 batches | ms/batch 58.78 | loss 454.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 119 | time: 20.28s | valid loss 374.68 | n100 0.424 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 120 |  100/ 233 batches | ms/batch 59.72 | loss 463.63\n",
      "| epoch 120 |  200/ 233 batches | ms/batch 57.62 | loss 455.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 120 | time: 19.87s | valid loss 374.75 | n100 0.424 | r20 0.393 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 121 |  100/ 233 batches | ms/batch 58.99 | loss 460.74\n",
      "| epoch 121 |  200/ 233 batches | ms/batch 57.92 | loss 460.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 121 | time: 20.20s | valid loss 374.69 | n100 0.424 | r20 0.393 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 122 |  100/ 233 batches | ms/batch 63.21 | loss 465.79\n",
      "| epoch 122 |  200/ 233 batches | ms/batch 57.75 | loss 451.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 122 | time: 20.21s | valid loss 374.92 | n100 0.424 | r20 0.393 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 123 |  100/ 233 batches | ms/batch 59.28 | loss 462.18\n",
      "| epoch 123 |  200/ 233 batches | ms/batch 57.81 | loss 457.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 123 | time: 19.90s | valid loss 375.02 | n100 0.424 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 124 |  100/ 233 batches | ms/batch 59.94 | loss 462.21\n",
      "| epoch 124 |  200/ 233 batches | ms/batch 58.30 | loss 457.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 124 | time: 20.23s | valid loss 374.95 | n100 0.425 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 125 |  100/ 233 batches | ms/batch 60.55 | loss 459.16\n",
      "| epoch 125 |  200/ 233 batches | ms/batch 58.29 | loss 458.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 125 | time: 19.99s | valid loss 375.27 | n100 0.423 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 126 |  100/ 233 batches | ms/batch 60.40 | loss 457.31\n",
      "| epoch 126 |  200/ 233 batches | ms/batch 58.61 | loss 462.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 126 | time: 20.10s | valid loss 375.14 | n100 0.424 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 127 |  100/ 233 batches | ms/batch 59.62 | loss 467.36\n",
      "| epoch 127 |  200/ 233 batches | ms/batch 57.97 | loss 455.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 127 | time: 20.23s | valid loss 375.33 | n100 0.424 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 128 |  100/ 233 batches | ms/batch 60.05 | loss 465.16\n",
      "| epoch 128 |  200/ 233 batches | ms/batch 58.67 | loss 454.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 128 | time: 20.07s | valid loss 375.33 | n100 0.424 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 129 |  100/ 233 batches | ms/batch 59.64 | loss 464.17\n",
      "| epoch 129 |  200/ 233 batches | ms/batch 57.68 | loss 456.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 129 | time: 19.88s | valid loss 375.38 | n100 0.425 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 130 |  100/ 233 batches | ms/batch 59.64 | loss 463.36\n",
      "| epoch 130 |  200/ 233 batches | ms/batch 58.95 | loss 458.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 130 | time: 20.55s | valid loss 375.46 | n100 0.425 | r20 0.394 | r50 0.532\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 131 |  100/ 233 batches | ms/batch 60.28 | loss 466.50\n",
      "| epoch 131 |  200/ 233 batches | ms/batch 57.79 | loss 455.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 131 | time: 20.00s | valid loss 375.49 | n100 0.425 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 132 |  100/ 233 batches | ms/batch 59.60 | loss 463.58\n",
      "| epoch 132 |  200/ 233 batches | ms/batch 57.87 | loss 457.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 132 | time: 20.00s | valid loss 375.52 | n100 0.425 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 133 |  100/ 233 batches | ms/batch 60.31 | loss 464.46\n",
      "| epoch 133 |  200/ 233 batches | ms/batch 58.41 | loss 455.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 133 | time: 20.37s | valid loss 375.60 | n100 0.425 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 134 |  100/ 233 batches | ms/batch 59.51 | loss 463.65\n",
      "| epoch 134 |  200/ 233 batches | ms/batch 58.15 | loss 456.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 134 | time: 19.90s | valid loss 375.82 | n100 0.425 | r20 0.396 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 135 |  100/ 233 batches | ms/batch 59.38 | loss 460.58\n",
      "| epoch 135 |  200/ 233 batches | ms/batch 57.80 | loss 458.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 135 | time: 20.02s | valid loss 376.03 | n100 0.425 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 136 |  100/ 233 batches | ms/batch 60.21 | loss 459.19\n",
      "| epoch 136 |  200/ 233 batches | ms/batch 58.52 | loss 459.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 136 | time: 20.38s | valid loss 375.95 | n100 0.425 | r20 0.396 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 137 |  100/ 233 batches | ms/batch 59.58 | loss 466.20\n",
      "| epoch 137 |  200/ 233 batches | ms/batch 57.83 | loss 454.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 137 | time: 19.91s | valid loss 375.93 | n100 0.423 | r20 0.394 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 138 |  100/ 233 batches | ms/batch 60.62 | loss 462.10\n",
      "| epoch 138 |  200/ 233 batches | ms/batch 58.29 | loss 458.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 138 | time: 20.11s | valid loss 376.05 | n100 0.424 | r20 0.394 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 139 |  100/ 233 batches | ms/batch 59.46 | loss 460.97\n",
      "| epoch 139 |  200/ 233 batches | ms/batch 58.05 | loss 459.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 139 | time: 20.27s | valid loss 375.95 | n100 0.425 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 140 |  100/ 233 batches | ms/batch 59.68 | loss 466.79\n",
      "| epoch 140 |  200/ 233 batches | ms/batch 59.01 | loss 455.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 140 | time: 20.07s | valid loss 376.27 | n100 0.424 | r20 0.394 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 141 |  100/ 233 batches | ms/batch 59.81 | loss 464.26\n",
      "| epoch 141 |  200/ 233 batches | ms/batch 57.97 | loss 460.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 141 | time: 19.92s | valid loss 376.30 | n100 0.424 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 142 |  100/ 233 batches | ms/batch 59.21 | loss 462.89\n",
      "| epoch 142 |  200/ 233 batches | ms/batch 57.59 | loss 457.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 142 | time: 20.27s | valid loss 376.15 | n100 0.425 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 143 |  100/ 233 batches | ms/batch 60.07 | loss 461.22\n",
      "| epoch 143 |  200/ 233 batches | ms/batch 58.50 | loss 459.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 143 | time: 20.03s | valid loss 376.42 | n100 0.424 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 144 |  100/ 233 batches | ms/batch 59.56 | loss 463.42\n",
      "| epoch 144 |  200/ 233 batches | ms/batch 58.10 | loss 456.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 144 | time: 19.97s | valid loss 376.47 | n100 0.425 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 145 |  100/ 233 batches | ms/batch 59.91 | loss 458.89\n",
      "| epoch 145 |  200/ 233 batches | ms/batch 58.28 | loss 461.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 145 | time: 20.33s | valid loss 376.82 | n100 0.424 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 146 |  100/ 233 batches | ms/batch 59.06 | loss 460.01\n",
      "| epoch 146 |  200/ 233 batches | ms/batch 57.89 | loss 460.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 146 | time: 19.84s | valid loss 376.53 | n100 0.423 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 147 |  100/ 233 batches | ms/batch 59.83 | loss 459.08\n",
      "| epoch 147 |  200/ 233 batches | ms/batch 58.09 | loss 466.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 147 | time: 19.98s | valid loss 376.56 | n100 0.424 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 148 |  100/ 233 batches | ms/batch 59.42 | loss 468.40\n",
      "| epoch 148 |  200/ 233 batches | ms/batch 57.43 | loss 455.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 148 | time: 20.21s | valid loss 376.66 | n100 0.425 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 149 |  100/ 233 batches | ms/batch 59.35 | loss 462.55\n",
      "| epoch 149 |  200/ 233 batches | ms/batch 58.97 | loss 462.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 149 | time: 20.04s | valid loss 376.64 | n100 0.424 | r20 0.394 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 150 |  100/ 233 batches | ms/batch 59.33 | loss 464.92\n",
      "| epoch 150 |  200/ 233 batches | ms/batch 58.18 | loss 460.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 150 | time: 19.82s | valid loss 376.96 | n100 0.424 | r20 0.393 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 151 |  100/ 233 batches | ms/batch 59.20 | loss 465.57\n",
      "| epoch 151 |  200/ 233 batches | ms/batch 57.27 | loss 457.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 151 | time: 20.06s | valid loss 376.77 | n100 0.425 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 152 |  100/ 233 batches | ms/batch 60.13 | loss 461.19\n",
      "| epoch 152 |  200/ 233 batches | ms/batch 58.46 | loss 461.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 152 | time: 19.90s | valid loss 376.92 | n100 0.425 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 153 |  100/ 233 batches | ms/batch 59.16 | loss 463.29\n",
      "| epoch 153 |  200/ 233 batches | ms/batch 57.88 | loss 460.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 153 | time: 19.86s | valid loss 377.07 | n100 0.424 | r20 0.393 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 154 |  100/ 233 batches | ms/batch 59.94 | loss 464.99\n",
      "| epoch 154 |  200/ 233 batches | ms/batch 58.24 | loss 458.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 154 | time: 20.33s | valid loss 377.17 | n100 0.425 | r20 0.394 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 155 |  100/ 233 batches | ms/batch 59.21 | loss 468.54\n",
      "| epoch 155 |  200/ 233 batches | ms/batch 57.34 | loss 454.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 155 | time: 19.73s | valid loss 377.03 | n100 0.426 | r20 0.396 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 156 |  100/ 233 batches | ms/batch 59.67 | loss 465.98\n",
      "| epoch 156 |  200/ 233 batches | ms/batch 58.28 | loss 458.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 156 | time: 19.92s | valid loss 377.19 | n100 0.426 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 157 |  100/ 233 batches | ms/batch 59.20 | loss 461.01\n",
      "| epoch 157 |  200/ 233 batches | ms/batch 57.58 | loss 463.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 157 | time: 20.17s | valid loss 377.17 | n100 0.425 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 158 |  100/ 233 batches | ms/batch 59.31 | loss 469.36\n",
      "| epoch 158 |  200/ 233 batches | ms/batch 57.91 | loss 453.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 158 | time: 19.89s | valid loss 377.42 | n100 0.425 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 159 |  100/ 233 batches | ms/batch 59.88 | loss 464.30\n",
      "| epoch 159 |  200/ 233 batches | ms/batch 57.86 | loss 460.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 159 | time: 19.84s | valid loss 377.38 | n100 0.425 | r20 0.396 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 160 |  100/ 233 batches | ms/batch 59.32 | loss 461.91\n",
      "| epoch 160 |  200/ 233 batches | ms/batch 57.86 | loss 463.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 160 | time: 20.29s | valid loss 377.30 | n100 0.425 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 161 |  100/ 233 batches | ms/batch 61.67 | loss 465.42\n",
      "| epoch 161 |  200/ 233 batches | ms/batch 58.77 | loss 459.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 161 | time: 20.17s | valid loss 377.49 | n100 0.424 | r20 0.393 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 162 |  100/ 233 batches | ms/batch 59.06 | loss 464.37\n",
      "| epoch 162 |  200/ 233 batches | ms/batch 57.75 | loss 458.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 162 | time: 19.81s | valid loss 377.38 | n100 0.426 | r20 0.397 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 163 |  100/ 233 batches | ms/batch 60.23 | loss 463.43\n",
      "| epoch 163 |  200/ 233 batches | ms/batch 58.31 | loss 460.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 163 | time: 20.18s | valid loss 377.43 | n100 0.425 | r20 0.396 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 164 |  100/ 233 batches | ms/batch 60.71 | loss 464.21\n",
      "| epoch 164 |  200/ 233 batches | ms/batch 57.55 | loss 456.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 164 | time: 19.93s | valid loss 377.57 | n100 0.425 | r20 0.394 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 165 |  100/ 233 batches | ms/batch 59.51 | loss 466.30\n",
      "| epoch 165 |  200/ 233 batches | ms/batch 58.73 | loss 457.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 165 | time: 20.03s | valid loss 377.77 | n100 0.424 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 166 |  100/ 233 batches | ms/batch 59.30 | loss 462.84\n",
      "| epoch 166 |  200/ 233 batches | ms/batch 57.63 | loss 460.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 166 | time: 20.05s | valid loss 377.84 | n100 0.424 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 167 |  100/ 233 batches | ms/batch 60.37 | loss 463.27\n",
      "| epoch 167 |  200/ 233 batches | ms/batch 57.93 | loss 459.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 167 | time: 20.00s | valid loss 377.96 | n100 0.424 | r20 0.396 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 168 |  100/ 233 batches | ms/batch 60.07 | loss 466.60\n",
      "| epoch 168 |  200/ 233 batches | ms/batch 57.64 | loss 462.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 168 | time: 19.88s | valid loss 378.01 | n100 0.426 | r20 0.396 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 169 |  100/ 233 batches | ms/batch 59.23 | loss 462.24\n",
      "| epoch 169 |  200/ 233 batches | ms/batch 57.65 | loss 464.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 169 | time: 20.05s | valid loss 378.03 | n100 0.425 | r20 0.395 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 170 |  100/ 233 batches | ms/batch 62.04 | loss 458.36\n",
      "| epoch 170 |  200/ 233 batches | ms/batch 58.53 | loss 466.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 170 | time: 20.22s | valid loss 377.95 | n100 0.424 | r20 0.394 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 171 |  100/ 233 batches | ms/batch 59.39 | loss 467.25\n",
      "| epoch 171 |  200/ 233 batches | ms/batch 57.64 | loss 457.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 171 | time: 19.87s | valid loss 378.27 | n100 0.426 | r20 0.396 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 172 |  100/ 233 batches | ms/batch 60.60 | loss 467.71\n",
      "| epoch 172 |  200/ 233 batches | ms/batch 58.07 | loss 456.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 172 | time: 20.21s | valid loss 378.29 | n100 0.423 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 173 |  100/ 233 batches | ms/batch 60.17 | loss 467.40\n",
      "| epoch 173 |  200/ 233 batches | ms/batch 57.97 | loss 456.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 173 | time: 19.96s | valid loss 378.20 | n100 0.423 | r20 0.394 | r50 0.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 174 |  100/ 233 batches | ms/batch 59.22 | loss 464.41\n",
      "| epoch 174 |  200/ 233 batches | ms/batch 58.40 | loss 460.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 174 | time: 19.92s | valid loss 378.45 | n100 0.424 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 175 |  100/ 233 batches | ms/batch 59.52 | loss 461.21\n",
      "| epoch 175 |  200/ 233 batches | ms/batch 57.48 | loss 462.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 175 | time: 20.02s | valid loss 378.13 | n100 0.425 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 176 |  100/ 233 batches | ms/batch 60.60 | loss 462.18\n",
      "| epoch 176 |  200/ 233 batches | ms/batch 58.29 | loss 466.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 176 | time: 20.16s | valid loss 378.36 | n100 0.424 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 177 |  100/ 233 batches | ms/batch 60.10 | loss 466.57\n",
      "| epoch 177 |  200/ 233 batches | ms/batch 57.58 | loss 459.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 177 | time: 19.88s | valid loss 378.04 | n100 0.426 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 178 |  100/ 233 batches | ms/batch 59.74 | loss 457.64\n",
      "| epoch 178 |  200/ 233 batches | ms/batch 57.61 | loss 465.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 178 | time: 20.24s | valid loss 378.36 | n100 0.425 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 179 |  100/ 233 batches | ms/batch 60.37 | loss 462.43\n",
      "| epoch 179 |  200/ 233 batches | ms/batch 58.84 | loss 465.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 179 | time: 20.03s | valid loss 378.13 | n100 0.425 | r20 0.396 | r50 0.536\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 180 |  100/ 233 batches | ms/batch 59.00 | loss 461.87\n",
      "| epoch 180 |  200/ 233 batches | ms/batch 57.70 | loss 460.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 180 | time: 19.77s | valid loss 378.22 | n100 0.425 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 181 |  100/ 233 batches | ms/batch 60.48 | loss 463.37\n",
      "| epoch 181 |  200/ 233 batches | ms/batch 58.23 | loss 459.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 181 | time: 20.27s | valid loss 378.27 | n100 0.424 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 182 |  100/ 233 batches | ms/batch 60.35 | loss 464.27\n",
      "| epoch 182 |  200/ 233 batches | ms/batch 57.46 | loss 462.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 182 | time: 19.92s | valid loss 378.09 | n100 0.425 | r20 0.394 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 183 |  100/ 233 batches | ms/batch 58.95 | loss 462.52\n",
      "| epoch 183 |  200/ 233 batches | ms/batch 58.48 | loss 461.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 183 | time: 19.93s | valid loss 378.14 | n100 0.426 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 184 |  100/ 233 batches | ms/batch 59.46 | loss 465.74\n",
      "| epoch 184 |  200/ 233 batches | ms/batch 57.71 | loss 456.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 184 | time: 20.10s | valid loss 378.18 | n100 0.426 | r20 0.396 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 185 |  100/ 233 batches | ms/batch 60.15 | loss 464.94\n",
      "| epoch 185 |  200/ 233 batches | ms/batch 58.62 | loss 459.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 185 | time: 20.15s | valid loss 378.26 | n100 0.425 | r20 0.396 | r50 0.536\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 186 |  100/ 233 batches | ms/batch 59.77 | loss 465.48\n",
      "| epoch 186 |  200/ 233 batches | ms/batch 57.25 | loss 454.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 186 | time: 19.91s | valid loss 378.16 | n100 0.425 | r20 0.396 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 187 |  100/ 233 batches | ms/batch 59.21 | loss 462.56\n",
      "| epoch 187 |  200/ 233 batches | ms/batch 57.94 | loss 462.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 187 | time: 20.24s | valid loss 378.26 | n100 0.425 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 188 |  100/ 233 batches | ms/batch 59.97 | loss 462.47\n",
      "| epoch 188 |  200/ 233 batches | ms/batch 59.52 | loss 463.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 188 | time: 20.09s | valid loss 378.53 | n100 0.424 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 189 |  100/ 233 batches | ms/batch 59.17 | loss 468.05\n",
      "| epoch 189 |  200/ 233 batches | ms/batch 57.66 | loss 458.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 189 | time: 19.78s | valid loss 378.23 | n100 0.426 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 190 |  100/ 233 batches | ms/batch 60.15 | loss 467.07\n",
      "| epoch 190 |  200/ 233 batches | ms/batch 58.22 | loss 457.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 190 | time: 20.33s | valid loss 378.42 | n100 0.424 | r20 0.395 | r50 0.537\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 191 |  100/ 233 batches | ms/batch 58.98 | loss 464.64\n",
      "| epoch 191 |  200/ 233 batches | ms/batch 59.30 | loss 458.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 191 | time: 20.05s | valid loss 378.13 | n100 0.426 | r20 0.395 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 192 |  100/ 233 batches | ms/batch 59.92 | loss 462.85\n",
      "| epoch 192 |  200/ 233 batches | ms/batch 58.72 | loss 463.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 192 | time: 20.07s | valid loss 378.35 | n100 0.426 | r20 0.395 | r50 0.536\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 193 |  100/ 233 batches | ms/batch 59.27 | loss 466.45\n",
      "| epoch 193 |  200/ 233 batches | ms/batch 57.64 | loss 458.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 193 | time: 20.17s | valid loss 378.36 | n100 0.424 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 194 |  100/ 233 batches | ms/batch 58.93 | loss 463.35\n",
      "| epoch 194 |  200/ 233 batches | ms/batch 59.66 | loss 462.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 194 | time: 20.02s | valid loss 378.48 | n100 0.426 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 195 |  100/ 233 batches | ms/batch 59.49 | loss 464.17\n",
      "| epoch 195 |  200/ 233 batches | ms/batch 57.47 | loss 458.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 195 | time: 19.83s | valid loss 378.33 | n100 0.426 | r20 0.397 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 196 |  100/ 233 batches | ms/batch 59.17 | loss 464.69\n",
      "| epoch 196 |  200/ 233 batches | ms/batch 58.39 | loss 459.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 196 | time: 20.18s | valid loss 378.35 | n100 0.424 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 197 |  100/ 233 batches | ms/batch 59.96 | loss 460.82\n",
      "| epoch 197 |  200/ 233 batches | ms/batch 59.66 | loss 466.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 197 | time: 20.06s | valid loss 378.34 | n100 0.427 | r20 0.397 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 198 |  100/ 233 batches | ms/batch 59.19 | loss 466.81\n",
      "| epoch 198 |  200/ 233 batches | ms/batch 57.75 | loss 458.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 198 | time: 19.84s | valid loss 378.49 | n100 0.426 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 199 |  100/ 233 batches | ms/batch 60.05 | loss 461.38\n",
      "| epoch 199 |  200/ 233 batches | ms/batch 59.38 | loss 462.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 199 | time: 20.31s | valid loss 378.40 | n100 0.426 | r20 0.396 | r50 0.535\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 200 |  100/ 233 batches | ms/batch 59.13 | loss 465.88\n",
      "| epoch 200 |  200/ 233 batches | ms/batch 59.16 | loss 457.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 200 | time: 19.98s | valid loss 378.29 | n100 0.425 | r20 0.395 | r50 0.534\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_n100 = -np.inf\n",
    "update_count = 0\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss, n100, r20, r50 = evaluate(vad_data_tr, vad_data_te)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "                'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                    epoch, time.time() - epoch_start_time, val_loss,\n",
    "                    n100, r20, r50))\n",
    "        print('-' * 89)\n",
    "\n",
    "        n_iter = epoch * len(range(0, N, BATCH_SIZE))\n",
    "        writer.add_scalars('data/loss', {'valid': val_loss}, n_iter)\n",
    "        writer.add_scalar('data/n100', n100, n_iter)\n",
    "        writer.add_scalar('data/r20', r20, n_iter)\n",
    "        writer.add_scalar('data/r50', r50, n_iter)\n",
    "\n",
    "        # Save the model if the n100 is the best we've seen so far.\n",
    "        if n100 > best_n100:\n",
    "            with open(SAVE_PATH, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_n100 = n100\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f3f3a071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "MODEL_PATH = SAVE_PATH\n",
    "with open(SAVE_PATH, 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "19c6cc47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 377.47 | n1 0.369 | n100 0.428 | r20 0.400 | r50 0.537\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss, n1, n100, r20, r50 = evaluate(test_data_tr, test_data_te)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f} | n100 {:4.3f} | r20 {:4.3f} | '\n",
    "        'r50 {:4.3f}'.format(test_loss, n1, n100, r20, r50))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4820f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate expectational metrics via Gumbel sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4ef8cf1c-fea8-4a18-8b62-62096ac66f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# beta = np.log(1/n_items)\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "126ad097-15ac-47df-a45e-4557e76d71e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def evaluate_expectation(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                recon_batch = F.log_softmax(recon_batch, 1)\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    # Add Gumbel samples\n",
    "                    np.random.seed(seed=l)\n",
    "                    recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # Exclude examples from training set\n",
    "                    recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ee9a7831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "MODEL_PATH = SAVE_PATH\n",
    "with open(SAVE_PATH, 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "00d678fd-d2e7-47c2-ab85-ee0bd8030da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575323"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "66516dd0-1d22-4224-973c-4199c686bb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138922"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_te.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b73ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_expectation(test_data_tr, test_data_te, n_sampling=100)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7c22ae81-36c0-492c-abe8-f2f42c96ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 377.47 | n1 0.074(0.000) | n100 0.232(0.000) | r20 0.185(0.000) | r50 0.347(0.000)\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1a317285-b4cb-4eff-8868-d5ef8560f5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07368144894865716"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(n1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0fb29f47-47af-4cda-9f29-987588df02cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2612517809116018"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(n1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea5c41-bef5-4c11-920b-d56a04293aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc803cf5-1add-4814-ab04-415eed2c2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_expectation2(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                recon_batch = F.log_softmax(recon_batch, 1)\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    # Add Gumbel samples\n",
    "                    np.random.seed(seed=l)\n",
    "                    recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # Exclude examples from training set\n",
    "                    recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f00cdd-96c9-4b1f-954c-d11215ea0dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "prob-vae-pytorch",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "prob-vae-pytorch",
   "language": "python",
   "name": "prob-vae-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
