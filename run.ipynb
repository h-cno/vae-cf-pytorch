{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c711483c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "from multiprocessing import Process,Manager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import models\n",
    "import data\n",
    "import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2499a047-f928-4276-b810-ec3664fbdbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'metric' from '/home/jupyter/vae-cf-pytorch/metric.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fffb4bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cf2289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f096d43e310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Set the random seed manually for reproductibility.\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b95f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20254a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items:20101\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "loader = data.DataLoader('ml-20m')\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "print(\"# of items:{}\".format(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a233eb47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:MultiVAE(\n",
      "  (q_layers): ModuleList(\n",
      "    (0): Linear(in_features=20101, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=400, bias=True)\n",
      "  )\n",
      "  (p_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=20101, bias=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "p_dims = [200, 600, n_items]\n",
    "model = models.MultiVAE(p_dims).to(device)\n",
    "\n",
    "print(f\"Model Structure:{model}\\n\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.00)\n",
    "criterion = models.loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47fcf582-afc3-45aa-bb09-44d7a250f03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorboardX Writer\n",
    "writer= SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5190bee1-0d8a-4738-a998-bbf66ae68880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cffc33d-a12b-4704-aa18-9956c9dd6de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "TOTAL_ANNEAL_STEPS = 200000\n",
    "ANNEAL_CAP = 0.2\n",
    "LOG_INTERVAL = 100\n",
    "# EPOCHS = 100\n",
    "EPOCHS = 200\n",
    "SAVE_PATH = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e9a5898-5c51-4c4b-b361-41185fd6294d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33bfa8a2-9c1b-475c-a9dc-6527f09aee18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "718bbfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate(range(0, N, BATCH_SIZE)):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "        if TOTAL_ANNEAL_STEPS > 0:\n",
    "            anneal = min(ANNEAL_CAP, \n",
    "                            1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "        else:\n",
    "            anneal = ANNEAL_CAP\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % LOG_INTERVAL == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, BATCH_SIZE)),\n",
    "                        elapsed * 1000 / LOG_INTERVAL,\n",
    "                        train_loss / LOG_INTERVAL))\n",
    "            \n",
    "            # Log loss to tensorboard\n",
    "            n_iter = (epoch - 1) * len(range(0, N, BATCH_SIZE)) + batch_idx\n",
    "            writer.add_scalars('data/loss', {'train': train_loss / LOG_INTERVAL}, n_iter)\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0058f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_tr, data_te):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "    \n",
    "            # cno : avoid users who have no clicks in heldout_data\n",
    "            u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "            data = data[u_idxlist_wo_any_iteracts]\n",
    "            heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "            \n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "            if TOTAL_ANNEAL_STEPS > 0:\n",
    "                anneal = min(ANNEAL_CAP, \n",
    "                               1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "            else:\n",
    "                anneal = ANNEAL_CAP\n",
    "\n",
    "            recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "            loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n1 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1)\n",
    "            n100 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            r20 = metric.Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
    "            r50 = metric.Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
    "\n",
    "            n1_list.append(n1)\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r50_list.append(r50)\n",
    " \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331a9527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  100/ 233 batches | ms/batch 134.92 | loss 572.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "best_n100 = -np.inf\n",
    "update_count = 0\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss, n100, r20, r50 = evaluate(vad_data_tr, vad_data_te)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "                'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                    epoch, time.time() - epoch_start_time, val_loss,\n",
    "                    n100, r20, r50))\n",
    "        print('-' * 89)\n",
    "\n",
    "        n_iter = epoch * len(range(0, N, BATCH_SIZE))\n",
    "        writer.add_scalars('data/loss', {'valid': val_loss}, n_iter)\n",
    "        writer.add_scalar('data/n100', n100, n_iter)\n",
    "        writer.add_scalar('data/r20', r20, n_iter)\n",
    "        writer.add_scalar('data/r50', r50, n_iter)\n",
    "\n",
    "        # Save the model if the n100 is the best we've seen so far.\n",
    "        if n100 > best_n100:\n",
    "            with open(SAVE_PATH, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_n100 = n100\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "print(update_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3f3a071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "MODEL_PATH = SAVE_PATH\n",
    "with open(SAVE_PATH, 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19c6cc47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.369 | n100 0.428 | r20 0.400 | r50 0.537\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "# update_count = 0\n",
    "test_loss, n1, n100, r20, r50 = evaluate(test_data_tr, test_data_te)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f} | n100 {:4.3f} | r20 {:4.3f} | '\n",
    "        'r50 {:4.3f}'.format(test_loss, np.mean(n1), np.mean(n100), np.mean(r20), np.mean(r50)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679326ab-6f6d-494c-87bb-add56d986b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259db998-751a-4f30-8d5d-5b93d7ffe1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e99b2-f0c6-4e2e-8cdf-1ad2fd087867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index items by using weights in the encoding of VAE-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29b62da-9d3f-415c-a70f-a6769d2a2483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['q_layers.0.weight', 'q_layers.0.bias', 'q_layers.1.weight', 'q_layers.1.bias', 'p_layers.0.weight', 'p_layers.0.bias', 'p_layers.1.weight', 'p_layers.1.bias'])\n"
     ]
    }
   ],
   "source": [
    "stdict = model.state_dict()\n",
    "print(stdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe985269-d0cf-4a57-b4bb-12ca7d9992a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B(Az+b)+b' = BAz + Bb + b' = np.column_stack((BA,Bb+b'))*np.append(z,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63b3168b-1c7d-4bac-8823-a09662151aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 200])\n",
      "torch.Size([600])\n",
      "torch.Size([20101, 600])\n",
      "torch.Size([20101])\n"
     ]
    }
   ],
   "source": [
    "P0 = stdict['p_layers.0.weight']\n",
    "p0_bias = stdict['p_layers.0.bias']\n",
    "P1 = stdict['p_layers.1.weight']\n",
    "p1_bias = stdict['p_layers.1.bias']\n",
    "print(P0.shape)\n",
    "print(p0_bias.shape)\n",
    "print(P1.shape)\n",
    "print(p1_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f1abaf8-1dec-4544-9995-7a0a3681acd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.matmul(P1,np.tanh((np.matmul(P0,z)+P0_bias)))+P1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afd3e483-4a68-456a-b100-e7abae307cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20101, 601])\n"
     ]
    }
   ],
   "source": [
    "# B(tanh(Az+b))+b' = Bz'+b' = ([B,b'](z',1))\n",
    "P1_dash = torch.column_stack((P1,p1_bias))\n",
    "print(P1_dash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6117b0c-d8cf-4c64-b651-7c0c9d966a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(P0.shape)\n",
    "# print(P0_bias.shape)\n",
    "# print(torch.matmul(z_gpu,P0.T).to(device)[5])\n",
    "# print(torch.matmul(z_gpu,P0.T).to(device).shape)\n",
    "# print(P0_bias.to(device)[5])\n",
    "# print((torch.matmul(P0,z_gpu)+P0_bias).to(device)[5])\n",
    "# print(torch.add(torch.matmul(P0,z_gpu),P0_bias).to(device)[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8351423d-a7cc-4037-8689-f143a6abbe2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20101\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "# use a single GPU\n",
    "res = faiss.StandardGpuResources()\n",
    "# # use a multi GPUs\n",
    "# ngpus = faiss.get_num_gpus()\n",
    "\n",
    "# build a flat (CPU) index\n",
    "d = P1_dash.shape[1]\n",
    "index_flat = faiss.IndexFlatIP(d)\n",
    "# gpu_index_flat = faiss.GpuIndexFlatIP(res,d)\n",
    "\n",
    "# make it into a gpu index\n",
    "gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "# # make it into a gpu index(multi GPUs)\n",
    "# gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n",
    "#     cpu_index\n",
    "# )\n",
    "\n",
    "# indexing\n",
    "gpu_index_flat.add(P1_dash.cpu())\n",
    "# gpu_index_flat.add(P1_dash) \n",
    "print(gpu_index_flat.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3fe3ba9-d07c-4b35-87a9-8ac6832ec8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlist = 100\n",
    "quantizer = faiss.IndexFlatIP(d)\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "gpu_index_ivf = faiss.index_cpu_to_gpu(res, 0, index_ivf)\n",
    "gpu_index_ivf.train(P1_dash.cpu())\n",
    "gpu_index_ivf.add(P1_dash.cpu())\n",
    "\n",
    "gpu_index_ivf.nprobe = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90ed9490-93c7-449a-bef9-55e51239ac2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# P0 = torch.FloatTensor(P0).to(device)\n",
    "# P0_bias = torch.FloatTensor(P0_bias).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7ae6291-1e93-46ed-b561-ea00f73629a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_count = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42967c4-bf2f-46d7-aa9d-6932a5b001c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# tauをアイテム数で決める(gumbel-sharpをアイテム数で決めてみる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c91a190-0519-46a8-be0a-93f88b120df8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43603469522020893\n",
      "2.293395481969557\n"
     ]
    }
   ],
   "source": [
    "# beta = np.log(1/n_items)\n",
    "beta = 1\n",
    "# tau = 0.1\n",
    "tau = 1/np.log(np.log(n_items))\n",
    "print(tau)\n",
    "print(1/tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c19f807-4f42-4920-8977-4aaf48b7ea71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampling_ranking(scores, heldout_data, seed, n1_list_per_sampling, n100_list_per_sampling, r20_list_per_sampling, r50_list_per_sampling):\n",
    "    # Add Gumbel samples\n",
    "    np.random.seed(seed=seed)\n",
    "    gumbel_sampled_scores = scores + np.vectorize(gumbel_inverse)(np.random.uniform(size=scores.shape))\n",
    "    # Exclude examples from training set\n",
    "    # gumbel_sampled_scores[data.nonzero()] = -np.inf\n",
    "\n",
    "    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(gumbel_sampled_scores, heldout_data, 1))\n",
    "    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(gumbel_sampled_scores, heldout_data, 100))\n",
    "    r20_list_per_sampling.append(metric.Recall_at_k_batch(gumbel_sampled_scores, heldout_data, 20))\n",
    "    r50_list_per_sampling.append(metric.Recall_at_k_batch(gumbel_sampled_scores, heldout_data, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc803cf5-1add-4814-ab04-415eed2c2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_expectation2(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    manager = Manager()\n",
    "    dummy = manager.dict()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                # print(torch.mean(recon_batch,1))\n",
    "                # print(torch.transpose(recon_batch,0,1).size())\n",
    "                # print(torch.mean(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),1))\n",
    "                # print(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau).size())\n",
    "                # print(torch.mean(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau),1))\n",
    "\n",
    "                recon_batch = F.log_softmax(torch.div(recon_batch,tau), 1)\n",
    "                # recon_batch = F.log_softmax(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau), 1)\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "                \n",
    "                with Manager() as manager:\n",
    "                    # d = manager.dict()\n",
    "                    # l = manager.list()\n",
    "                    n1_list_per_sampling = manager.list()\n",
    "                    n100_list_per_sampling = manager.list()\n",
    "                    r20_list_per_sampling = manager.list()\n",
    "                    r50_list_per_sampling = manager.list()\n",
    "                    p_list = []\n",
    "                    for l in range(n_sampling):\n",
    "                        p = Process(target=sampling_ranking, args=(recon_batch,heldout_data,l,n1_list_per_sampling,n100_list_per_sampling,r20_list_per_sampling,r50_list_per_sampling))\n",
    "                        p.start()\n",
    "                        p_list.append(p)\n",
    "                        if len(p_list) % 4 == 0:\n",
    "                            for p in p_list:\n",
    "                                p.join()\n",
    "                            p_list = []\n",
    "                    \n",
    "                    for p in p_list:\n",
    "                        p.join()\n",
    "                    \n",
    "                    # print(n1_list_per_sampling)\n",
    "                    # print(n100_list_per_sampling)\n",
    "\n",
    "                    n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                    n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                    r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                    r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16bc6408-e2f6-4708-92b5-5d2a08a61493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [2:08:43<00:00, 386.16s/it]t]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.068(0.001) | n100 0.247(0.000) | r20 0.201(0.001) | r50 0.386(0.001)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss2, n1_list2, n100_list2, r20_list2, r50_list2 = evaluate_expectation2(test_data_tr, test_data_te, n_sampling=20)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss2, np.mean(n1_list2), np.std(n1_list2)/np.sqrt(len(n1_list2)), np.mean(n100_list2), np.std(n100_list2)/np.sqrt(len(n100_list2)), np.mean(r20_list2), np.std(r20_list2)/np.sqrt(len(r20_list2)), np.mean(r50_list2), np.std(r50_list2)/np.sqrt(len(r50_list2))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9cede-ab56-4cee-a7fb-078777a384a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b52f22da-cea6-4a38-a562-39e6198e480e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# stochastic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6896e891-13a5-46c6-8017-6e68b3b5d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochasticVAE(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                # recon_batch, mu, logvar = model(data_tensor)\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    z = reparameterize(mu, logvar)\n",
    "                    # print(z)\n",
    "                    recon_batch = model.decode(z)\n",
    "                    recon_batch = recon_batch.cpu().numpy()\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb7676a0-ab7d-4b82-bfbf-794bba403ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [02:07<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 0.00 | n1 0.059(0.000) | n100 0.254(0.000) | r20 0.217(0.000) | r50 0.395(0.000)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_stochasticVAE(test_data_tr, test_data_te, n_sampling=20)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86e996-0062-4257-8e54-5fa7012d1220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a959eb-8f48-4e62-982b-1e4b44a0b77f",
   "metadata": {},
   "source": [
    "# evaluate multi-VAE, Gumbel-VAE, Stochastic-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3fa5c05-5122-4d56-8fed-1205f1750616",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.2\n",
    "# beta = 1\n",
    "# beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@20\" : [[] for _ in range(n_sampling)],\n",
    "               # \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Faiss\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic-Faiss\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "                \n",
    "                n_batch_user = data.shape[0]\n",
    "                non_zero_indices = data.nonzero()\n",
    "\n",
    "                # if TOTAL_ANNEAL_STEPS > 0:\n",
    "                #     anneal = min(ANNEAL_CAP, \n",
    "                #                    1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                # else:\n",
    "                #     anneal = ANNEAL_CAP\n",
    "                \n",
    "                non_zero_indices = np.split(data.indices, data.indptr)\n",
    "\n",
    "                # encoding\n",
    "                start = time.perf_counter()\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "                t_encode = time.perf_counter() - start\n",
    "                \n",
    "                # decoding\n",
    "                start = time.perf_counter()\n",
    "                recon_batch = model.decode(mu)  \n",
    "                t_decode = time.perf_counter() - start\n",
    "                \n",
    "                recon_batch_clone = recon_batch.clone()\n",
    "                \n",
    "                start = time.perf_counter()\n",
    "                recon_batch = recon_batch.cpu()\n",
    "                recon_batch = recon_batch.numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "                t_to_cpu = time.perf_counter() - start\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                \n",
    "                # https://stackoverflow.com/questions/59338537/summarize-non-zero-values-in-a-scipy-matrix-by-axis\n",
    "                n_already_intaract_item = data.indptr[1:] - data.indptr[:-1]\n",
    "                max_n_already_intaract_item = int(np.max(n_already_intaract_item))\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    torch.manual_seed(l)\n",
    "                    \n",
    "                    # bluring z\n",
    "                    start = time.perf_counter()\n",
    "                    z_blurred = reparameterize(mu, logvar)\n",
    "                    t_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    # Stochastic multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch_blurred = model.decode(z_blurred)\n",
    "                    recon_batch_blurred = recon_batch_blurred.cpu()\n",
    "                    recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "                    # recon_batch_blurred[non_zero_indices] = -np.inf\n",
    "                    t_decode_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch_blurred, max_n_already_intaract_item+20)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_decode_blurred + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "                    \n",
    "                    # Stochastic multi-VAE with Faiss NNS\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "                    z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = gpu_index_flat.search(z_dash_blurred_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch_by_topk_indexes(topk_indexes, heldout_data, n_batch_user, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    \n",
    "                    # Stochastic multi-VAE with Faiss NNS(IVF)\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "                    z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = gpu_index_ivf.search(z_dash_blurred_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch_by_topk_indexes(topk_indexes, heldout_data, n_batch_user, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling\n",
    "                    start = time.perf_counter() \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_clone - beta * (-torch.rand(recon_batch_clone.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    # recon_batch_gumbel_sampled[non_zero_indices] = -np.inf\n",
    "                    t_gumbel_sampling = time.perf_counter() - start\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch_gumbel_sampled, max_n_already_intaract_item+20)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append((t_encode + t_decode + t_gumbel_sampling + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Faiss\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "                    z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = gpu_index_flat.search(z_dash_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Faiss(IVF)\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "                    z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = gpu_index_ivf.search(z_dash_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch, max_n_already_intaract_item+20)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append((t_encode + t_decode + t_to_cpu + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06e63bb1-8290-42e7-b289-2f0ae16c1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [2:07:03<00:00, 381.15s/it]t]\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "total_loss, metrics_dic = evaluate_stochastic(test_data_tr, test_data_te, n_sampling=100)\n",
    "# print('=' * 89)\n",
    "# print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "#         'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "# print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d322c94-357a-43ff-adee-b291f7460e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b02706-ff13-4a67-803f-4542e97c3aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.std(np.array(metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"])[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af0ced-aad0-4413-99f8-dbfebbf3c010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.std(np.array(metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"])[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5c41eeb2-82ea-4203-b640-9f07bd384b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13931001977971966"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(np.array(metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"])[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ef993f0f-3c5f-4897-9bba-23df57d56a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09817638518083249"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(np.array(metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"])[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a062481a-3d3c-4165-aa5f-e9c09a07a595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.0224)</td>\n",
       "      <td>0.3656(0.0277)</td>\n",
       "      <td>0.1759(0.0152)</td>\n",
       "      <td>0.8777(0.0328)</td>\n",
       "      <td>0.000284(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.0224)</td>\n",
       "      <td>0.3656(0.0277)</td>\n",
       "      <td>0.1759(0.0152)</td>\n",
       "      <td>0.8777(0.0328)</td>\n",
       "      <td>0.000037(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.0226)</td>\n",
       "      <td>0.3467(0.0277)</td>\n",
       "      <td>0.1672(0.0150)</td>\n",
       "      <td>0.8620(0.0345)</td>\n",
       "      <td>0.000031(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3078(0.0210)</td>\n",
       "      <td>0.3506(0.0276)</td>\n",
       "      <td>0.1634(0.0138)</td>\n",
       "      <td>0.8722(0.0334)</td>\n",
       "      <td>0.000269(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2948(0.0211)</td>\n",
       "      <td>0.3269(0.0267)</td>\n",
       "      <td>0.1591(0.0143)</td>\n",
       "      <td>0.8534(0.0354)</td>\n",
       "      <td>0.000277(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.2948(0.0211)</td>\n",
       "      <td>0.3269(0.0267)</td>\n",
       "      <td>0.1591(0.0143)</td>\n",
       "      <td>0.8534(0.0354)</td>\n",
       "      <td>0.000038(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.2893(0.0213)</td>\n",
       "      <td>0.3142(0.0266)</td>\n",
       "      <td>0.1538(0.0142)</td>\n",
       "      <td>0.8413(0.0365)</td>\n",
       "      <td>0.000031(0.000001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.0224)  0.3656(0.0277)   \n",
       "multi-VAE-Faiss                 0.3390(0.0224)  0.3656(0.0277)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.0226)  0.3467(0.0277)   \n",
       "multi-VAE-Gumbel                0.3078(0.0210)  0.3506(0.0276)   \n",
       "multi-VAE-Stochastic            0.2948(0.0211)  0.3269(0.0267)   \n",
       "multi-VAE-Stochastic-Faiss      0.2948(0.0211)  0.3269(0.0267)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.2893(0.0213)  0.3142(0.0266)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.0152)  0.8777(0.0328)   \n",
       "multi-VAE-Faiss                 0.1759(0.0152)  0.8777(0.0328)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.0150)  0.8620(0.0345)   \n",
       "multi-VAE-Gumbel                0.1634(0.0138)  0.8722(0.0334)   \n",
       "multi-VAE-Stochastic            0.1591(0.0143)  0.8534(0.0354)   \n",
       "multi-VAE-Stochastic-Faiss      0.1591(0.0143)  0.8534(0.0354)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1538(0.0142)  0.8413(0.0365)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000284(0.000003)  \n",
       "multi-VAE-Faiss                 0.000037(0.000001)  \n",
       "multi-VAE-Faiss-IVF             0.000031(0.000001)  \n",
       "multi-VAE-Gumbel                0.000269(0.000003)  \n",
       "multi-VAE-Stochastic            0.000277(0.000003)  \n",
       "multi-VAE-Stochastic-Faiss      0.000038(0.000001)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000031(0.000001)  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "        # # results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6e99047-04f5-41ad-a8c6-98cf26aa21ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.0023)</td>\n",
       "      <td>0.3656(0.0028)</td>\n",
       "      <td>0.1759(0.0015)</td>\n",
       "      <td>0.8777(0.0033)</td>\n",
       "      <td>0.000284(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.0023)</td>\n",
       "      <td>0.3656(0.0028)</td>\n",
       "      <td>0.1759(0.0015)</td>\n",
       "      <td>0.8777(0.0033)</td>\n",
       "      <td>0.000037(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.0023)</td>\n",
       "      <td>0.3467(0.0028)</td>\n",
       "      <td>0.1672(0.0015)</td>\n",
       "      <td>0.8620(0.0035)</td>\n",
       "      <td>0.000031(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3657(0.0022)</td>\n",
       "      <td>0.3927(0.0028)</td>\n",
       "      <td>0.1852(0.0015)</td>\n",
       "      <td>0.8999(0.0030)</td>\n",
       "      <td>0.000269(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.3651(0.0022)</td>\n",
       "      <td>0.3956(0.0028)</td>\n",
       "      <td>0.1865(0.0015)</td>\n",
       "      <td>0.9097(0.0029)</td>\n",
       "      <td>0.000277(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.3651(0.0022)</td>\n",
       "      <td>0.3956(0.0028)</td>\n",
       "      <td>0.1865(0.0015)</td>\n",
       "      <td>0.9097(0.0029)</td>\n",
       "      <td>0.000038(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.3551(0.0022)</td>\n",
       "      <td>0.3777(0.0028)</td>\n",
       "      <td>0.1786(0.0015)</td>\n",
       "      <td>0.8948(0.0031)</td>\n",
       "      <td>0.000031(0.000001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.0023)  0.3656(0.0028)   \n",
       "multi-VAE-Faiss                 0.3390(0.0023)  0.3656(0.0028)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.0023)  0.3467(0.0028)   \n",
       "multi-VAE-Gumbel                0.3657(0.0022)  0.3927(0.0028)   \n",
       "multi-VAE-Stochastic            0.3651(0.0022)  0.3956(0.0028)   \n",
       "multi-VAE-Stochastic-Faiss      0.3651(0.0022)  0.3956(0.0028)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.3551(0.0022)  0.3777(0.0028)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.0015)  0.8777(0.0033)   \n",
       "multi-VAE-Faiss                 0.1759(0.0015)  0.8777(0.0033)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.0015)  0.8620(0.0035)   \n",
       "multi-VAE-Gumbel                0.1852(0.0015)  0.8999(0.0030)   \n",
       "multi-VAE-Stochastic            0.1865(0.0015)  0.9097(0.0029)   \n",
       "multi-VAE-Stochastic-Faiss      0.1865(0.0015)  0.9097(0.0029)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1786(0.0015)  0.8948(0.0031)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000284(0.000003)  \n",
       "multi-VAE-Faiss                 0.000037(0.000001)  \n",
       "multi-VAE-Faiss-IVF             0.000031(0.000001)  \n",
       "multi-VAE-Gumbel                0.000269(0.000003)  \n",
       "multi-VAE-Stochastic            0.000277(0.000003)  \n",
       "multi-VAE-Stochastic-Faiss      0.000038(0.000001)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000031(0.000001)  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(np.quantile(metric_list, 0.8, axis=0)), np.std(np.quantile(metric_list, 0.8, axis=0)/np.sqrt(len(np.quantile(metric_list, 0.8, axis=0))))))\n",
    "    results_top20per.append(results)\n",
    "results_top20per = pd.DataFrame(results_top20per, columns=metric_names, index=method_names)\n",
    "print(\"Top 20%\")\n",
    "results_top20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5285addb-1baf-4241-8379-46c4ae9bfbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.0023)</td>\n",
       "      <td>0.3656(0.0028)</td>\n",
       "      <td>0.1759(0.0015)</td>\n",
       "      <td>0.8777(0.0033)</td>\n",
       "      <td>0.000284(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.0023)</td>\n",
       "      <td>0.3656(0.0028)</td>\n",
       "      <td>0.1759(0.0015)</td>\n",
       "      <td>0.8777(0.0033)</td>\n",
       "      <td>0.000037(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.0023)</td>\n",
       "      <td>0.3467(0.0028)</td>\n",
       "      <td>0.1672(0.0015)</td>\n",
       "      <td>0.8620(0.0035)</td>\n",
       "      <td>0.000031(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.2470(0.0018)</td>\n",
       "      <td>0.3065(0.0027)</td>\n",
       "      <td>0.1410(0.0012)</td>\n",
       "      <td>0.8453(0.0037)</td>\n",
       "      <td>0.000269(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2214(0.0017)</td>\n",
       "      <td>0.2560(0.0024)</td>\n",
       "      <td>0.1313(0.0013)</td>\n",
       "      <td>0.8022(0.0040)</td>\n",
       "      <td>0.000277(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.2214(0.0017)</td>\n",
       "      <td>0.2560(0.0024)</td>\n",
       "      <td>0.1313(0.0013)</td>\n",
       "      <td>0.8022(0.0040)</td>\n",
       "      <td>0.000038(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.2208(0.0018)</td>\n",
       "      <td>0.2494(0.0024)</td>\n",
       "      <td>0.1287(0.0013)</td>\n",
       "      <td>0.7928(0.0041)</td>\n",
       "      <td>0.000031(0.000001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.0023)  0.3656(0.0028)   \n",
       "multi-VAE-Faiss                 0.3390(0.0023)  0.3656(0.0028)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.0023)  0.3467(0.0028)   \n",
       "multi-VAE-Gumbel                0.2470(0.0018)  0.3065(0.0027)   \n",
       "multi-VAE-Stochastic            0.2214(0.0017)  0.2560(0.0024)   \n",
       "multi-VAE-Stochastic-Faiss      0.2214(0.0017)  0.2560(0.0024)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.2208(0.0018)  0.2494(0.0024)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.0015)  0.8777(0.0033)   \n",
       "multi-VAE-Faiss                 0.1759(0.0015)  0.8777(0.0033)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.0015)  0.8620(0.0035)   \n",
       "multi-VAE-Gumbel                0.1410(0.0012)  0.8453(0.0037)   \n",
       "multi-VAE-Stochastic            0.1313(0.0013)  0.8022(0.0040)   \n",
       "multi-VAE-Stochastic-Faiss      0.1313(0.0013)  0.8022(0.0040)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1287(0.0013)  0.7928(0.0041)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000284(0.000003)  \n",
       "multi-VAE-Faiss                 0.000037(0.000001)  \n",
       "multi-VAE-Faiss-IVF             0.000031(0.000001)  \n",
       "multi-VAE-Gumbel                0.000269(0.000003)  \n",
       "multi-VAE-Stochastic            0.000277(0.000003)  \n",
       "multi-VAE-Stochastic-Faiss      0.000038(0.000001)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000031(0.000001)  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bottom20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(np.quantile(metric_list, 0.2, axis=0)), np.std(np.quantile(metric_list, 0.2, axis=0)/np.sqrt(len(np.quantile(metric_list, 0.2, axis=0))))))\n",
    "    results_bottom20per.append(results)\n",
    "results_bottom20per = pd.DataFrame(results_bottom20per, columns=metric_names, index=method_names)\n",
    "print(\"Bottom 20%\")\n",
    "results_bottom20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db8a1a6e-4481-44c0-8a71-8d45ff37856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.0023)</td>\n",
       "      <td>0.3656(0.0028)</td>\n",
       "      <td>0.1759(0.0015)</td>\n",
       "      <td>0.8777(0.0033)</td>\n",
       "      <td>0.000284(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.0023)</td>\n",
       "      <td>0.3656(0.0028)</td>\n",
       "      <td>0.1759(0.0015)</td>\n",
       "      <td>0.8777(0.0033)</td>\n",
       "      <td>0.000037(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.0023)</td>\n",
       "      <td>0.3467(0.0028)</td>\n",
       "      <td>0.1672(0.0015)</td>\n",
       "      <td>0.8620(0.0035)</td>\n",
       "      <td>0.000031(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3019(0.0020)</td>\n",
       "      <td>0.3477(0.0028)</td>\n",
       "      <td>0.1626(0.0014)</td>\n",
       "      <td>0.8736(0.0034)</td>\n",
       "      <td>0.000269(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2879(0.0020)</td>\n",
       "      <td>0.3251(0.0026)</td>\n",
       "      <td>0.1585(0.0014)</td>\n",
       "      <td>0.8637(0.0035)</td>\n",
       "      <td>0.000277(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.2879(0.0020)</td>\n",
       "      <td>0.3251(0.0026)</td>\n",
       "      <td>0.1585(0.0014)</td>\n",
       "      <td>0.8637(0.0035)</td>\n",
       "      <td>0.000038(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.2834(0.0020)</td>\n",
       "      <td>0.3141(0.0026)</td>\n",
       "      <td>0.1534(0.0014)</td>\n",
       "      <td>0.8529(0.0036)</td>\n",
       "      <td>0.000031(0.000001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.0023)  0.3656(0.0028)   \n",
       "multi-VAE-Faiss                 0.3390(0.0023)  0.3656(0.0028)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.0023)  0.3467(0.0028)   \n",
       "multi-VAE-Gumbel                0.3019(0.0020)  0.3477(0.0028)   \n",
       "multi-VAE-Stochastic            0.2879(0.0020)  0.3251(0.0026)   \n",
       "multi-VAE-Stochastic-Faiss      0.2879(0.0020)  0.3251(0.0026)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.2834(0.0020)  0.3141(0.0026)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.0015)  0.8777(0.0033)   \n",
       "multi-VAE-Faiss                 0.1759(0.0015)  0.8777(0.0033)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.0015)  0.8620(0.0035)   \n",
       "multi-VAE-Gumbel                0.1626(0.0014)  0.8736(0.0034)   \n",
       "multi-VAE-Stochastic            0.1585(0.0014)  0.8637(0.0035)   \n",
       "multi-VAE-Stochastic-Faiss      0.1585(0.0014)  0.8637(0.0035)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1534(0.0014)  0.8529(0.0036)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000284(0.000003)  \n",
       "multi-VAE-Faiss                 0.000037(0.000001)  \n",
       "multi-VAE-Faiss-IVF             0.000031(0.000001)  \n",
       "multi-VAE-Gumbel                0.000269(0.000003)  \n",
       "multi-VAE-Stochastic            0.000277(0.000003)  \n",
       "multi-VAE-Stochastic-Faiss      0.000038(0.000001)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000031(0.000001)  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top50per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(np.quantile(metric_list, 0.5, axis=0)), np.std(np.quantile(metric_list, 0.5, axis=0)/np.sqrt(len(np.quantile(metric_list, 0.5, axis=0))))))\n",
    "    results_top50per.append(results)\n",
    "results_top50per = pd.DataFrame(results_top50per, columns=metric_names, index=method_names)\n",
    "print(\"Top 50%\")\n",
    "results_top50per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f59ae7-a146-451e-b98e-531ef67bfbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "354c947b-9f1f-4522-af57-89cdd460997e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-VAE\tndcg@20:0.3390019526235259\n",
      "multi-VAE\tndcg@100:0.427921597610972\n",
      "multi-VAE\trecall@20:0.36564431951977444\n",
      "multi-VAE\trecall@50:0.5309715538503059\n",
      "multi-VAE\tprecision@20:0.1758791678022434\n",
      "multi-VAE\tprecision@50:0.11315272909866547\n",
      "multi-VAE\thit_rate@20:0.8777346145982416\n",
      "multi-VAE\thit_rate@100:0.9597219382539358\n",
      "multi-VAE\tprediction_time:0.025096308455737015\n",
      "multi-VAE-Gumbel\tndcg@20:0.24272735428304837\n",
      "multi-VAE-Gumbel\tndcg@100:0.3353621752809509\n",
      "multi-VAE-Gumbel\trecall@20:0.3126150629968296\n",
      "multi-VAE-Gumbel\trecall@50:0.482291047211655\n",
      "multi-VAE-Gumbel\tprecision@20:0.11779002481942416\n",
      "multi-VAE-Gumbel\tprecision@50:0.08251114230331066\n",
      "multi-VAE-Gumbel\thit_rate@20:0.9085217746882028\n",
      "multi-VAE-Gumbel\thit_rate@100:0.9757963606624412\n",
      "multi-VAE-Gumbel\tprediction_time:0.02695225974983259\n",
      "multi-VAE-Stochastic\tndcg@20:0.39673035017837077\n",
      "multi-VAE-Stochastic\tndcg@100:0.4738134190134512\n",
      "multi-VAE-Stochastic\trecall@20:0.42287913535252136\n",
      "multi-VAE-Stochastic\trecall@50:0.5839383764597308\n",
      "multi-VAE-Stochastic\tprecision@20:0.19731323060719935\n",
      "multi-VAE-Stochastic\tprecision@50:0.12172169247350062\n",
      "multi-VAE-Stochastic\thit_rate@20:0.9205929257820487\n",
      "multi-VAE-Stochastic\thit_rate@100:0.9745982416683705\n",
      "multi-VAE-Stochastic\tprediction_time:0.02917656848584011\n"
     ]
    }
   ],
   "source": [
    "for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            # if metric_name in (\"prediction_time\"):\n",
    "                # print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.9, axis=1))))\n",
    "            # if metric_name in (\"hit_rate@20\",\"hit_rate@50\"):\n",
    "                # print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.5, axis=1))))\n",
    "            print(\"{}\\t{}:{}\".format(method_name, metric_name, np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.99, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4583a77-938f-430a-8f37-bf7ec11962e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3307657c-75d5-485d-9a0a-e2893ad19eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.2\n",
    "# beta = 1\n",
    "# beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@20\" : [[] for _ in range(n_sampling)],\n",
    "               # \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Faiss\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel-k\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-k\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-Faiss\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-Faiss-k\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "                \n",
    "                n_batch_user = data.shape[0]\n",
    "                non_zero_indices = data.nonzero()\n",
    "\n",
    "                # if TOTAL_ANNEAL_STEPS > 0:\n",
    "                #     anneal = min(ANNEAL_CAP, \n",
    "                #                    1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                # else:\n",
    "                #     anneal = ANNEAL_CAP\n",
    "                \n",
    "                non_zero_indices = np.split(data.indices, data.indptr)\n",
    "\n",
    "                # encoding\n",
    "                start = time.perf_counter()\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "                t_encode = time.perf_counter() - start\n",
    "                \n",
    "                # decoding\n",
    "                start = time.perf_counter()\n",
    "                recon_batch = model.decode(mu)  \n",
    "                t_decode = time.perf_counter() - start\n",
    "                \n",
    "                recon_batch_clone = recon_batch.clone()\n",
    "                \n",
    "                start = time.perf_counter()\n",
    "                recon_batch = recon_batch.cpu()\n",
    "                recon_batch = recon_batch.numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "                t_to_cpu = time.perf_counter() - start\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                \n",
    "                # https://stackoverflow.com/questions/59338537/summarize-non-zero-values-in-a-scipy-matrix-by-axis\n",
    "                n_already_intaract_item = data.indptr[1:] - data.indptr[:-1]\n",
    "                max_n_already_intaract_item = int(np.max(n_already_intaract_item))\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    torch.manual_seed(l)\n",
    "                    \n",
    "#                     # bluring z\n",
    "#                     start = time.perf_counter()\n",
    "#                     z_blurred = reparameterize(mu, logvar)\n",
    "#                     t_blurred = time.perf_counter() - start\n",
    "                    \n",
    "#                     # Stochastic multi-VAE\n",
    "#                     start = time.perf_counter()\n",
    "#                     recon_batch_blurred = model.decode(z_blurred)\n",
    "#                     recon_batch_blurred = recon_batch_blurred.cpu()\n",
    "#                     recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "#                     # recon_batch_blurred[non_zero_indices] = -np.inf\n",
    "#                     t_decode_blurred = time.perf_counter() - start\n",
    "                    \n",
    "#                     start = time.perf_counter()\n",
    "#                     topk_indexes = metric.get_idx_topk(recon_batch_blurred, max_n_already_intaract_item+20)\n",
    "#                     t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_decode_blurred + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "#                     # Stochastic multi-VAE (add noise for each position in a ranking)\n",
    "#                     start = time.perf_counter()\n",
    "#                     topk_indexes = np.empty((n_batch_user,0),dtype=int)\n",
    "#                     data_tmp = data.copy()\n",
    "#                     for i in range(20):\n",
    "#                         z_blurred = reparameterize(mu, logvar)\n",
    "#                         recon_batch_blurred = model.decode(z_blurred)\n",
    "#                         recon_batch_blurred = recon_batch_blurred.cpu()\n",
    "#                         recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "#                         recon_batch_blurred[data_tmp.nonzero()] = -np.inf\n",
    "#                         topk_indexes_tmp = metric.get_idx_topk(recon_batch_blurred, 1)\n",
    "#                         # print(topk_indexes_tmp[0])\n",
    "#                         topk_indexes = np.concatenate((topk_indexes, topk_indexes_tmp), axis=1)\n",
    "#                         data_tmp += coo_matrix((np.ones(n_batch_user, dtype=np.int8), (np.arange(n_batch_user), topk_indexes_tmp.flatten())), shape=data_tmp.shape)\n",
    "#                         # topk_indexes = np.array([list(OrderedDict.fromkeys(row))[:i+1] for row in topk_indexes])\n",
    "#                         # print(topk_indexes[0])\n",
    "#                         # print(\"---\")\n",
    "#                     t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,20+1), dtype=float)), axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     # recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_decode_blurred + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling\n",
    "                    start = time.perf_counter() \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    beta = 0.2\n",
    "                    recon_batch_gumbel_sampled = recon_batch_clone - beta * (-torch.rand(recon_batch_clone.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    # recon_batch_gumbel_sampled[non_zero_indices] = -np.inf\n",
    "                    t_gumbel_sampling = time.perf_counter() - start\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch_gumbel_sampled, max_n_already_intaract_item+20)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append((t_encode + t_decode + t_gumbel_sampling + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling (add noise for each position in a ranking)\n",
    "                    start = time.perf_counter() \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    topk_indexes = np.empty((n_batch_user,0),dtype=int)\n",
    "                    data_tmp = data.copy()\n",
    "                    for i in range(20):\n",
    "                        beta = i * 0.02\n",
    "                        recon_batch_gumbel_sampled = recon_batch_clone - beta * (-torch.rand(recon_batch_clone.shape, device=device).log()).log()\n",
    "                        recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "                        recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                        recon_batch_gumbel_sampled[data_tmp.nonzero()] = -np.inf\n",
    "                        # topk_indexes_tmp = metric.get_idx_topk(recon_batch_gumbel_sampled, i+1)\n",
    "                        topk_indexes_tmp = metric.get_idx_topk(recon_batch_gumbel_sampled, 1)\n",
    "                        topk_indexes = np.concatenate((topk_indexes, topk_indexes_tmp), axis=1) \n",
    "                        data_tmp += coo_matrix((np.ones(n_batch_user, dtype=np.int8), (np.arange(n_batch_user), topk_indexes_tmp.flatten())), shape=data_tmp.shape)\n",
    "                        # topk_indexes = np.array([list(OrderedDict.fromkeys(row))[:i+1] for row in topk_indexes])\n",
    "                        # print(topk_indexes[0])\n",
    "                        \n",
    "                    # recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "                    # recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    \n",
    "                    # t_gumbel_sampling = time.perf_counter() - start\n",
    "                    \n",
    "                    # start = time.perf_counter()\n",
    "                    # topk_indexes = metric.get_idx_topk(recon_batch_gumbel_sampled, max_n_already_intaract_item+20)\n",
    "                        \n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,21), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Gumbel-k\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-k\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-k\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-k\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel-k\"][\"prediction_time\"][l].append((t_encode + t_decode + t_gumbel_sampling + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "#                     # multi-VAE + Faiss\n",
    "#                     start = time.perf_counter()\n",
    "#                     z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "#                     z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "#                     topk_scores, topk_indexes = gpu_index_flat.search(z_dash_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "#                     # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "#                     t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "#                     # create dummy recon_batch which include the corresponded score to top-k items\n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "#                     # multi-VAE + Faiss(IVF)\n",
    "#                     start = time.perf_counter()\n",
    "#                     z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "#                     z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "#                     topk_scores, topk_indexes = gpu_index_ivf.search(z_dash_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "#                     # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "#                     t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "#                     # create dummy recon_batch which include the corresponded score to top-k items\n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "#                     metrics_dic[\"multi-VAE-Faiss-IVF\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss-IVF\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss-IVF\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss-IVF\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss-IVF\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch, max_n_already_intaract_item+20)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append((t_encode + t_decode + t_to_cpu + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35511ef0-9ca7-4eb1-bc3d-67922801ed0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [05:48<00:00, 17.43s/it]\n"
     ]
    }
   ],
   "source": [
    "total_loss, metrics_dic = evaluate_stochastic(test_data_tr, test_data_te, n_sampling=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3916663d-73ec-4d8b-a73a-386ab11589a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000281(0.000030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000034(0.000007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.2258)</td>\n",
       "      <td>0.3467(0.2772)</td>\n",
       "      <td>0.1672(0.1499)</td>\n",
       "      <td>0.8620(0.3449)</td>\n",
       "      <td>0.000028(0.000004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3078(0.2102)</td>\n",
       "      <td>0.3508(0.2763)</td>\n",
       "      <td>0.1635(0.1382)</td>\n",
       "      <td>0.8720(0.3340)</td>\n",
       "      <td>0.000266(0.000030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel-k</th>\n",
       "      <td>0.3080(0.2104)</td>\n",
       "      <td>0.3507(0.2762)</td>\n",
       "      <td>0.1636(0.1383)</td>\n",
       "      <td>0.8718(0.3343)</td>\n",
       "      <td>0.003902(0.000056)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2946(0.2107)</td>\n",
       "      <td>0.3267(0.2664)</td>\n",
       "      <td>0.1591(0.1428)</td>\n",
       "      <td>0.8537(0.3534)</td>\n",
       "      <td>0.000274(0.000030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-k</th>\n",
       "      <td>0.3001(0.2085)</td>\n",
       "      <td>0.3408(0.2731)</td>\n",
       "      <td>0.1598(0.1368)</td>\n",
       "      <td>0.8655(0.3412)</td>\n",
       "      <td>0.004041(0.000058)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ndcg@20       recall@20    precision@20  \\\n",
       "multi-VAE               0.3390(0.2239)  0.3656(0.2772)  0.1759(0.1524)   \n",
       "multi-VAE-Faiss         0.3390(0.2239)  0.3656(0.2772)  0.1759(0.1524)   \n",
       "multi-VAE-Faiss-IVF     0.3273(0.2258)  0.3467(0.2772)  0.1672(0.1499)   \n",
       "multi-VAE-Gumbel        0.3078(0.2102)  0.3508(0.2763)  0.1635(0.1382)   \n",
       "multi-VAE-Gumbel-k      0.3080(0.2104)  0.3507(0.2762)  0.1636(0.1383)   \n",
       "multi-VAE-Stochastic    0.2946(0.2107)  0.3267(0.2664)  0.1591(0.1428)   \n",
       "multi-VAE-Stochastic-k  0.3001(0.2085)  0.3408(0.2731)  0.1598(0.1368)   \n",
       "\n",
       "                           hit_rate@20     prediction_time  \n",
       "multi-VAE               0.8777(0.3276)  0.000281(0.000030)  \n",
       "multi-VAE-Faiss         0.8777(0.3276)  0.000034(0.000007)  \n",
       "multi-VAE-Faiss-IVF     0.8620(0.3449)  0.000028(0.000004)  \n",
       "multi-VAE-Gumbel        0.8720(0.3340)  0.000266(0.000030)  \n",
       "multi-VAE-Gumbel-k      0.8718(0.3343)  0.003902(0.000056)  \n",
       "multi-VAE-Stochastic    0.8537(0.3534)  0.000274(0.000030)  \n",
       "multi-VAE-Stochastic-k  0.8655(0.3412)  0.004041(0.000058)  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metrics_dic[method_name][metric_name]), np.std(metrics_dic[method_name][metric_name])))\n",
    "        else:\n",
    "        # # results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metrics_dic[method_name][metric_name]), np.std(metrics_dic[method_name][metric_name])))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "806dbb86-447b-4bdf-890a-71af59884aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000276(0.000030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3084(0.2113)</td>\n",
       "      <td>0.3509(0.2772)</td>\n",
       "      <td>0.1632(0.1379)</td>\n",
       "      <td>0.8723(0.3338)</td>\n",
       "      <td>0.000273(0.000116)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel-k</th>\n",
       "      <td>0.1885(0.1911)</td>\n",
       "      <td>0.1579(0.2049)</td>\n",
       "      <td>0.0635(0.0609)</td>\n",
       "      <td>0.6830(0.4653)</td>\n",
       "      <td>0.004033(0.000133)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ndcg@20       recall@20    precision@20  \\\n",
       "multi-VAE           0.3390(0.2239)  0.3656(0.2772)  0.1759(0.1524)   \n",
       "multi-VAE-Gumbel    0.3084(0.2113)  0.3509(0.2772)  0.1632(0.1379)   \n",
       "multi-VAE-Gumbel-k  0.1885(0.1911)  0.1579(0.2049)  0.0635(0.0609)   \n",
       "\n",
       "                       hit_rate@20     prediction_time  \n",
       "multi-VAE           0.8777(0.3276)  0.000276(0.000030)  \n",
       "multi-VAE-Gumbel    0.8723(0.3338)  0.000273(0.000116)  \n",
       "multi-VAE-Gumbel-k  0.6830(0.4653)  0.004033(0.000133)  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metrics_dic[method_name][metric_name]), np.std(metrics_dic[method_name][metric_name])))\n",
    "        else:\n",
    "        # # results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metrics_dic[method_name][metric_name]), np.std(metrics_dic[method_name][metric_name])))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "445fc2ce-d663-48c2-8d6a-a3aa6ab9387f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000281(0.000030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000034(0.000007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.2258)</td>\n",
       "      <td>0.3467(0.2772)</td>\n",
       "      <td>0.1672(0.1499)</td>\n",
       "      <td>0.8620(0.3449)</td>\n",
       "      <td>0.000028(0.000004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3647(0.2189)</td>\n",
       "      <td>0.3919(0.2782)</td>\n",
       "      <td>0.1849(0.1485)</td>\n",
       "      <td>0.8990(0.3008)</td>\n",
       "      <td>0.000266(0.000030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel-k</th>\n",
       "      <td>0.3655(0.2193)</td>\n",
       "      <td>0.3924(0.2780)</td>\n",
       "      <td>0.1851(0.1485)</td>\n",
       "      <td>0.8999(0.2994)</td>\n",
       "      <td>0.003902(0.000056)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.3638(0.2132)</td>\n",
       "      <td>0.3945(0.2728)</td>\n",
       "      <td>0.1861(0.1505)</td>\n",
       "      <td>0.9100(0.2851)</td>\n",
       "      <td>0.000274(0.000030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-k</th>\n",
       "      <td>0.3597(0.2143)</td>\n",
       "      <td>0.3932(0.2785)</td>\n",
       "      <td>0.1814(0.1439)</td>\n",
       "      <td>0.9059(0.2913)</td>\n",
       "      <td>0.004041(0.000058)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ndcg@20       recall@20    precision@20  \\\n",
       "multi-VAE               0.3390(0.2239)  0.3656(0.2772)  0.1759(0.1524)   \n",
       "multi-VAE-Faiss         0.3390(0.2239)  0.3656(0.2772)  0.1759(0.1524)   \n",
       "multi-VAE-Faiss-IVF     0.3273(0.2258)  0.3467(0.2772)  0.1672(0.1499)   \n",
       "multi-VAE-Gumbel        0.3647(0.2189)  0.3919(0.2782)  0.1849(0.1485)   \n",
       "multi-VAE-Gumbel-k      0.3655(0.2193)  0.3924(0.2780)  0.1851(0.1485)   \n",
       "multi-VAE-Stochastic    0.3638(0.2132)  0.3945(0.2728)  0.1861(0.1505)   \n",
       "multi-VAE-Stochastic-k  0.3597(0.2143)  0.3932(0.2785)  0.1814(0.1439)   \n",
       "\n",
       "                           hit_rate@20     prediction_time  \n",
       "multi-VAE               0.8777(0.3276)  0.000281(0.000030)  \n",
       "multi-VAE-Faiss         0.8777(0.3276)  0.000034(0.000007)  \n",
       "multi-VAE-Faiss-IVF     0.8620(0.3449)  0.000028(0.000004)  \n",
       "multi-VAE-Gumbel        0.8990(0.3008)  0.000266(0.000030)  \n",
       "multi-VAE-Gumbel-k      0.8999(0.2994)  0.003902(0.000056)  \n",
       "multi-VAE-Stochastic    0.9100(0.2851)  0.000274(0.000030)  \n",
       "multi-VAE-Stochastic-k  0.9059(0.2913)  0.004041(0.000058)  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)))\n",
    "        else:\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(np.quantile(metric_list, 0.8, axis=0)), np.std(np.quantile(metric_list, 0.8, axis=0))))\n",
    "            # results.append(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.8, axis=0)))\n",
    "    results_top20per.append(results)\n",
    "results_top20per = pd.DataFrame(results_top20per, columns=metric_names, index=method_names)\n",
    "print(\"Top 20%\")\n",
    "results_top20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e2fc7e-c671-4dd7-8ab2-b4caabd353f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bdf4c1fe-1caf-41ee-9964-7dd502e486a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta = 0.2\n",
    "# beta = 1\n",
    "# beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic(data_tr, data_te, n_sampling=1, k=20):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@{}\".format(k):[[] for _ in range(n_sampling)],\n",
    "               # \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@{}\".format(k):[[] for _ in range(n_sampling)],\n",
    "               # \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@{}\".format(k):[[] for _ in range(n_sampling)],\n",
    "               # \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@{}\".format(k) : [[] for _ in range(n_sampling)],\n",
    "               # \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Faiss\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-k\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-k\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-Faiss\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-Faiss-k\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    n_intaracts = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "                \n",
    "                n_batch_user = data.shape[0]\n",
    "                non_zero_indices = data.nonzero()\n",
    "\n",
    "                # if TOTAL_ANNEAL_STEPS > 0:\n",
    "                #     anneal = min(ANNEAL_CAP, \n",
    "                #                    1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                # else:\n",
    "                #     anneal = ANNEAL_CAP\n",
    "                \n",
    "                non_zero_indices = np.split(data.indices, data.indptr)\n",
    "\n",
    "                # encoding\n",
    "                start = time.perf_counter()\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "                t_encode = time.perf_counter() - start\n",
    "                \n",
    "                # decoding\n",
    "                start = time.perf_counter()\n",
    "                recon_batch = model.decode(mu)  \n",
    "                t_decode = time.perf_counter() - start\n",
    "                \n",
    "                recon_batch_clone = recon_batch.clone()\n",
    "                \n",
    "                start = time.perf_counter()\n",
    "                recon_batch = recon_batch.cpu()\n",
    "                recon_batch = recon_batch.numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "                t_to_cpu = time.perf_counter() - start\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                \n",
    "                # https://stackoverflow.com/questions/59338537/summarize-non-zero-values-in-a-scipy-matrix-by-axis\n",
    "                n_already_intaract_item = data.indptr[1:] - data.indptr[:-1]\n",
    "                max_n_already_intaract_item = int(np.max(n_already_intaract_item))\n",
    "                \n",
    "                # n_intaracts.extend(n_already_intaract_item)\n",
    "                n_intaracts.extend(logvar.cpu())\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    torch.manual_seed(l)\n",
    "                    \n",
    "                    # bluring z\n",
    "                    start = time.perf_counter()\n",
    "                    z_blurred = reparameterize(mu, logvar)\n",
    "                    t_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    # Stochastic multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch_blurred = model.decode(z_blurred)\n",
    "                    recon_batch_blurred = recon_batch_blurred.cpu()\n",
    "                    recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "                    # recon_batch_blurred[non_zero_indices] = -np.inf\n",
    "                    t_decode_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch_blurred, max_n_already_intaract_item+k)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+k+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@{}\".format(k)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@{}\".format(k)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@{}\".format(k)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@{}\".format(k)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_decode_blurred + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "#                     # Stochastic multi-VAE (add noise for each position in a ranking)\n",
    "#                     start = time.perf_counter()\n",
    "#                     topk_indexes = np.empty((n_batch_user,0),dtype=int)\n",
    "#                     for i in range(20):\n",
    "#                         z_blurred = reparameterize(mu, logvar)\n",
    "#                         recon_batch_blurred = model.decode(z_blurred)\n",
    "#                         recon_batch_blurred = recon_batch_blurred.cpu()\n",
    "#                         recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "#                         recon_batch_blurred[data.nonzero()] = -np.inf\n",
    "#                         topk_indexes_tmp = metric.get_idx_topk(recon_batch_blurred, i+1)\n",
    "#                         # print(topk_indexes_tmp[0])\n",
    "#                         topk_indexes = np.concatenate((topk_indexes, topk_indexes_tmp), axis=1)\n",
    "#                         topk_indexes = np.array([list(OrderedDict.fromkeys(row))[:i+1] for row in topk_indexes])\n",
    "#                         # print(topk_indexes[0])\n",
    "#                         # print(\"---\")\n",
    "#                     t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,20+1), dtype=float)), axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     # recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-k\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_decode_blurred + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "                    \n",
    "#                     # Stochastic multi-VAE with Faiss NNS\n",
    "#                     start = time.perf_counter()\n",
    "#                     z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "#                     z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(n_batch_user, device=device)))\n",
    "#                     topk_scores, topk_indexes = gpu_index_flat.search(z_dash_blurred_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "#                     # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "#                     t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "#                     # create dummy recon_batch which include the corresponded score to top-k items\n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch_by_topk_indexes(topk_indexes, heldout_data, n_batch_user, 20))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "#                     # Stochastic multi-VAE with Faiss NNS (add noise for each position in a ranking)\n",
    "#                     start = time.perf_counter()\n",
    "#                     topk_indexes = [[] for _ in range(n_batch_user)] \n",
    "#                     for i in range(20):\n",
    "#                         z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "#                         z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(n_batch_user, device=device)))\n",
    "#                         rank_i_scores, topk_indexes_tmp = gpu_index_flat.search(z_dash_blurred_wi_constant.cpu(), max_n_already_intaract_item+i+1) # perform searching on GPU\n",
    "#                         topk_indexes_tmp = [[elem for elem in a if elem not in b] for a, b in zip(topk_indexes_tmp, non_zero_indices)]\n",
    "#                         # if i == 0:\n",
    "#                         #     topk_indexes = topk_indexes_tmp\n",
    "#                         # else:\n",
    "#                         topk_indexes = [a + b for a, b in zip(topk_indexes, topk_indexes_tmp)]\n",
    "#                         topk_indexes = [list(OrderedDict.fromkeys(row))[:i+1] for row in topk_indexes]\n",
    "                        \n",
    "#                     topk_indexes = np.array(topk_indexes)\n",
    "#                     t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "#                     # create dummy recon_batch which include the corresponded score to top-k items\n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,21), dtype=float)), axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch_by_topk_indexes(topk_indexes, heldout_data, n_batch_user, 20))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss-k\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss-k\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss-k\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss-k\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Stochastic-Faiss-k\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # Stochastic multi-VAE with Faiss NNS(IVF)\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "                    z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = gpu_index_ivf.search(z_dash_blurred_wi_constant.cpu(), max_n_already_intaract_item+k) # perform searching on GPU\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch_by_topk_indexes(topk_indexes, heldout_data, n_batch_user, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"ndcg@{}\".format(k)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"recall@{}\".format(k)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"precision@{}\".format(k)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"hit_rate@{}\".format(k)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling\n",
    "                    start = time.perf_counter() \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_clone - beta * (-torch.rand(recon_batch_clone.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    # recon_batch_gumbel_sampled[non_zero_indices] = -np.inf\n",
    "                    t_gumbel_sampling = time.perf_counter() - start\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch_gumbel_sampled, max_n_already_intaract_item+k)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+k+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@{}\".format(k)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@{}\".format(k)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@{}\".format(k)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@{}\".format(k)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append((t_encode + t_decode + t_gumbel_sampling + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "#                     # multi-VAE + Gumbel Max Sampling (add noise for each position in a ranking)\n",
    "#                     start = time.perf_counter() \n",
    "#                     # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "#                     # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "#                     topk_indexes = np.empty((n_batch_user,0),dtype=int)\n",
    "#                     for i in range(20):\n",
    "#                         recon_batch_gumbel_sampled = recon_batch_clone - beta * (-torch.rand(recon_batch_clone.shape, device=device).log()).log()\n",
    "#                         recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "#                         recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "#                         recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "#                         topk_indexes_tmp = metric.get_idx_topk(recon_batch_gumbel_sampled, 1)\n",
    "#                         topk_indexes = np.concatenate((topk_indexes, topk_indexes_tmp), axis=1)\n",
    "#                         # topk_indexes = np.array([list(OrderedDict.fromkeys(row))[:i+1] for row in topk_indexes])\n",
    "#                         # topk_indexes = np.array([np.unique(row)[:i+1] for row in topk_indexes])\n",
    "#                         # print(topk_indexes[0])\n",
    "                        \n",
    "#                     # todo \n",
    "#                     # 各行にどうやってappendしていくか\n",
    "#                     # 前に選んだitem indexをどうやってtopkから除くか\n",
    "                        \n",
    "#                     # recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "#                     # recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    \n",
    "#                     # t_gumbel_sampling = time.perf_counter() - start\n",
    "                    \n",
    "#                     # start = time.perf_counter()\n",
    "#                     # topk_indexes = metric.get_idx_topk(recon_batch_gumbel_sampled, max_n_already_intaract_item+20)\n",
    "                        \n",
    "#                     t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,21), dtype=float)), axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "#                     metrics_dic[\"multi-VAE-Gumbel-k\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel-k\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel-k\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel-k\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel-k\"][\"prediction_time\"][l].append((t_encode + t_decode + t_gumbel_sampling + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "#                     # multi-VAE + Faiss\n",
    "#                     start = time.perf_counter()\n",
    "#                     z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "#                     z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "#                     topk_scores, topk_indexes = gpu_index_flat.search(z_dash_wi_constant.cpu(), max_n_already_intaract_item+k) # perform searching on GPU\n",
    "#                     # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "#                     t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "#                     # create dummy recon_batch which include the corresponded score to top-k items\n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"ndcg@{}\".format(k)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"recall@{}\".format(k)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"precision@{}\".format(k)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"hit_rate@{}\".format(k)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Faiss\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Faiss(IVF)\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "                    z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = gpu_index_ivf.search(z_dash_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"ndcg@{}\".format(k)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"recall@{}\".format(k)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"precision@{}\".format(k)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"hit_rate@{}\".format(k)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch, max_n_already_intaract_item+k)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+k+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@{}\".format(k)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@{}\".format(k)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@{}\".format(k)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@{}\".format(k)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, k))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append((t_encode + t_decode + t_to_cpu + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic, n_intaracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96f41948-7664-41bc-a64c-8810d01a8c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [09:18<00:00, 27.94s/it]\n"
     ]
    }
   ],
   "source": [
    "total_loss, metrics_dic, n_intaracts = evaluate_stochastic(test_data_tr, test_data_te, n_sampling=10, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a96f603-6858-479e-9e3a-3f82066b6b33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.0224)</td>\n",
       "      <td>0.3656(0.0277)</td>\n",
       "      <td>0.1759(0.0152)</td>\n",
       "      <td>0.8777(0.0328)</td>\n",
       "      <td>0.000261(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.0226)</td>\n",
       "      <td>0.3467(0.0277)</td>\n",
       "      <td>0.1672(0.0150)</td>\n",
       "      <td>0.8620(0.0345)</td>\n",
       "      <td>0.000028(0.000000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3078(0.0210)</td>\n",
       "      <td>0.3506(0.0276)</td>\n",
       "      <td>0.1634(0.0138)</td>\n",
       "      <td>0.8722(0.0334)</td>\n",
       "      <td>0.000256(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2948(0.0211)</td>\n",
       "      <td>0.3269(0.0267)</td>\n",
       "      <td>0.1591(0.0143)</td>\n",
       "      <td>0.8534(0.0354)</td>\n",
       "      <td>0.000264(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.2893(0.0213)</td>\n",
       "      <td>0.3142(0.0266)</td>\n",
       "      <td>0.1538(0.0142)</td>\n",
       "      <td>0.8413(0.0365)</td>\n",
       "      <td>0.000028(0.000000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.0224)  0.3656(0.0277)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.0226)  0.3467(0.0277)   \n",
       "multi-VAE-Gumbel                0.3078(0.0210)  0.3506(0.0276)   \n",
       "multi-VAE-Stochastic            0.2948(0.0211)  0.3269(0.0267)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.2893(0.0213)  0.3142(0.0266)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.0152)  0.8777(0.0328)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.0150)  0.8620(0.0345)   \n",
       "multi-VAE-Gumbel                0.1634(0.0138)  0.8722(0.0334)   \n",
       "multi-VAE-Stochastic            0.1591(0.0143)  0.8534(0.0354)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1538(0.0142)  0.8413(0.0365)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000261(0.000003)  \n",
       "multi-VAE-Faiss-IVF             0.000028(0.000000)  \n",
       "multi-VAE-Gumbel                0.000256(0.000003)  \n",
       "multi-VAE-Stochastic            0.000264(0.000003)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000028(0.000000)  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "        # # results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "330abac3-6d25-4ea7-9055-aec64da87b34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.0708)</td>\n",
       "      <td>0.3656(0.0877)</td>\n",
       "      <td>0.1759(0.0482)</td>\n",
       "      <td>0.8777(0.1036)</td>\n",
       "      <td>0.000278(0.000010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.0714)</td>\n",
       "      <td>0.3467(0.0877)</td>\n",
       "      <td>0.1672(0.0474)</td>\n",
       "      <td>0.8620(0.1091)</td>\n",
       "      <td>0.000028(0.000001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3074(0.0664)</td>\n",
       "      <td>0.3505(0.0873)</td>\n",
       "      <td>0.1633(0.0436)</td>\n",
       "      <td>0.8717(0.1058)</td>\n",
       "      <td>0.000264(0.000009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2949(0.0667)</td>\n",
       "      <td>0.3265(0.0842)</td>\n",
       "      <td>0.1591(0.0452)</td>\n",
       "      <td>0.8537(0.1118)</td>\n",
       "      <td>0.000273(0.000009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.2894(0.0673)</td>\n",
       "      <td>0.3140(0.0841)</td>\n",
       "      <td>0.1538(0.0450)</td>\n",
       "      <td>0.8416(0.1155)</td>\n",
       "      <td>0.000035(0.000025)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.0708)  0.3656(0.0877)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.0714)  0.3467(0.0877)   \n",
       "multi-VAE-Gumbel                0.3074(0.0664)  0.3505(0.0873)   \n",
       "multi-VAE-Stochastic            0.2949(0.0667)  0.3265(0.0842)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.2894(0.0673)  0.3140(0.0841)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.0482)  0.8777(0.1036)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.0474)  0.8620(0.1091)   \n",
       "multi-VAE-Gumbel                0.1633(0.0436)  0.8717(0.1058)   \n",
       "multi-VAE-Stochastic            0.1591(0.0452)  0.8537(0.1118)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1538(0.0450)  0.8416(0.1155)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000278(0.000010)  \n",
       "multi-VAE-Faiss-IVF             0.000028(0.000001)  \n",
       "multi-VAE-Gumbel                0.000264(0.000009)  \n",
       "multi-VAE-Stochastic            0.000273(0.000009)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000035(0.000025)  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "        # # results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6d3df96f-a320-4863-b69f-dd8f3fc55fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6365446\n",
      "0.1462994322180748\n",
      "0.6981931924819946\n",
      "0.9902460813522339\n"
     ]
    }
   ],
   "source": [
    "# for i,a in enumerate(n_intaracts):\n",
    "#     n_intaracts[i] = a.cpu()\n",
    "# n_intaracts = np.concatenate(n_intaracts)\n",
    "n_intaracts = np.exp(n_intaracts)\n",
    "print(np.mean(n_intaracts))\n",
    "print(np.quantile(n_intaracts, 0.1))\n",
    "print(np.quantile(n_intaracts, 0.5))\n",
    "print(np.quantile(n_intaracts, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38facb7b-e397-4f61-a941-8986bca99789",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9782"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_intaracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d98715d9-2f97-4ff8-aeae-ab368d4fe17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.2323(0.0290)</td>\n",
       "      <td>0.4451(0.0457)</td>\n",
       "      <td>0.0308(0.0033)</td>\n",
       "      <td>0.5214(0.0500)</td>\n",
       "      <td>0.000261(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.2234(0.0292)</td>\n",
       "      <td>0.4191(0.0456)</td>\n",
       "      <td>0.0288(0.0032)</td>\n",
       "      <td>0.4902(0.0500)</td>\n",
       "      <td>0.000028(0.000000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.2180(0.0278)</td>\n",
       "      <td>0.4322(0.0456)</td>\n",
       "      <td>0.0299(0.0032)</td>\n",
       "      <td>0.5063(0.0500)</td>\n",
       "      <td>0.000256(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.1928(0.0271)</td>\n",
       "      <td>0.3806(0.0448)</td>\n",
       "      <td>0.0264(0.0032)</td>\n",
       "      <td>0.4514(0.0498)</td>\n",
       "      <td>0.000264(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.1877(0.0271)</td>\n",
       "      <td>0.3673(0.0446)</td>\n",
       "      <td>0.0254(0.0031)</td>\n",
       "      <td>0.4347(0.0496)</td>\n",
       "      <td>0.000028(0.000000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.2323(0.0290)  0.4451(0.0457)   \n",
       "multi-VAE-Faiss-IVF             0.2234(0.0292)  0.4191(0.0456)   \n",
       "multi-VAE-Gumbel                0.2180(0.0278)  0.4322(0.0456)   \n",
       "multi-VAE-Stochastic            0.1928(0.0271)  0.3806(0.0448)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1877(0.0271)  0.3673(0.0446)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.0308(0.0033)  0.5214(0.0500)   \n",
       "multi-VAE-Faiss-IVF             0.0288(0.0032)  0.4902(0.0500)   \n",
       "multi-VAE-Gumbel                0.0299(0.0032)  0.5063(0.0500)   \n",
       "multi-VAE-Stochastic            0.0264(0.0032)  0.4514(0.0498)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.0254(0.0031)  0.4347(0.0496)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000261(0.000003)  \n",
       "multi-VAE-Faiss-IVF             0.000028(0.000000)  \n",
       "multi-VAE-Gumbel                0.000256(0.000003)  \n",
       "multi-VAE-Stochastic            0.000264(0.000003)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000028(0.000000)  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        metric_list = np.array(metric_list)\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "            metric_list_low_intaracts= metric_list[:,np.array(n_intaracts)<10]\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list_low_intaracts), np.std(metric_list_low_intaracts)/np.sqrt(len(metric_list_low_intaracts))))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "837d85c9-e30b-4d95-916d-6d33afc34708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.4731(0.0163)</td>\n",
       "      <td>0.1677(0.0074)</td>\n",
       "      <td>0.4429(0.0149)</td>\n",
       "      <td>1.0000(0.0000)</td>\n",
       "      <td>0.000261(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.4770(0.0164)</td>\n",
       "      <td>0.1629(0.0071)</td>\n",
       "      <td>0.4358(0.0152)</td>\n",
       "      <td>0.9990(0.0032)</td>\n",
       "      <td>0.000028(0.000000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.4054(0.0151)</td>\n",
       "      <td>0.1473(0.0069)</td>\n",
       "      <td>0.3872(0.0137)</td>\n",
       "      <td>0.9990(0.0032)</td>\n",
       "      <td>0.000256(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.4284(0.0157)</td>\n",
       "      <td>0.1541(0.0069)</td>\n",
       "      <td>0.4082(0.0143)</td>\n",
       "      <td>0.9993(0.0027)</td>\n",
       "      <td>0.000264(0.000003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.4428(0.0160)</td>\n",
       "      <td>0.1527(0.0067)</td>\n",
       "      <td>0.4101(0.0148)</td>\n",
       "      <td>0.9988(0.0034)</td>\n",
       "      <td>0.000028(0.000000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.4731(0.0163)  0.1677(0.0074)   \n",
       "multi-VAE-Faiss-IVF             0.4770(0.0164)  0.1629(0.0071)   \n",
       "multi-VAE-Gumbel                0.4054(0.0151)  0.1473(0.0069)   \n",
       "multi-VAE-Stochastic            0.4284(0.0157)  0.1541(0.0069)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.4428(0.0160)  0.1527(0.0067)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.4429(0.0149)  1.0000(0.0000)   \n",
       "multi-VAE-Faiss-IVF             0.4358(0.0152)  0.9990(0.0032)   \n",
       "multi-VAE-Gumbel                0.3872(0.0137)  0.9990(0.0032)   \n",
       "multi-VAE-Stochastic            0.4082(0.0143)  0.9993(0.0027)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.4101(0.0148)  0.9988(0.0034)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000261(0.000003)  \n",
       "multi-VAE-Faiss-IVF             0.000028(0.000000)  \n",
       "multi-VAE-Gumbel                0.000256(0.000003)  \n",
       "multi-VAE-Stochastic            0.000264(0.000003)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000028(0.000000)  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        metric_list = np.array(metric_list)\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "            metric_list_many_intaracts= metric_list[:,np.array(n_intaracts)>137]\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list_many_intaracts), np.std(metric_list_many_intaracts)/np.sqrt(len(metric_list_many_intaracts))))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54c72e-717e-4891-b2a7-47ef6369626d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9a946518-9030-453c-9d6d-514ff2ab3d13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "x = csr_matrix([[3, 0, 1], [0, 4, 0]])\n",
    "array1 = np.array([[0, 1, 2], [2, 1, 0]])\n",
    "\n",
    "# xの非ゼロ要素のインデックスを取得\n",
    "non_zero_indices = x.nonzero()\n",
    "\n",
    "# map関数とラムダ式を使用して各行で重複する要素を削除\n",
    "array1_result = list(map(lambda i: [ele for ele in array1[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]], range(len(array1))))\n",
    "print(array1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "88fe7f05-6e67-4933-b50f-b987ad406b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array(array1_result, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ebee3093-ad15-42b7-a86c-43c7a97126cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1]), list([2, 0])], dtype=object)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5efcf616-4355-4775-89ba-7dc4359e03aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.5       , 0.33333333, 0.25      , 0.2       ,\n",
       "       0.16666667, 0.14285714, 0.125     , 0.11111111, 0.1       ,\n",
       "       0.09090909, 0.08333333, 0.07692308, 0.07142857, 0.06666667,\n",
       "       0.0625    , 0.05882353, 0.05555556, 0.05263158, 0.05      ])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reciprocal(np.array(range(1,21), dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e965a2e-5ad0-4535-9a4e-47186b1c374c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [1]]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "A = [[1,2],[1,3,4]]\n",
    "B = [[1],[3,4]]\n",
    "# AとBの各行に対して内包表記を用いて処理\n",
    "A = [[elem for elem in a if elem not in b] for a, b in zip(A, B)]\n",
    "print(A)\n",
    "print(A[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "300e1286-4417-45d5-a8f2-979beaab1639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], []]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[] for _ in range(3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96ba9979-eb82-439d-8f59-67c88c02f049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "# AとBの初期化\n",
    "A = csr_matrix((5, 5), dtype=np.int8)\n",
    "B = np.array([[1,2],[2,3]])\n",
    "\n",
    "# Bの各行と各要素を組み合わせてAのインデックスを作成\n",
    "rows = np.repeat(np.arange(B.shape[0]), B.shape[1])\n",
    "cols = B.flatten()\n",
    "\n",
    "# 新たなスパース行列を作成\n",
    "values = np.ones(rows.shape[0], dtype=np.int8)\n",
    "B_sparse = coo_matrix((values, (rows, cols)), shape=A.shape)\n",
    "\n",
    "# AにB_sparseを追加\n",
    "A += B_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "80157a3a-49b4-406c-99cf-6cbb0225f0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a19e544c-b167-4a97-89c1-6edb2770dea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 3])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da776e59-0314-486f-9cf5-08c840e9f6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# バイナリ行列の作成\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# data = [[0, 1, 1, 0],\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         [1, 0, 1, 0],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#         [1, 0, 1, 0]]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame(data)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m data \u001b[38;5;241m=\u001b[39m train_data\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 組み合わせの大きさの範囲\u001b[39;00m\n\u001b[1;32m     17\u001b[0m r_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# バイナリ行列の作成\n",
    "# data = [[0, 1, 1, 0],\n",
    "#         [1, 0, 1, 0],\n",
    "#         [0, 1, 1, 0],\n",
    "#         [1, 1, 0, 0],\n",
    "#         [0, 1, 1, 1],\n",
    "#         [1, 0, 1, 0]]\n",
    "# df = pd.DataFrame(data)\n",
    "data = train_data\n",
    "\n",
    "# 組み合わせの大きさの範囲\n",
    "r_min = 5\n",
    "r_max = 10\n",
    "if r_max > data.shape[1]+1:\n",
    "    os.exit(0)\n",
    "\n",
    "# 各行の1の値が含まれている列の組み合わせを求める\n",
    "combinations = []\n",
    "for i in range(data.shape[0]):\n",
    "    row = data[i]\n",
    "    ones = row.nonzero()[1]  # 非ゼロ要素の列番号を取得\n",
    "    for r in range(r_min, r_max):\n",
    "        combinations.extend(itertools.combinations(ones, r))\n",
    "# for i in range(len(df)):\n",
    "#     row = df.iloc[i]\n",
    "#     ones = row[row==1].index\n",
    "#     for r in range(M, len(ones)+1):\n",
    "#         combinations.extend(itertools.combinations(ones, r))\n",
    "        \n",
    "print(combinations[:10])\n",
    "\n",
    "# 各組み合わせが何回出現するかカウントする\n",
    "counter = Counter(combinations)\n",
    "\n",
    "# 最も頻繁に出現する組み合わせを求める\n",
    "most_common_combinations = counter.most_common(1)\n",
    "\n",
    "print(most_common_combinations)\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac445a9f-4730-4fba-b3fc-29704c710c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "beta = 0.2\n",
    "# beta = 1\n",
    "# beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic_sampling_in_sequence(data_tr, data_te, n_sampling=1, K=20):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@{}\".format(K):[[] for _ in range(n_sampling)],\n",
    "               # \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@{}\".format(K):[[] for _ in range(n_sampling)],\n",
    "               # \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@{}\".format(K):[[] for _ in range(n_sampling)],\n",
    "               # \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@{}\".format(K) : [[] for _ in range(n_sampling)],\n",
    "               # \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        # \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Faiss\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-k\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-k\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-Faiss\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-Faiss-k\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Stochastic-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    n_intaracts = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "                \n",
    "                n_batch_user = data.shape[0]\n",
    "                # non_zero_indices = np.split(data.indices, data.indptr)\n",
    "                # https://stackoverflow.com/questions/59338537/summarize-non-zero-values-in-a-scipy-matrix-by-axis\n",
    "                n_already_intaract_item = data.indptr[1:] - data.indptr[:-1]\n",
    "                max_n_already_intaract_item = int(np.max(n_already_intaract_item))\n",
    "                n_intaracts.extend(n_already_intaract_item)\n",
    "                \n",
    "                for l in range(n_sampling):\n",
    "                    torch.manual_seed(l)\n",
    "                    topk_indexes = np.empty((n_batch_user,0),dtype=int)\n",
    "                    data_tmp = data.copy()\n",
    "                    \n",
    "                    t_encode = 0\n",
    "                    t_blurred = 0\n",
    "                    t_decode_blurred = 0\n",
    "                    t_get_idx_topk = 0\n",
    "                    \n",
    "                    for k in range(1,K+1):\n",
    "                        # non_zero_indices = np.split(data_tmp.indices, data_tmp.indptr)\n",
    "                        data_tensor = naive_sparse2tensor(data_tmp).to(device)\n",
    "\n",
    "                        # encoding\n",
    "                        start = time.perf_counter()\n",
    "                        mu, logvar = model.encode(data_tensor)\n",
    "                        t_encode += time.perf_counter() - start\n",
    "\n",
    "#                         # decoding\n",
    "#                         start = time.perf_counter()\n",
    "#                         recon_batch = model.decode(mu)  \n",
    "#                         t_decode = time.perf_counter() - start\n",
    "\n",
    "#                         recon_batch_clone = recon_batch.clone()\n",
    "\n",
    "#                         start = time.perf_counter()\n",
    "#                         recon_batch = recon_batch.cpu()\n",
    "#                         recon_batch = recon_batch.numpy()\n",
    "#                         # recon_batch[data.nonzero()] = -np.inf\n",
    "#                         t_to_cpu = time.perf_counter() - start\n",
    "                    \n",
    "                        # bluring z\n",
    "                        start = time.perf_counter()\n",
    "                        z_blurred = reparameterize(mu, logvar)\n",
    "                        t_blurred += time.perf_counter() - start\n",
    "\n",
    "                        # Stochastic multi-VAE\n",
    "                        start = time.perf_counter()\n",
    "                        recon_batch_blurred = model.decode(z_blurred)\n",
    "                        recon_batch_blurred = recon_batch_blurred.cpu()\n",
    "                        recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "                        recon_batch_blurred[data_tmp.nonzero()] = -np.inf\n",
    "                        t_decode_blurred += time.perf_counter() - start\n",
    "\n",
    "                        start = time.perf_counter()\n",
    "                        top_indexes = metric.get_idx_topk(recon_batch_blurred, 1)\n",
    "                        t_get_idx_topk += time.perf_counter() - start\n",
    "                        \n",
    "                        # print(top_indexes.shape)\n",
    "                        # print(data_tmp.shape)\n",
    "                        \n",
    "                        topk_indexes = np.concatenate((topk_indexes, top_indexes), axis=1)\n",
    "                        data_tmp += coo_matrix((np.ones(n_batch_user, dtype=np.int8), (np.arange(n_batch_user), top_indexes.flatten())), shape=data_tmp.shape)\n",
    "\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,K+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@{}\".format(K)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@{}\".format(K)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@{}\".format(K)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@{}\".format(K)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_decode_blurred + t_get_idx_topk)/n_batch_user)\n",
    "\n",
    "#                     # multi-VAE + Gumbel Max Sampling\n",
    "#                     start = time.perf_counter() \n",
    "#                     # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "#                     # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "#                     # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "#                     recon_batch_gumbel_sampled = recon_batch_clone - beta * (-torch.rand(recon_batch_clone.shape, device=device).log()).log()\n",
    "#                     recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.cpu()\n",
    "#                     recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "#                     # recon_batch_gumbel_sampled[non_zero_indices] = -np.inf\n",
    "#                     t_gumbel_sampling = time.perf_counter() - start\n",
    "\n",
    "#                     start = time.perf_counter()\n",
    "#                     topk_indexes = metric.get_idx_topk(recon_batch_gumbel_sampled, max_n_already_intaract_item+k)\n",
    "#                     t_get_idx_topk = time.perf_counter() - start\n",
    "\n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+k+1), dtype=float)), axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@{}\".format(k)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel\"][\"recall@{}\".format(k)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel\"][\"precision@{}\".format(k)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@{}\".format(k)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append((t_encode + t_decode + t_gumbel_sampling + t_get_idx_topk)/n_batch_user)\n",
    "\n",
    "#                     # multi-VAE\n",
    "#                     start = time.perf_counter()\n",
    "#                     topk_indexes = metric.get_idx_topk(recon_batch, max_n_already_intaract_item+k)\n",
    "#                     t_get_idx_topk = time.perf_counter() - start\n",
    "\n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+k+1), dtype=float)), axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "#                     metrics_dic[\"multi-VAE\"][\"ndcg@{}\".format(k)][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "#                     # metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE\"][\"recall@{}\".format(k)][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "#                     # metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE\"][\"precision@{}\".format(k)][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "#                     # metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE\"][\"hit_rate@{}\".format(k)][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, K))\n",
    "#                     # metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append((t_encode + t_decode + t_to_cpu + t_get_idx_topk)/n_batch_user)\n",
    "\n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic, n_intaracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c5b64-2191-4146-a85a-e0a3f21b1863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]:  90%|█████████ | 18/20 [04:45<00:31, 15.86s/it]"
     ]
    }
   ],
   "source": [
    "total_loss, metrics_dic, n_intaracts = evaluate_stochastic_sampling_in_sequence(test_data_tr, test_data_te, n_sampling=5, K=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0447d534-c45e-4d07-a20c-1f92e502bd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.1475(0.0269)</td>\n",
       "      <td>0.2343(0.0386)</td>\n",
       "      <td>0.0165(0.0027)</td>\n",
       "      <td>0.2953(0.0456)</td>\n",
       "      <td>0.004123(0.000006)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ndcg@20       recall@20    precision@20  \\\n",
       "multi-VAE-Stochastic  0.1475(0.0269)  0.2343(0.0386)  0.0165(0.0027)   \n",
       "\n",
       "                         hit_rate@20     prediction_time  \n",
       "multi-VAE-Stochastic  0.2953(0.0456)  0.004123(0.000006)  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        metric_list = np.array(metric_list)\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "        else:\n",
    "            metric_list_low_intaracts= metric_list[:,np.array(n_intaracts)<10]\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list_low_intaracts), np.std(metric_list_low_intaracts)/np.sqrt(len(metric_list_low_intaracts))))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08b89c55-3d69-4954-84c6-97f630586298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.1360(0.1010)</td>\n",
       "      <td>0.2659(0.1804)</td>\n",
       "      <td>0.0184(0.0124)</td>\n",
       "      <td>0.3309(0.2104)</td>\n",
       "      <td>0.004108(0.000031)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ndcg@20       recall@20    precision@20  \\\n",
       "multi-VAE-Stochastic  0.1360(0.1010)  0.2659(0.1804)  0.0184(0.0124)   \n",
       "\n",
       "                         hit_rate@20     prediction_time  \n",
       "multi-VAE-Stochastic  0.3309(0.2104)  0.004108(0.000031)  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method_names = list(metrics_dic.keys())\n",
    "# metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "# results_all = []\n",
    "# for method_name, metrics in metrics_dic.items():\n",
    "#     results = []\n",
    "#     for metric_name, metric_list in metrics.items():\n",
    "#         metric_list = np.array(metric_list)\n",
    "#         if metric_name == \"prediction_time\":\n",
    "#             results.append(\"{:6f}({:6f})\".format(np.mean(metric_list), np.std(metric_list)/np.sqrt(len(metric_list))))\n",
    "#         else:\n",
    "#             metric_list_low_intaracts= metric_list[:,np.array(n_intaracts)<10]\n",
    "#             results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list_low_intaracts), np.std(metric_list_low_intaracts)/np.sqrt(len(metric_list_low_intaracts))))\n",
    "#     results_all.append(results)\n",
    "# results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "# print(\"All\")\n",
    "# results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a83f84-801b-460f-bc0a-ca8720d13908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "prob-vae-pytorch",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "prob-vae-pytorch",
   "language": "python",
   "name": "prob-vae-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
