{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c711483c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "from multiprocessing import Process,Manager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import models\n",
    "import data\n",
    "import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2499a047-f928-4276-b810-ec3664fbdbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fffb4bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cf2289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f301d906170>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Set the random seed manually for reproductibility.\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b95f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device= torch.device(\"cuda\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20254a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items:20101\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "loader = data.DataLoader('ml-20m')\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "print(\"# of items:{}\".format(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a233eb47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:MultiVAE(\n",
      "  (q_layers): ModuleList(\n",
      "    (0): Linear(in_features=20101, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=400, bias=True)\n",
      "  )\n",
      "  (p_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=600, bias=True)\n",
      "    (1): Linear(in_features=600, out_features=20101, bias=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "p_dims = [200, 600, n_items]\n",
    "model = models.MultiVAE(p_dims).to(device)\n",
    "\n",
    "print(f\"Model Structure:{model}\\n\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.00)\n",
    "criterion = models.loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47fcf582-afc3-45aa-bb09-44d7a250f03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorboardX Writer\n",
    "writer= SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5190bee1-0d8a-4738-a998-bbf66ae68880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cffc33d-a12b-4704-aa18-9956c9dd6de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "TOTAL_ANNEAL_STEPS = 200000\n",
    "ANNEAL_CAP = 0.2\n",
    "LOG_INTERVAL = 100\n",
    "# EPOCHS = 100\n",
    "EPOCHS = 200\n",
    "SAVE_PATH = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e9a5898-5c51-4c4b-b361-41185fd6294d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33bfa8a2-9c1b-475c-a9dc-6527f09aee18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "718bbfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate(range(0, N, BATCH_SIZE)):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "        if TOTAL_ANNEAL_STEPS > 0:\n",
    "            anneal = min(ANNEAL_CAP, \n",
    "                            1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "        else:\n",
    "            anneal = ANNEAL_CAP\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % LOG_INTERVAL == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, BATCH_SIZE)),\n",
    "                        elapsed * 1000 / LOG_INTERVAL,\n",
    "                        train_loss / LOG_INTERVAL))\n",
    "            \n",
    "            # Log loss to tensorboard\n",
    "            n_iter = (epoch - 1) * len(range(0, N, BATCH_SIZE)) + batch_idx\n",
    "            writer.add_scalars('data/loss', {'train': train_loss / LOG_INTERVAL}, n_iter)\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0058f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_tr, data_te):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "    \n",
    "            # cno : avoid users who have no clicks in heldout_data\n",
    "            u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "            data = data[u_idxlist_wo_any_iteracts]\n",
    "            heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "            \n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "            if TOTAL_ANNEAL_STEPS > 0:\n",
    "                anneal = min(ANNEAL_CAP, \n",
    "                               1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "            else:\n",
    "                anneal = ANNEAL_CAP\n",
    "\n",
    "            recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "            loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n1 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1)\n",
    "            n100 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            r20 = metric.Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
    "            r50 = metric.Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
    "\n",
    "            n1_list.append(n1)\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r50_list.append(r50)\n",
    " \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331a9527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  100/ 233 batches | ms/batch 134.92 | loss 572.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "best_n100 = -np.inf\n",
    "update_count = 0\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss, n100, r20, r50 = evaluate(vad_data_tr, vad_data_te)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "                'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                    epoch, time.time() - epoch_start_time, val_loss,\n",
    "                    n100, r20, r50))\n",
    "        print('-' * 89)\n",
    "\n",
    "        n_iter = epoch * len(range(0, N, BATCH_SIZE))\n",
    "        writer.add_scalars('data/loss', {'valid': val_loss}, n_iter)\n",
    "        writer.add_scalar('data/n100', n100, n_iter)\n",
    "        writer.add_scalar('data/r20', r20, n_iter)\n",
    "        writer.add_scalar('data/r50', r50, n_iter)\n",
    "\n",
    "        # Save the model if the n100 is the best we've seen so far.\n",
    "        if n100 > best_n100:\n",
    "            with open(SAVE_PATH, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_n100 = n100\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "print(update_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3f3a071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "MODEL_PATH = SAVE_PATH\n",
    "with open(SAVE_PATH, 'rb') as f:\n",
    "    model = torch.load(f).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19c6cc47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.369 | n100 0.428 | r20 0.400 | r50 0.537\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "# update_count = 0\n",
    "test_loss, n1, n100, r20, r50 = evaluate(test_data_tr, test_data_te)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}'.format(test_loss, np.mean(n1), np.std(n1), np.mean(n100), np.std(n100), np.mean(r20), np.std(r20), np.mean(r50), np.std(r50)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679326ab-6f6d-494c-87bb-add56d986b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259db998-751a-4f30-8d5d-5b93d7ffe1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e99b2-f0c6-4e2e-8cdf-1ad2fd087867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index items by using weights in the encoding of VAE-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29b62da-9d3f-415c-a70f-a6769d2a2483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['q_layers.0.weight', 'q_layers.0.bias', 'q_layers.1.weight', 'q_layers.1.bias', 'p_layers.0.weight', 'p_layers.0.bias', 'p_layers.1.weight', 'p_layers.1.bias'])\n"
     ]
    }
   ],
   "source": [
    "stdict = model.state_dict()\n",
    "print(stdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63b3168b-1c7d-4bac-8823-a09662151aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 200])\n",
      "torch.Size([600])\n",
      "torch.Size([20101, 600])\n",
      "torch.Size([20101])\n"
     ]
    }
   ],
   "source": [
    "P0 = stdict['p_layers.0.weight']\n",
    "p0_bias = stdict['p_layers.0.bias']\n",
    "P1 = stdict['p_layers.1.weight']\n",
    "p1_bias = stdict['p_layers.1.bias']\n",
    "print(P0.shape)\n",
    "print(p0_bias.shape)\n",
    "print(P1.shape)\n",
    "print(p1_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd3e483-4a68-456a-b100-e7abae307cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20101, 601])\n"
     ]
    }
   ],
   "source": [
    "# B(tanh(Az+b))+b' = Bz'+b' = ([B,b'](z',1))\n",
    "P1_dash = torch.column_stack((P1,p1_bias))\n",
    "print(P1_dash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2857819a-71fb-488b-ae46-4639c7d965fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 18:43:43.980406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# https://qiita.com/saliton/items/3650e8518d8bf0684332\n",
    "import scann\n",
    "d = P1_dash.shape[1]\n",
    "scann_brute = scann.scann_ops_pybind.builder(P1_dash.numpy(), d, \"dot_product\").score_brute_force().build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244de89-a972-4a9c-9592-1462fd45026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scann_searcher = scann.scann_ops_pybind.builder(P1_dash.numpy(), d, \"dot_product\").tree(\n",
    "    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(\n",
    "    2, anisotropic_quantization_threshold=0.2).reorder(100).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8622ddf-e259-41fe-adf0-3ca432f1f217",
   "metadata": {
    "tags": []
   },
   "source": [
    "# indexing by faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8351423d-a7cc-4037-8689-f143a6abbe2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20101\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# build a flat (CPU) index\n",
    "d = P1_dash.shape[1]\n",
    "index_flat = faiss.IndexFlatIP(d)\n",
    "\n",
    "# make it into a gpu index\n",
    "# gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "# # make it into a gpu index(multi GPUs)\n",
    "# gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n",
    "#     cpu_index\n",
    "# )\n",
    "\n",
    "# indexing\n",
    "index_flat.add(P1_dash)  \n",
    "print(index_flat.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23cb45a3-bef3-4ff8-b856-b8308ec5289a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlist = 100\n",
    "quantizer = faiss.IndexFlatIP(d)\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "index_ivf.train(P1_dash)\n",
    "index_ivf.add(P1_dash)\n",
    "\n",
    "index_ivf.nprobe = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7ae6291-1e93-46ed-b561-ea00f73629a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_count = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42967c4-bf2f-46d7-aa9d-6932a5b001c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# tauをアイテム数で決める(gumbel-sharpをアイテム数で決めてみる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c91a190-0519-46a8-be0a-93f88b120df8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43603469522020893\n",
      "2.293395481969557\n"
     ]
    }
   ],
   "source": [
    "# beta = np.log(1/n_items)\n",
    "beta = 1\n",
    "# tau = 0.1\n",
    "tau = 1/np.log(np.log(n_items))\n",
    "print(tau)\n",
    "print(1/tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c19f807-4f42-4920-8977-4aaf48b7ea71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampling_ranking(scores, heldout_data, seed, n1_list_per_sampling, n100_list_per_sampling, r20_list_per_sampling, r50_list_per_sampling):\n",
    "    # Add Gumbel samples\n",
    "    np.random.seed(seed=seed)\n",
    "    gumbel_sampled_scores = scores + np.vectorize(gumbel_inverse)(np.random.uniform(size=scores.shape))\n",
    "    # Exclude examples from training set\n",
    "    # gumbel_sampled_scores[data.nonzero()] = -np.inf\n",
    "\n",
    "    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(gumbel_sampled_scores, heldout_data, 1))\n",
    "    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(gumbel_sampled_scores, heldout_data, 100))\n",
    "    r20_list_per_sampling.append(metric.Recall_at_k_batch(gumbel_sampled_scores, heldout_data, 20))\n",
    "    r50_list_per_sampling.append(metric.Recall_at_k_batch(gumbel_sampled_scores, heldout_data, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc803cf5-1add-4814-ab04-415eed2c2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_expectation2(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    manager = Manager()\n",
    "    dummy = manager.dict()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "                loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                total_loss += loss.item()\n",
    "                # pbar.set_description(OrderedDict(total_loss=total_loss))\n",
    "\n",
    "                # print(torch.mean(recon_batch,1))\n",
    "                # print(torch.transpose(recon_batch,0,1).size())\n",
    "                # print(torch.mean(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),1))\n",
    "                # print(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau).size())\n",
    "                # print(torch.mean(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau),1))\n",
    "\n",
    "                recon_batch = F.log_softmax(torch.div(recon_batch,tau), 1)\n",
    "                # recon_batch = F.log_softmax(torch.div(torch.transpose(torch.div(torch.transpose(recon_batch,0,1), torch.mean(recon_batch,1)),0,1),tau), 1)\n",
    "                recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "                \n",
    "                with Manager() as manager:\n",
    "                    # d = manager.dict()\n",
    "                    # l = manager.list()\n",
    "                    n1_list_per_sampling = manager.list()\n",
    "                    n100_list_per_sampling = manager.list()\n",
    "                    r20_list_per_sampling = manager.list()\n",
    "                    r50_list_per_sampling = manager.list()\n",
    "                    p_list = []\n",
    "                    for l in range(n_sampling):\n",
    "                        p = Process(target=sampling_ranking, args=(recon_batch,heldout_data,l,n1_list_per_sampling,n100_list_per_sampling,r20_list_per_sampling,r50_list_per_sampling))\n",
    "                        p.start()\n",
    "                        p_list.append(p)\n",
    "                        if len(p_list) % 4 == 0:\n",
    "                            for p in p_list:\n",
    "                                p.join()\n",
    "                            p_list = []\n",
    "                    \n",
    "                    for p in p_list:\n",
    "                        p.join()\n",
    "                    \n",
    "                    # print(n1_list_per_sampling)\n",
    "                    # print(n100_list_per_sampling)\n",
    "\n",
    "                    n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                    n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                    r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                    r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16bc6408-e2f6-4708-92b5-5d2a08a61493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [2:08:43<00:00, 386.16s/it]t]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 366.42 | n1 0.068(0.001) | n100 0.247(0.000) | r20 0.201(0.001) | r50 0.386(0.001)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss2, n1_list2, n100_list2, r20_list2, r50_list2 = evaluate_expectation2(test_data_tr, test_data_te, n_sampling=20)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss2, np.mean(n1_list2), np.std(n1_list2)/np.sqrt(len(n1_list2)), np.mean(n100_list2), np.std(n100_list2)/np.sqrt(len(n100_list2)), np.mean(r20_list2), np.std(r20_list2)/np.sqrt(len(r20_list2)), np.mean(r50_list2), np.std(r50_list2)/np.sqrt(len(r50_list2))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9cede-ab56-4cee-a7fb-078777a384a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b52f22da-cea6-4a38-a562-39e6198e480e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# stochastic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6896e891-13a5-46c6-8017-6e68b3b5d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochasticVAE(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n1_list = []\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    n1_list_per_sampling = []\n",
    "    n100_list_per_sampling = []\n",
    "    r20_list_per_sampling = []\n",
    "    r50_list_per_sampling = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                if TOTAL_ANNEAL_STEPS > 0:\n",
    "                    anneal = min(ANNEAL_CAP, \n",
    "                                   1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                else:\n",
    "                    anneal = ANNEAL_CAP\n",
    "\n",
    "                # recon_batch, mu, logvar = model(data_tensor)\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    z = reparameterize(mu, logvar)\n",
    "                    # print(z)\n",
    "                    recon_batch = model.decode(z)\n",
    "                    recon_batch = recon_batch.cpu().numpy()\n",
    "\n",
    "                    n1_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 1))\n",
    "                    n100_list_per_sampling.append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    r20_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    r50_list_per_sampling.append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "\n",
    "                n1_list.append(np.concatenate(n1_list_per_sampling))\n",
    "                n100_list.append(np.concatenate(n100_list_per_sampling))\n",
    "                r20_list.append(np.concatenate(r20_list_per_sampling))\n",
    "                r50_list.append(np.concatenate(r50_list_per_sampling))\n",
    "    \n",
    "    total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    n1_list = np.concatenate(n1_list)\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, n1_list, n100_list, r20_list, r50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb7676a0-ab7d-4b82-bfbf-794bba403ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [02:07<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 0.00 | n1 0.059(0.000) | n100 0.254(0.000) | r20 0.217(0.000) | r50 0.395(0.000)\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, n1_list, n100_list, r20_list, r50_list = evaluate_stochasticVAE(test_data_tr, test_data_te, n_sampling=20)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "        'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86e996-0062-4257-8e54-5fa7012d1220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a959eb-8f48-4e62-982b-1e4b44a0b77f",
   "metadata": {},
   "source": [
    "# evaluate multi-VAE, Gumbel-VAE, Stochastic-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3fa5c05-5122-4d56-8fed-1205f1750616",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.2\n",
    "# beta = 1\n",
    "# beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@20\":[[] for _ in range(n_sampling)],\n",
    "               \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@20\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@20\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@20\" : [[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic-Faiss\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                # if TOTAL_ANNEAL_STEPS > 0:\n",
    "                #     anneal = min(ANNEAL_CAP, \n",
    "                #                    1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                # else:\n",
    "                #     anneal = ANNEAL_CAP\n",
    "\n",
    "                # encoding\n",
    "                start = time.perf_counter()\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "                t_encode = time.perf_counter() - start\n",
    "                \n",
    "                # decoding\n",
    "                start = time.perf_counter()\n",
    "                recon_batch = model.decode(mu)\n",
    "                recon_batch_cpu = recon_batch.numpy()\n",
    "                t_decode = time.perf_counter() - start\n",
    "                \n",
    "                # start = time.perf_counter()\n",
    "                # recon_batch_cpu = recon_batch.cpu()\n",
    "                # t_to_cpu = time.perf_counter() - start\n",
    "                recon_batch_cpu[data.nonzero()] = -np.inf\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                # \n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    torch.manual_seed(l)\n",
    "                    \n",
    "                    # bluring z\n",
    "                    start = time.perf_counter()\n",
    "                    z_blurred = reparameterize(mu, logvar)\n",
    "                    t_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    # Stochastic multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch_blurred = model.decode(z_blurred)\n",
    "                    # recon_batch_blurred = recon_batch_blurred.cpu()\n",
    "                    recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "                    t_decode_blurred = time.perf_counter() - start\n",
    "                    recon_batch_blurred[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append(t_encode + t_blurred + t_decode_blurred)\n",
    "                    \n",
    "                    # Stochastic multi-VAE with Faiss NNS\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "                    z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(z_dash_blurred.shape[0], device=device)))\n",
    "                    # topk_scores, topk_indexes = gpu_index_flat.search(z_dash_blurred_wi_constant.cpu(), 200)\n",
    "                    topk_indexes, topk_scores = scann_brute.search_batched(z_dash_blurred_wi_constant, 200)\n",
    "                    t_nns_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((z_blurred.shape[0],n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"prediction_time\"][l].append(t_encode + t_blurred + t_nns_topk)\n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling\n",
    "                    start = time.perf_counter() \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch - beta * (-torch.rand(recon_batch.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled\n",
    "                    t_gumbel_sampling = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append(t_encode + t_decode + t_gumbel_sampling)\n",
    "                    \n",
    "                    # multi-VAE\n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_cpu, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_cpu, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_cpu, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_cpu, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append(t_encode + t_decode)\n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84009494-6ed2-43ca-b90a-7bb57507b229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta = 0.2\n",
    "# beta = 1\n",
    "# beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               \"recall@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               \"precision@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               \"hit_rate@20\" : [[] for _ in range(n_sampling)],\n",
    "               # \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Faiss\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-ScaNN\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic-Faiss\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic-Faiss-IVF\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "                \n",
    "                n_batch_user = data.shape[0]\n",
    "                non_zero_indices = data.nonzero()\n",
    "\n",
    "                # if TOTAL_ANNEAL_STEPS > 0:\n",
    "                #     anneal = min(ANNEAL_CAP, \n",
    "                #                    1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                # else:\n",
    "                #     anneal = ANNEAL_CAP\n",
    "\n",
    "                # encoding\n",
    "                start = time.perf_counter()\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "                t_encode = time.perf_counter() - start\n",
    "                \n",
    "                # decoding\n",
    "                start = time.perf_counter()\n",
    "                recon_batch = model.decode(mu)  \n",
    "                t_decode = time.perf_counter() - start\n",
    "                \n",
    "                recon_batch_clone = recon_batch.clone()\n",
    "                \n",
    "                start = time.perf_counter()\n",
    "                recon_batch = recon_batch.numpy()\n",
    "                # recon_batch[data.nonzero()] = -np.inf\n",
    "                t_to_cpu = time.perf_counter() - start\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                \n",
    "                # https://stackoverflow.com/questions/59338537/summarize-non-zero-values-in-a-scipy-matrix-by-axis\n",
    "                n_already_intaract_item = data.indptr[1:] - data.indptr[:-1]\n",
    "                max_n_already_intaract_item = int(np.max(n_already_intaract_item))\n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    torch.manual_seed(l)\n",
    "                    \n",
    "                    # bluring z\n",
    "                    start = time.perf_counter()\n",
    "                    z_blurred = reparameterize(mu, logvar)\n",
    "                    t_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    # Stochastic multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch_blurred = model.decode(z_blurred)\n",
    "                    recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "                    # recon_batch_blurred[non_zero_indices] = -np.inf\n",
    "                    t_decode_blurred = time.perf_counter() - start\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch_blurred, max_n_already_intaract_item+20)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_decode_blurred + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "                    \n",
    "                    # Stochastic multi-VAE with Faiss NNS\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "                    z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = index_flat.search(z_dash_blurred_wi_constant, max_n_already_intaract_item+20) # perform searching on GPU\n",
    "                    # topk_indexes, topk_scores = scann_brute.search_batched(z_dash_blurred_wi_constant, max_n_already_intaract_item+20)\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch_by_topk_indexes(topk_indexes, heldout_data, n_batch_user, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # Stochastic multi-VAE with Faiss NNS(IVF)\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "                    z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = index_ivf.search(z_dash_blurred_wi_constant, max_n_already_intaract_item+20)\n",
    "                    # topk_indexes, topk_scores = scann_brute.search_batched(z_dash_blurred_wi_constant, max_n_already_intaract_item+20)\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch_by_topk_indexes(topk_indexes, heldout_data, n_batch_user, 20))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss-IVF\"][\"prediction_time\"][l].append((t_encode + t_blurred + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling\n",
    "                    start = time.perf_counter() \n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_clone - beta * (-torch.rand(recon_batch_clone.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    # recon_batch_gumbel_sampled[non_zero_indices] = -np.inf\n",
    "                    t_gumbel_sampling = time.perf_counter() - start\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch_gumbel_sampled, max_n_already_intaract_item+20)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append((t_encode + t_decode + t_gumbel_sampling + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Faiss\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "                    z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = index_flat.search(z_dash_wi_constant, max_n_already_intaract_item+20)\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "#                     # multi-VAE + ScaNN\n",
    "#                     start = time.perf_counter()\n",
    "#                     z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "#                     z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "#                     # topk_scores, topk_indexes = gpu_index_flat.search(z_dash_wi_constant.cpu(), max_n_already_intaract_item+20) # perform searching on GPU\n",
    "#                     topk_indexes, topk_scores = scann_brute.search_batched(z_dash_wi_constant, max_n_already_intaract_item+20)\n",
    "#                     # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "#                     t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "#                     # create dummy recon_batch which include the corresponded score to top-k items\n",
    "#                     recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "#                     np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "#                     # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "#                     recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "#                     metrics_dic[\"multi-VAE-ScaNN\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-ScaNN\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-ScaNN\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "#                     metrics_dic[\"multi-VAE-ScaNN\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "#                     # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "#                     metrics_dic[\"multi-VAE-ScaNN\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE + Faiss(IVF)\n",
    "                    start = time.perf_counter()\n",
    "                    z_dash = torch.tanh(torch.add(torch.matmul(mu,P0.T),p0_bias))\n",
    "                    z_dash_wi_constant = torch.column_stack((z_dash, torch.ones(n_batch_user, device=device)))\n",
    "                    topk_scores, topk_indexes = index_ivf.search(z_dash_wi_constant, max_n_already_intaract_item+20) # perform searching on GPU\n",
    "                    # topk_indexes, topk_scores = scann_brute.search_batched(z_dash_wi_constant, max_n_already_intaract_item+20)\n",
    "                    # topk_indexes = list(map(lambda i: [ele for ele in topk_indexes[i] if ele not in non_zero_indices[1][non_zero_indices[0] == i]][:20], range(n_batch_user)))\n",
    "                    t_k_nns = time.perf_counter() - start\n",
    "                    \n",
    "                    # create dummy recon_batch which include the corresponded score to top-k items\n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Faiss-IVF\"][\"prediction_time\"][l].append((t_encode + t_k_nns)/n_batch_user)\n",
    "                    \n",
    "                    # multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    topk_indexes = metric.get_idx_topk(recon_batch, max_n_already_intaract_item+20)\n",
    "                    t_get_idx_topk = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((n_batch_user,n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(np.array(range(1,max_n_already_intaract_item+20+1), dtype=float)), axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.array(range(1,21), dtype=float), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch, heldout_data, 50))\n",
    "                    metrics_dic[\"multi-VAE\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append((t_encode + t_decode + t_to_cpu + t_get_idx_topk)/n_batch_user)\n",
    "                    \n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06e63bb1-8290-42e7-b289-2f0ae16c1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]: 100%|██████████| 20/20 [1:13:21<00:00, 220.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "total_loss, metrics_dic = evaluate_stochastic(test_data_tr, test_data_te, n_sampling=50)\n",
    "# print('=' * 89)\n",
    "# print('| End of training | test loss {:4.2f} | n1 {:4.3f}({:4.3f}) | n100 {:4.3f}({:4.3f}) | r20 {:4.3f}({:4.3f}) | '\n",
    "#         'r50 {:4.3f}({:4.3f})'.format(test_loss, np.mean(n1_list), np.std(n1_list)/np.sqrt(len(n1_list)), np.mean(n100_list), np.std(n100_list)/np.sqrt(len(n100_list)), np.mean(r20_list), np.std(r20_list)/np.sqrt(len(r20_list)), np.mean(r50_list), np.std(r50_list)/np.sqrt(len(r50_list))))\n",
    "# print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a062481a-3d3c-4165-aa5f-e9c09a07a595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000614(0.000031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000493(0.000046)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.2258)</td>\n",
       "      <td>0.3467(0.2772)</td>\n",
       "      <td>0.1672(0.1499)</td>\n",
       "      <td>0.8620(0.3449)</td>\n",
       "      <td>0.000333(0.000017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3079(0.2102)</td>\n",
       "      <td>0.3506(0.2762)</td>\n",
       "      <td>0.1634(0.1382)</td>\n",
       "      <td>0.8725(0.3336)</td>\n",
       "      <td>0.000848(0.000035)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2948(0.2110)</td>\n",
       "      <td>0.3268(0.2666)</td>\n",
       "      <td>0.1591(0.1428)</td>\n",
       "      <td>0.8529(0.3542)</td>\n",
       "      <td>0.000628(0.000047)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.2948(0.2110)</td>\n",
       "      <td>0.3268(0.2666)</td>\n",
       "      <td>0.1591(0.1428)</td>\n",
       "      <td>0.8529(0.3542)</td>\n",
       "      <td>0.000498(0.000053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.2894(0.2129)</td>\n",
       "      <td>0.3140(0.2661)</td>\n",
       "      <td>0.1538(0.1423)</td>\n",
       "      <td>0.8409(0.3658)</td>\n",
       "      <td>0.000337(0.000020)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.2239)  0.3656(0.2772)   \n",
       "multi-VAE-Faiss                 0.3390(0.2239)  0.3656(0.2772)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.2258)  0.3467(0.2772)   \n",
       "multi-VAE-Gumbel                0.3079(0.2102)  0.3506(0.2762)   \n",
       "multi-VAE-Stochastic            0.2948(0.2110)  0.3268(0.2666)   \n",
       "multi-VAE-Stochastic-Faiss      0.2948(0.2110)  0.3268(0.2666)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.2894(0.2129)  0.3140(0.2661)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.1524)  0.8777(0.3276)   \n",
       "multi-VAE-Faiss                 0.1759(0.1524)  0.8777(0.3276)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.1499)  0.8620(0.3449)   \n",
       "multi-VAE-Gumbel                0.1634(0.1382)  0.8725(0.3336)   \n",
       "multi-VAE-Stochastic            0.1591(0.1428)  0.8529(0.3542)   \n",
       "multi-VAE-Stochastic-Faiss      0.1591(0.1428)  0.8529(0.3542)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1538(0.1423)  0.8409(0.3658)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000614(0.000031)  \n",
       "multi-VAE-Faiss                 0.000493(0.000046)  \n",
       "multi-VAE-Faiss-IVF             0.000333(0.000017)  \n",
       "multi-VAE-Gumbel                0.000848(0.000035)  \n",
       "multi-VAE-Stochastic            0.000628(0.000047)  \n",
       "multi-VAE-Stochastic-Faiss      0.000498(0.000053)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000337(0.000020)  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic.keys())\n",
    "metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metrics_dic[method_name][metric_name]), np.std(metrics_dic[method_name][metric_name])))\n",
    "        else:\n",
    "        # # results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metrics_dic[method_name][metric_name]), np.std(metrics_dic[method_name][metric_name])))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6e99047-04f5-41ad-a8c6-98cf26aa21ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000614(0.000031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000493(0.000046)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.2258)</td>\n",
       "      <td>0.3467(0.2772)</td>\n",
       "      <td>0.1672(0.1499)</td>\n",
       "      <td>0.8620(0.3449)</td>\n",
       "      <td>0.000333(0.000017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3647(0.2190)</td>\n",
       "      <td>0.3916(0.2778)</td>\n",
       "      <td>0.1849(0.1484)</td>\n",
       "      <td>0.8991(0.3007)</td>\n",
       "      <td>0.000848(0.000035)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.3644(0.2136)</td>\n",
       "      <td>0.3956(0.2728)</td>\n",
       "      <td>0.1862(0.1504)</td>\n",
       "      <td>0.9110(0.2837)</td>\n",
       "      <td>0.000628(0.000047)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.3644(0.2136)</td>\n",
       "      <td>0.3956(0.2728)</td>\n",
       "      <td>0.1862(0.1504)</td>\n",
       "      <td>0.9110(0.2837)</td>\n",
       "      <td>0.000498(0.000053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.3543(0.2183)</td>\n",
       "      <td>0.3767(0.2759)</td>\n",
       "      <td>0.1782(0.1498)</td>\n",
       "      <td>0.8941(0.3068)</td>\n",
       "      <td>0.000337(0.000020)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.2239)  0.3656(0.2772)   \n",
       "multi-VAE-Faiss                 0.3390(0.2239)  0.3656(0.2772)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.2258)  0.3467(0.2772)   \n",
       "multi-VAE-Gumbel                0.3647(0.2190)  0.3916(0.2778)   \n",
       "multi-VAE-Stochastic            0.3644(0.2136)  0.3956(0.2728)   \n",
       "multi-VAE-Stochastic-Faiss      0.3644(0.2136)  0.3956(0.2728)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.3543(0.2183)  0.3767(0.2759)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.1524)  0.8777(0.3276)   \n",
       "multi-VAE-Faiss                 0.1759(0.1524)  0.8777(0.3276)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.1499)  0.8620(0.3449)   \n",
       "multi-VAE-Gumbel                0.1849(0.1484)  0.8991(0.3007)   \n",
       "multi-VAE-Stochastic            0.1862(0.1504)  0.9110(0.2837)   \n",
       "multi-VAE-Stochastic-Faiss      0.1862(0.1504)  0.9110(0.2837)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1782(0.1498)  0.8941(0.3068)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000614(0.000031)  \n",
       "multi-VAE-Faiss                 0.000493(0.000046)  \n",
       "multi-VAE-Faiss-IVF             0.000333(0.000017)  \n",
       "multi-VAE-Gumbel                0.000848(0.000035)  \n",
       "multi-VAE-Stochastic            0.000628(0.000047)  \n",
       "multi-VAE-Stochastic-Faiss      0.000498(0.000053)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000337(0.000020)  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method_names = list(metrics_dic.keys())\n",
    "# metric_names = list(metrics_dic[method_names[0]].keys())\n",
    "\n",
    "results_top20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metrics_dic[method_name][metric_name]), np.std(metrics_dic[method_name][metric_name])))\n",
    "        else:\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.8, axis=0)), np.std(np.quantile(metrics_dic[method_name][metric_name], 0.8, axis=0))))\n",
    "            # results.append(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.8, axis=0)))\n",
    "    results_top20per.append(results)\n",
    "results_top20per = pd.DataFrame(results_top20per, columns=metric_names, index=method_names)\n",
    "print(\"Top 20%\")\n",
    "results_top20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5285addb-1baf-4241-8379-46c4ae9bfbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>hit_rate@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000614(0.000031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.3656(0.2772)</td>\n",
       "      <td>0.1759(0.1524)</td>\n",
       "      <td>0.8777(0.3276)</td>\n",
       "      <td>0.000493(0.000046)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Faiss-IVF</th>\n",
       "      <td>0.3273(0.2258)</td>\n",
       "      <td>0.3467(0.2772)</td>\n",
       "      <td>0.1672(0.1499)</td>\n",
       "      <td>0.8620(0.3449)</td>\n",
       "      <td>0.000333(0.000017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.2480(0.1765)</td>\n",
       "      <td>0.3072(0.2646)</td>\n",
       "      <td>0.1413(0.1208)</td>\n",
       "      <td>0.8465(0.3600)</td>\n",
       "      <td>0.000848(0.000035)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2222(0.1726)</td>\n",
       "      <td>0.2564(0.2329)</td>\n",
       "      <td>0.1316(0.1265)</td>\n",
       "      <td>0.8025(0.3970)</td>\n",
       "      <td>0.000628(0.000047)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.2222(0.1726)</td>\n",
       "      <td>0.2564(0.2329)</td>\n",
       "      <td>0.1316(0.1265)</td>\n",
       "      <td>0.8025(0.3970)</td>\n",
       "      <td>0.000498(0.000053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss-IVF</th>\n",
       "      <td>0.2216(0.1765)</td>\n",
       "      <td>0.2499(0.2328)</td>\n",
       "      <td>0.1291(0.1277)</td>\n",
       "      <td>0.7929(0.4042)</td>\n",
       "      <td>0.000337(0.000020)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ndcg@20       recall@20  \\\n",
       "multi-VAE                       0.3390(0.2239)  0.3656(0.2772)   \n",
       "multi-VAE-Faiss                 0.3390(0.2239)  0.3656(0.2772)   \n",
       "multi-VAE-Faiss-IVF             0.3273(0.2258)  0.3467(0.2772)   \n",
       "multi-VAE-Gumbel                0.2480(0.1765)  0.3072(0.2646)   \n",
       "multi-VAE-Stochastic            0.2222(0.1726)  0.2564(0.2329)   \n",
       "multi-VAE-Stochastic-Faiss      0.2222(0.1726)  0.2564(0.2329)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.2216(0.1765)  0.2499(0.2328)   \n",
       "\n",
       "                                  precision@20     hit_rate@20  \\\n",
       "multi-VAE                       0.1759(0.1524)  0.8777(0.3276)   \n",
       "multi-VAE-Faiss                 0.1759(0.1524)  0.8777(0.3276)   \n",
       "multi-VAE-Faiss-IVF             0.1672(0.1499)  0.8620(0.3449)   \n",
       "multi-VAE-Gumbel                0.1413(0.1208)  0.8465(0.3600)   \n",
       "multi-VAE-Stochastic            0.1316(0.1265)  0.8025(0.3970)   \n",
       "multi-VAE-Stochastic-Faiss      0.1316(0.1265)  0.8025(0.3970)   \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.1291(0.1277)  0.7929(0.4042)   \n",
       "\n",
       "                                   prediction_time  \n",
       "multi-VAE                       0.000614(0.000031)  \n",
       "multi-VAE-Faiss                 0.000493(0.000046)  \n",
       "multi-VAE-Faiss-IVF             0.000333(0.000017)  \n",
       "multi-VAE-Gumbel                0.000848(0.000035)  \n",
       "multi-VAE-Stochastic            0.000628(0.000047)  \n",
       "multi-VAE-Stochastic-Faiss      0.000498(0.000053)  \n",
       "multi-VAE-Stochastic-Faiss-IVF  0.000337(0.000020)  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bottom20per = []\n",
    "for method_name, metrics in metrics_dic.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6f}({:6f})\".format(np.mean(metrics_dic[method_name][metric_name]), np.std(metrics_dic[method_name][metric_name])))\n",
    "        else:\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.2, axis=0)), np.std(np.quantile(metrics_dic[method_name][metric_name], 0.2, axis=0))))\n",
    "            # results.append(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.2, axis=0)))\n",
    "    results_bottom20per.append(results)\n",
    "results_bottom20per = pd.DataFrame(results_bottom20per, columns=metric_names, index=method_names)\n",
    "print(\"Bottom 20%\")\n",
    "results_bottom20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a1a6e-4481-44c0-8a71-8d45ff37856c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f59ae7-a146-451e-b98e-531ef67bfbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d9aac0a-65f4-4b9a-857c-7f3c967e507d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta = 0.2\n",
    "# beta = 1\n",
    "# beta_dash = 0.2\n",
    "\n",
    "def gumbel_inverse(x):\n",
    "    return -beta*np.log(-np.log(x))\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "def evaluate_stochastic_naive(data_tr, data_te, n_sampling=1):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    \n",
    "    metrics = {\"ndcg@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"ndcg@100\":[[] for _ in range(n_sampling)],\n",
    "               # \"recall@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"recall@50\":[[] for _ in range(n_sampling)],\n",
    "               # \"precision@20\":[[] for _ in range(n_sampling)],\n",
    "               # \"precision@50\":[[] for _ in range(n_sampling)],\n",
    "               # \"hit_rate@20\" : [[] for _ in range(n_sampling)],\n",
    "               # \"hit_rate@100\" : [[] for _ in range(n_sampling)],\n",
    "               \"prediction_time\": [[] for _ in range(n_sampling)],\n",
    "              }\n",
    "    \n",
    "    metrics_dic = {\n",
    "        \"multi-VAE\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Gumbel\":copy.deepcopy(metrics),\n",
    "        # \"multi-VAE-Gumbel-low-beta\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic\":copy.deepcopy(metrics),\n",
    "        \"multi-VAE-Stochastic-Faiss\":copy.deepcopy(metrics),\n",
    "                }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(range(0, e_N, BATCH_SIZE)) as pbar:\n",
    "        # for start_idx in tqdm(range(0, e_N, BATCH_SIZE)):\n",
    "            for start_idx in pbar:\n",
    "                pbar.set_description(\"[test]\")\n",
    "                  \n",
    "                end_idx = min(start_idx + BATCH_SIZE, N)\n",
    "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "                u_idxlist_wo_any_iteracts = [i for i, x in enumerate(heldout_data.toarray().sum(axis=1)) if x >0]\n",
    "                data = data[u_idxlist_wo_any_iteracts]\n",
    "                heldout_data = heldout_data[u_idxlist_wo_any_iteracts]\n",
    "\n",
    "                data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "                # if TOTAL_ANNEAL_STEPS > 0:\n",
    "                #     anneal = min(ANNEAL_CAP, \n",
    "                #                    1. * update_count / TOTAL_ANNEAL_STEPS)\n",
    "                # else:\n",
    "                #     anneal = ANNEAL_CAP\n",
    "\n",
    "                # encoding\n",
    "                start = time.perf_counter()\n",
    "                mu, logvar = model.encode(data_tensor)\n",
    "                t_encode = time.perf_counter() - start\n",
    "\n",
    "                # loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "                # total_loss += loss.item()\n",
    "                # recon_batch = recon_batch.cpu().numpy()\n",
    "                # \n",
    "\n",
    "                for l in range(n_sampling):\n",
    "                    # Stochastic multi-VAE with Faiss NNS\n",
    "                    torch.manual_seed(l)\n",
    "                    start = time.perf_counter()\n",
    "                    z_blurred = reparameterize(mu, logvar)\n",
    "                    z_dash_blurred = torch.tanh(torch.add(torch.matmul(z_blurred,P0.T),p0_bias))\n",
    "                    z_dash_blurred_wi_constant = torch.column_stack((z_dash_blurred,torch.ones(z_dash_blurred.shape[0], device=device)))\n",
    "                    # topk_scores, topk_indexes = gpu_index_flat.search(z_dash_blurred_wi_constant.cpu(), 200)\n",
    "                    topk_indexes, topk_scores = scann_brute.search_batched(z_dash_blurred_wi_constant, 200)\n",
    "                    t_decode = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_dummy = np.ones((z_blurred.shape[0],n_items)) * (-np.inf)\n",
    "                    np.put_along_axis(recon_batch_dummy, topk_indexes, topk_scores, axis=1)\n",
    "                    # np.put_along_axis(recon_batch_dummy, topk_indexes, np.reciprocal(topk_dists), axis=1)\n",
    "                    recon_batch_dummy[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_dummy, heldout_data, 50))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_dummy, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic-Faiss\"][\"prediction_time\"][l].append((t_encode + t_decode)/recon_batch_dummy.shape[0])\n",
    "                    print('---')\n",
    "                    print(t_encode)\n",
    "                    print(t_decode)\n",
    "                    \n",
    "                    torch.manual_seed(l)\n",
    "                    # Stochastic multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    z_blurred = reparameterize(mu, logvar)\n",
    "                    recon_batch_blurred = model.decode(z_blurred)\n",
    "                    recon_batch_blurred = recon_batch_blurred.numpy()\n",
    "                    t_decode = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_blurred[data.nonzero()] = -np.inf\n",
    "\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_blurred, heldout_data, 50))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Stochastic\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_blurred, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Stochastic\"][\"prediction_time\"][l].append((t_encode + t_decode)/recon_batch_blurred.shape[0])\n",
    "                    print(t_decode)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # multi-VAE + Gumbel Max Sampling\n",
    "                    # recon_batch_gumbel_sampled = recon_batch + np.vectorize(gumbel_inverse)(np.random.uniform(size=recon_batch.shape))\n",
    "                    # https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "                    # recon_batch_gumbel_sampled = recon_batch - torch.empty_like(recon_batch, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch = model.decode(mu)  \n",
    "                    recon_batch_gumbel_sampled = recon_batch - beta * (-torch.rand(recon_batch.shape, device=device).log()).log()\n",
    "                    recon_batch_gumbel_sampled = recon_batch_gumbel_sampled.numpy()\n",
    "                    t_decode = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch_gumbel_sampled[data.nonzero()] = -np.inf\n",
    "                    \n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 50))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE-Gumbel\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_gumbel_sampled, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE-Gumbel\"][\"prediction_time\"][l].append((t_encode + t_decode)/recon_batch_gumbel_sampled.shape[0])\n",
    "                    \n",
    "                    # multi-VAE\n",
    "                    start = time.perf_counter()\n",
    "                    recon_batch = model.decode(mu)  \n",
    "                    recon_batch = recon_batch.numpy()\n",
    "                    t_decode = time.perf_counter() - start\n",
    "                    \n",
    "                    recon_batch[data.nonzero()] = -np.inf\n",
    "                    metrics_dic[\"multi-VAE\"][\"ndcg@20\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"ndcg@100\"][l].append(metric.NDCG_binary_at_k_batch(recon_batch_cpu, heldout_data, 100))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"recall@20\"][l].append(metric.Recall_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"recall@50\"][l].append(metric.Recall_at_k_batch(recon_batch_cpu, heldout_data, 50))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"precision@20\"][l].append(metric.Precision_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"precision@50\"][l].append(metric.Precision_at_k_batch(recon_batch_cpu, heldout_data, 50))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"hit_rate@20\"][l].append(metric.HitRate_at_k_batch(recon_batch_cpu, heldout_data, 20))\n",
    "                    # metrics_dic[\"multi-VAE\"][\"hit_rate@100\"][l].append(metric.HitRate_at_k_batch(recon_batch_cpu, heldout_data, 100))\n",
    "                    metrics_dic[\"multi-VAE\"][\"prediction_time\"][l].append((t_encode + t_decode)/recon_batch.shape[0])\n",
    "                    print(t_decode)\n",
    "    \n",
    "    # total_loss /= len(range(0, e_N, BATCH_SIZE))\n",
    "    \n",
    "    for method_name, metrics in metrics_dic.items():\n",
    "        for metric_name, metric_list in metrics.items():\n",
    "            if metric_name == \"prediction_time\":\n",
    "                continue\n",
    "            for l in range(n_sampling):\n",
    "                metric_list[l] = np.concatenate(metric_list[l])\n",
    "\n",
    "    return total_loss, metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2526f8a4-cce2-42f3-979c-61edf172f717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "0.10165012600191403\n",
      "0.20238748300471343\n",
      "0.10031771200010553\n",
      "0.09559305400762241\n",
      "---\n",
      "0.10165012600191403\n",
      "0.20341147101134993\n",
      "0.09514349499659147\n",
      "0.0977189169934718\n",
      "---\n",
      "0.10165012600191403\n",
      "0.1947029730072245\n",
      "0.09814140500384383\n",
      "0.09816164898802526\n",
      "---\n",
      "0.10165012600191403\n",
      "0.20039231599366758\n",
      "0.09703384699241724\n",
      "0.09781384198868182\n",
      "---\n",
      "0.10165012600191403\n",
      "0.20374920299218502\n",
      "0.09622806799598038\n",
      "0.09781208899221383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test]:   0%|          | 0/20 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "0.10165012600191403\n",
      "0.20513160800328478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m total_loss_naive, metrics_dic_naive \u001b[38;5;241m=\u001b[39m evaluate_stochastic_naive(test_data_tr, test_data_te, n_sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[102], line 105\u001b[0m, in \u001b[0;36mevaluate_stochastic_naive\u001b[0;34m(data_tr, data_te, n_sampling)\u001b[0m\n\u001b[1;32m    103\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    104\u001b[0m z_blurred \u001b[38;5;241m=\u001b[39m reparameterize(mu, logvar)\n\u001b[0;32m--> 105\u001b[0m recon_batch_blurred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(z_blurred)\n\u001b[1;32m    106\u001b[0m recon_batch_blurred \u001b[38;5;241m=\u001b[39m recon_batch_blurred\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    107\u001b[0m t_decode \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/vae-cf-pytorch/models.py:113\u001b[0m, in \u001b[0;36mMultiVAE.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    111\u001b[0m h \u001b[38;5;241m=\u001b[39m z\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_layers):\n\u001b[0;32m--> 113\u001b[0m     h \u001b[38;5;241m=\u001b[39m layer(h)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_layers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    115\u001b[0m         h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mtanh(h)\n",
      "File \u001b[0;32m/opt/conda/envs/prob-vae-pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/prob-vae-pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/prob-vae-pytorch/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_loss_naive, metrics_dic_naive = evaluate_stochastic_naive(test_data_tr, test_data_te, n_sampling=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "46932c8e-9702-463b-96a1-460d0a50f431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.000394(0.000035)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3081(0.2102)</td>\n",
       "      <td>0.000629(0.000046)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.2952(0.2113)</td>\n",
       "      <td>0.000397(0.000035)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.2952(0.2113)</td>\n",
       "      <td>0.000589(0.000016)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ndcg@20     prediction_time\n",
       "multi-VAE                   0.3390(0.2239)  0.000394(0.000035)\n",
       "multi-VAE-Gumbel            0.3081(0.2102)  0.000629(0.000046)\n",
       "multi-VAE-Stochastic        0.2952(0.2113)  0.000397(0.000035)\n",
       "multi-VAE-Stochastic-Faiss  0.2952(0.2113)  0.000589(0.000016)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_names = list(metrics_dic_naive.keys())\n",
    "metric_names = list(metrics_dic_naive[method_names[0]].keys())\n",
    "\n",
    "results_all = []\n",
    "for method_name, metrics in metrics_dic_naive.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:6.6f}({:6.6f})\".format(np.mean(metric_list), np.std(metric_list)))\n",
    "        else:\n",
    "        # # results.append(np.mean(metrics_dic[method_name][metric_name]))\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list), np.std(metric_list)))\n",
    "    results_all.append(results)\n",
    "results_all = pd.DataFrame(results_all, columns=metric_names, index=method_names)\n",
    "print(\"All\")\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d37cb04a-8873-46c3-a133-9c7f31fb78a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi-VAE</th>\n",
       "      <td>0.3390(0.2239)</td>\n",
       "      <td>0.0004(0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Gumbel</th>\n",
       "      <td>0.3585(0.2177)</td>\n",
       "      <td>0.0006(0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic</th>\n",
       "      <td>0.3568(0.2140)</td>\n",
       "      <td>0.0004(0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-VAE-Stochastic-Faiss</th>\n",
       "      <td>0.3568(0.2140)</td>\n",
       "      <td>0.0006(0.0000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ndcg@20 prediction_time\n",
       "multi-VAE                   0.3390(0.2239)  0.0004(0.0000)\n",
       "multi-VAE-Gumbel            0.3585(0.2177)  0.0006(0.0000)\n",
       "multi-VAE-Stochastic        0.3568(0.2140)  0.0004(0.0000)\n",
       "multi-VAE-Stochastic-Faiss  0.3568(0.2140)  0.0006(0.0000)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top20per = []\n",
    "for method_name, metrics in metrics_dic_naive.items():\n",
    "    results = []\n",
    "    for metric_name, metric_list in metrics.items():\n",
    "        if metric_name == \"prediction_time\":\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(metric_list), np.std(metric_list)))\n",
    "        else:\n",
    "            results.append(\"{:4.4f}({:4.4f})\".format(np.mean(np.quantile(metric_list, 0.8, axis=0)), np.std(np.quantile(metric_list, 0.8, axis=0))))\n",
    "            # results.append(np.mean(np.quantile(metrics_dic[method_name][metric_name], 0.8, axis=0)))\n",
    "    results_top20per.append(results)\n",
    "results_top20per = pd.DataFrame(results_top20per, columns=metric_names, index=method_names)\n",
    "print(\"Top 20%\")\n",
    "results_top20per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7e9c5-9378-4b82-bedf-d92721ef9f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "prob-vae-pytorch",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "prob-vae-pytorch",
   "language": "python",
   "name": "prob-vae-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
